[["index.html", "Computational Thinking for Social Scientists Chapter 1 Hello World 1.1 Special thanks 1.2 Suggestions, questions, or comments 1.3 License", " Computational Thinking for Social Scientists Jae Yeon Kim 2021-01-16 Chapter 1 Hello World print(&quot;Hello, World!&quot;) ## [1] &quot;Hello, World!&quot; Make simple things simple, and complex things possible. - Alan Kay This is the website for Computational Thinking for Social Scientists. This book intends to help social scientists to think computationally and develop proficiency with computational tools and techniques, necessary to conduct research in computational social science. Mastering these tools and techniques not only enables social scientists to collect, wrangle, analyze, and interpret data with less pain and more fun, but it also let them to work on research projects that would previously seem impossible. The book is not intended to be a comprehensive guide for computational social science or any particular programming language, computational tool or technique. For general introduction to computational social science, I recommend Matthew Salganik’s Bit By Bit (2017). The book is currently divided into two main subjects (fundamentals and applications) and seven main sessions. 1.0.1 Part I Fundamentals Why computational thinking Best practices in data and code management using Git and Bash How to wrangle, model, and visualize data easier and faster How to use functions to automate repeated things and develop data products (e.g., packages and apps) 1.0.2 Part II Applications How to collect and parse semi-structured data at scale (e.g., using APIs and webscraping) How to analyze high-dimensional data (e.g., text) using machine learning How to access, query, and manage big data using SQL and Spark The book teaches how to do all of these mostly in R, and sometimes in bash and Python. Why R? R is free, easy to learn (thanks to tidyverse and RStudio), fast (thanks to Rcpp), runs everywhere, open (16,000+ packages; counting only ones available at CRAN), and has a growing massive and inclusive community (#rstats). Why R + Python + bash? “For R and Python, Python is first and foremost a programming language. And that has a lot of good features, but it tends to mean, that if you are going to do data science in Python, you have to first learn how to program in Python. Whereas I think you are going to get up and running faster with R, than with Python because there’s just a bunch more stuff built in and you don’t have to learn as many programming concepts. You can focus on being a great political scientist or whatever you do and learning enough R that you don’t have to become an expert programmer as well to get stuff done.” - Hadley Wickham However, this feature of the R community also raises a challenge. Compared to other programming languages, the R community tends to be more focused on results instead of processes. Knowledge of software engineering best practices is patchy: for instance, not enough R programmers use source code control or automated testing. Inconsistency is rife across contributed packages, even within base R. You are confronted with over 20 years of evolution every time you use R. R is not a particularly fast programming language, and poorly written R code can be terribly slow. R is also a profligate user of memory. - Hadley Wickham RStudio, especially the tidyverse team, has made heroic efforts to amend the problems listed above. Readers will learn these recent advances in the R ecosystem and how to complement R with Python and Bash. If you’re serious in programming, I strongly recommend learning Python. Learning Python also helps you to fill gaps in the software engineering knowledge that are useful to be highly proficient in R. 1.1 Special thanks This book is collected as much as it is authored. It is a remix version of PS239T, a graduate-level computational methods course at UC Berkeley, originally developed by Rochelle Terman then revised by Rachel Bernhard. I have taught PS239T as lead instructor in Spring 2019 and TA in Spring 2018 and will co-teach it in Spring 2020. Other teaching materials draw from the workshops I have created for D-Lab (Summer - Fall 2020) and Data Science Discovery Program (Spring 2020) at UC Berkeley. I also have cited all the other references whenever I am aware of related books, articles, slides, blog posts, or YouTube video clips. 1.2 Suggestions, questions, or comments Please feel free to create issues if you find typos, errors, missing citations, etc via the GitHub repository associated with this book. 1.3 License This work is licensed under a Creative Commons Attribution 4.0 International License. "],["motivation.html", "Chapter 2 Computational thinking 2.1 Why computational thinking 2.2 Computational way of thinking about data 2.3 Computational way of thinking about research process 2.4 References", " Chapter 2 Computational thinking 2.1 Why computational thinking If social scientists want to know how to work smart and not just hard, they need to take full advantage of the power of modern programming languages, and that power is automation. Let’s think about the following two cases (these examples come from the column I contributed to D-Lab website). Case 1: Suppose a social scientist needs to collect data on civic organizations in the United States from websites, Internal Revenue Service reports, and social media posts. As the number of these organizations is large, the researcher could not collect a large volume of data from diverse sources, so they would hire undergraduates and distribute tasks among them. This is a typical data collection plan in social science research, and it is labor-intensive. Automation is not part of the game plan. Yet, it is critical for so many reasons. Because the process is costly, no one is likely to either replicate or update the data collection effort. Put differently, without making the process efficient, it is difficult for it to be reproducible and scalable. Case 2: An alternative is to write computer programs that collect such data automatically, parse them, and store them in interconnected databases. Additionally, someone may need to maintain and validate the quality of the data infrastructure. Nevertheless, this approach lowers the cost of the data collection process, thereby substantially increasing the reproducibility and scalability of the process. Furthermore, the researcher can document their code and publicly share it using their GitHub repository or even gather some of the functions they used and distribute them as open-source libraries. Programming is as valuable a skill as writing in social science research. The extent to which a researcher can automate the research process can determine its efficiency, reproducibility, and scalability. Every modern statistical and data analysis problem needs code to solve it. You shouldn’t learn just the basics of programming, spend some time gaining mastery. Improving your programming skills pays off because code is a force multiplier: once you’ve solved a problem once, code allows you to solve it much faster in the future. As your programming skill increases, the generality of your solutions improves: you solve not just the precise problem you encountered, but a wider class of related problems (in this way programming skill is very much like mathematical skill). Finally, sharing your code with others allows them to benefit from your experience. - Hadley Wickham What aspects of the research process can be automated and how? How can we teach a machine to perform these tasks for us? From BBC Bitesize This book teaches how you to do that in R in incremental steps. From graphic user interface to command-line interface (ch 3) From short programs to long programs (ch 4-5) The ultimate goal is to solve complex problems at scale using computation (ch 6-7) “[W]e wanted users to be able to begin in an interactive environment, where they did not consciously think of themselves as progamming. Then as their needs became clearer and their sophistication increased, they should be able to slide gradually into programming, when the language and system aspects would become more important.” - Stages in the Evolution of S by John Chambers (S is the progenitor of R) 2.2 Computational way of thinking about data 2.2.1 Structure Fixed number of columns and rows? Yes!: Structured data (e.g., Excel spreadsheets, CSVs) Tidy data (a special case of structured data) No!: semi-structured data (e.g., PDFs, websites, and social media posts) unstructured data (e.g., plain texts) 2.2.2 Dimension Suppose n = the number of observations and p = the number of variables Low-dimensional data (n &gt; p) Most survey, experimental, and administrative data High-dimensional data (n &lt; p) Text, speech, image, video, etc 2.2.3 Size Small and medium data: Data fit in your laptop’s memory Big data: Data don’t fit in your laptop’s memory 2.3 Computational way of thinking about research process Computational tools and techniques make … Doing traditional research easier, faster, and scalable Data wrangling Modeling Visualization Documentation and collaboration easier, faster and scalable Dynamic reporting (markdown) Version control system (Git and GitHub) Collecting and analyzing large and complex data feasible Digital data collection (web scraping and API) Building a data infrastructure (SQL) Machine learning Text as data, image as data, etc Machine learning applications to surveys and experiments 2.4 References Here are a couple of useful videos that introduce the main concepts of computational social science. Matthew Salganik (Professor of Sociology at Princeton) is co-founder of the Summer Institute in Computational Social Science and Rayid Ghani (Professor of Machine Learning and Public Policy at Carnegie Mellon) is founder of the Data Science for Social Good. (Full disclaimer: I’m a former participant (Princeton 2019) and local organizer (Bay Area 2020) of the Summer Institute in Computational Social Science. If you become interested in computational social science and wonder what should be your next steps, I would highly recommend applying for the program.) Data Science for Social Good is another cool program that deserves your attention. "],["git-bash.html", "Chapter 3 Managing data and code 3.1 Using Bash (command line interface) 3.2 Git and GitHub 3.3 Getting started in R 3.4 Project-oriented research 3.5 Writing code: How to code like a professional", " Chapter 3 Managing data and code 3.1 Using Bash (command line interface) As William Shotts the author of The Linux Command Line put it: graphical user interfaces make easy tasks easy, while command line interfaces make difficult tasks possible. 3.1.1 Why bother using command line? Create a plain text file that contains word “test” echo: “Write arguments to the standard output” This is equivalent as using a text editor (e.g., nano, vim, emacs) and write something. &gt; test Save the expression in a file named test. In general, if you don’t know what a command does, just type &lt;command name&gt; --help. If you need more detailed information, you can do man &lt;command name&gt;. man stands for manual. Finally, you can get more user-friendly information by using either tldr. echo &quot;test&quot; &gt; test Make 100 duplicates of this file. Let’s break down the seemingly complex commands. I did this using for loop (for i in {1..100}). Curly braces {} makes 1..100 integers from 1 to 100. ; is used to use multiple commands without making line breaks. $var returns the value associated with variable. Type name=&lt;Your name&gt;. Then, type echo $name. You should see your name printed. Variable assignment is one of the most basic things you’ll learn in any programming. For novice users, I warn you that these could be too much advanced concepts. If so, don’t pay too much attention to this. For now, it’s enough to have intuitions. for i in {1..100}; do cp test &quot;test_$i&quot;; done Append “no_test” to the file named test_100. Note that I used &gt;&gt; (append) not &gt;. echo &quot;no_test&quot; &gt;&gt; test_100 Let’s read (=cat (concatenate)) what’s in test_100 cat test_100 Find which fine contains the character “no_test.” This is literally equivalent to finding a needle in a haystack. This is a daunting task for a human researcher, but not for our robotic assistant. grep finds PATTERNS in each FIEL. What follows - are options (called flags): r (recursive), n (line number), w (match only whole words), e (use patterns for matching). rnw are for output control and e is for pattern selection. You can write grep -r -n -w -e \"no_test\", but the simpler the better. grep: command -rnw -e: flags no_test: argument (usually file or file paths) grep -rnw -e &quot;no_test&quot; Let’s remove (=rm) all the duplicate files as well as the original file. * (any number of characters) is a wildcard (if you want to identify a single number of character, use ?). It finds every file whose name starts with test_. rm test_* test This command should return “test_100:2:no_test.” (file test_100; line number 2; no_test) What is this black magic? Can you do the same thing using graphical interface? Which method is more efficient? I hope that this quick demonstration will give you enough sense of why learning command line could be incredibly useful. In my experience, mastering command line helps automating your research process almost from end to end. For instance, you don’t need to write files from a website using your web browser. You can run wget command in the terminal. Better yet, you don’t even need to run the command for the second time. You can write a Shell script (*.sh) that automates downloading, moving, and sorting multiple files. You can find one example of this from the PS239T course repository. copy_syllabus.sh automatically runs an R markdown file, produces HTML and PDF outputs, and move these files to a desired location. When I modified something in the syllabus, I just need to run this Shell script again. (No worries! I will explain what is Shell shortly.) Finally, if you need to interact with servers or supercomputers for your research, you are likely to use the command-line interface. 3.1.2 UNIX Shell The following materials on UNIX and Shell are adapted from [the software carpentry](https://bids.GitHub.io/2015-06-04-berkeley/shell/00-intro.html. 3.1.2.1 Unix UNIX is an operating system + a set of tools (utilities). It was developed by AT &amp; T employees at Bell Labs (1969-1971). From Mac OS X to Linux, many of current operation systems are some versions of UNIX. For this reason, if you’re using Max OS, then you don’t need to do anything else to experience UNIX. You’re already all set. If you’re using Windows, you need to install either GitBash (a good option if you only use Bash for Git and GitHub) or Windows Subsystem (highly recommended if your use case goes beyond Git and GitHub). For more information, see this installation guideline from the course repo. If you’re an Windows user and don’t use Windows 10, I recommend installing VirtualBox. UNIX is old, but it is still mainstream and it will be. Moreover, the UNIX philosophy (“Do One Thing And Do It Well”)—minimalist, modular software development—is highly and widely influential. Ken Thompson and Dennis Ritchie, key proponents of the Unix philosophy AT&T Archives: The UNIX Operating System Unix50 - Unix Today and Tomorrow: The Languages 3.1.2.2 Kernel The kernel of UNIX is the hub of the operating system: it allocates time and memory to programs and handles the filestore (e.g., files and directories) and communications in response to system calls. 3.1.2.3 Shell The shell is an interactive program that provides an interface between the user and the kernel. The shell interprets commands entered by the user or supplied by a Shell script, and passes them to the kernel for execution. 3.1.2.4 Human-Computer interfaces At a high level, computers do four things: run programs store data communicate with each other interact with us (through either CLI or GUI) 3.1.2.5 The Command Line This kind of interface is called a command-line interface, or CLI, to distinguish it from the graphical user interface, or GUI, that most people now use. The heart of a CLI is a read-evaluate-print loop, or REPL: when the user types a command and then presses the enter (or return) key, the computer reads it, executes it, and prints its output. The user then types another command, and so on until the user logs off. If you’re using RStudio, you can use terminal inside RStudio (next to the “Console”). (For instance, type Alt + Shift + M) 3.1.2.6 The Shell This description makes it sound as though the user sends commands directly to the computer, and the computer sends output directly to the user. In fact, there is usually a program in between called a command shell. Source: Prashant Lakhera What the user types goes into the shell; it figures out what commands to run and orders the computer to execute them. Note, the reason why the shell is called the shell: it encloses the operating system in order to hide some of its complexity and make it simpler to interact with. A shell is a program like any other. What’s special about it is that its job is to run other programs rather than to do calculations itself. The commands are themselves programs: when they terminate, the shell gives the user another prompt ($ on our systems). 3.1.2.7 Bash The most popular Unix shell is Bash, the Bourne Again Shell (so-called because it’s derived from a shell written by Stephen Bourne — this is what passes for wit among programmers). Bash is the default shell on most modern implementations of Unix, and in most packages that provide Unix-like tools for Windows. 3.1.2.8 Why Shell? Using Bash or any other shell sometimes feels more like programming than like using a mouse. Commands are terse (often only a couple of characters long), their names are frequently cryptic, and their output is lines of text rather than something visual like a graph. On the other hand, the shell allows us to combine existing tools in powerful ways with only a few keystrokes and to set up pipelines to handle large volumes of data automatically. In addition, the command line is often the easiest way to interact with remote machines (explains why we learn Bash before learning Git and GitHub). As clusters and cloud computing become more popular for scientific data crunching, being able to drive them is becoming a necessary skill. 3.1.2.9 Our first command The part of the operating system responsible for managing files and directories is called the file system. It organizes our data into files, which hold information, and directories (also called “folders”), which hold files or other directories. Several commands are frequently used to create, inspect, rename, and delete files and directories. To start exploring them, let’s open a shell window: jae@jae-X705UDR:~$ jae: a specific user name jae-X705UDR: your computer/server name ~: current directory (~ = home) $: a prompt, which shows us that the shell is waiting for input; your shell may show something more elaborate. Type the command whoami, then press the Enter key (sometimes marked Return) to send the command to the shell. The command’s output is the ID of the current user, i.e., it shows us who the shell thinks we are: $ whoami # Should be your user name jae More specifically, when we type whoami the shell: finds a program called whoami, runs that program, displays that program’s output, then displays a new prompt to tell us that it’s ready for more commands. 3.1.2.10 Communicating to other systems In the next unit, we’ll be focusing on the structure of our own operating systems. But our operating systems rarely work in isolation; often, we are relying on the Internet to communicate with others! You can visualize this sort of communication within your own shell by asking your computer to ping (based on the old term for submarine sonar) an IP address provided by Google (8.8.8.8); in effect, this will test whether your Internet is working. $ ping 8.8.8.8 Note: Windows users may have to try a slightly different alternative: $ ping -t 8.8.8.8 (Thanks Paul Thissen for the suggestion!) 3.1.2.11 File system organization Next, let’s find out where we are by running a command called pwd (print working directory). At any moment, our current working directory is our current default directory, i.e., the directory that the computer assumes we want to run commands in unless we explicitly specify something else. Here, the computer’s response is /home/jae, which is the home directory: $ pwd /home/jae Additional tips You can also download files in the terminal. Install wget utility sudo apt-get install wget Download target files wget https://download1.rstudio.org/desktop/bionic/amd64/rstudio-1.4.1103-amd64.deb Home Directory The home directory path will look different on different operating systems. On Linux it will look like /home/jae, and on Windows it will be similar to C:\\Documents and Settings\\jae. Note that it may look slightly different for different versions of Windows. Alphabet Soup If the command to find out who we are is whoami, the command to find out where we are ought to be called whereami, so why is it pwd instead? The usual answer is that in the early 1970s, when Unix was first being developed, every keystroke counted: the devices of the day were slow, and backspacing on a teletype was so painful that cutting the number of keystrokes in order to cut the number of typing mistakes was actually a win for usability. The reality is that commands were added to Unix one by one, without any master plan, by people who were immersed in its jargon. The result is as inconsistent as the roolz uv Inglish speling, but we’re stuck with it now. The good news: because these basic commands were so integral to the development of early Unix, they have stuck around, and appear (in some form) in almost all programming languages. To understand what a “home directory” is, let’s have a look at how the file system as a whole is organized. At the top is the root directory that holds everything else. We refer to it using a slash character / on its own; this is the leading slash in /home/jae. Inside that directory are several other directories: bin (which is where some built-in programs are stored), data (holding miscellaneous data files) etc (where local configuration files are stored), tmp (for temporary files that don’t need to be stored long-term), and so on. If you’re working on a Mac, the file structure will look similar, but not identical. The following image shows a file system graph for the typical Mac. File Directory We know that our current working directory /home/jae is stored inside /home because /home is the first part of its name. Similarly, we know that /home is stored inside the root directory / because its name begins with /. Food for thought This hierarchical way of thinking about organizing things is prevalent in the design of computation systems. However, it doesn’t need to be that way, especially when the system concerns something like conversations. Path Notice that there are two meanings for the / character. When it appears at the front of a file or directory name, it refers to the root directory. When it appears inside a name, it’s just a separator. 3.1.2.12 Listing Let’s see what’s in your home directory by running ls (**list files and directories): $ ls Applications Dropbox Pictures Creative Cloud Files Google Drive Public Desktop Library Untitled.ipynb Documents Movies anaconda Downloads Music file.txt ls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. We can make ls more useful by adding flags. For instance, you can make your computer show only directories in the file system using the following command. Here -F flag classifies files based on some types. / indicates directories. ls -F / The leading / tells the computer to follow the path from the root of the file system, so it always refers to exactly one directory, no matter where we are when we run the command. What if we want to change our current working directory? Before we do this, pwd shows us that we’re in /home/jae, and ls without any arguments shows us that directory’s contents: $ pwd /home/jae $ ls Applications Dropbox Pictures Creative Cloud Files Google Drive Public Desktop Library Untitled.ipynb Documents Movies anaconda Downloads Music file.txt Use relative paths (e.g., ../spring_2021/references.md) whenever it’s possible so that your code is not dependable on how your system is configured. Additional tips How can I find pdf files in Downloads using the terminal? Remember * wildcard? cd Downloads/ find *.pdf Also, note that you don’t need to type every character. Type the first few characters then press TAB (autocomplete). This is called tab completion, and we will see it in many other tools as we go on. 3.1.2.13 Moving around We can use cd (change directory) followed by a directory name to change our working directory. $ cd Desktop cd doesn’t print anything, but if we run pwd after it, we can see that we are now in /home/jae/Desktop. If we run ls without arguments now, it lists the contents of /home/jae/Desktop, because that’s where we now are: $ pwd /home/jae/Desktop We now know how to go down the directory tree: how do we go up? We could use an absolute path: $ cd /home/jae/ but it’s almost always simpler to use cd .. to go up one level: $ pwd /home/jae/Desktop $ cd .. .. is a special directory name meaning “the directory containing this one,” or more succinctly, the parent of the current directory. Sure enough, if we run pwd after running cd .., we’re back in /home/jae/: $ pwd /home/jae/ The special directory .. doesn’t usually show up when we run ls. If we want to display it, we can give ls the -a flag: $ ls -a . .localized Shared .. Guest rachel -a stands for “show all”; it forces ls to show us file and directory names that begin with ., such as ... Hidden Files: For Your Own Protection As you can see, a bunch of other items just appeared when we enter ls -a. These files and directories begin with . followed by a name. These areusually files and directories that hold important programmatic information, not usually edited by the casual computer user. They are kept hidden so that users don’t accidentally delete or edit them without knowing what they’re doing. As you can see, it also displays another special directory that’s just called ., which means “the current working directory.” It may seem redundant to have a name for it, but we’ll see some uses for it soon. Phone Home If you ever want to get to the home directory immediately, you can use theshortcut ~. For example, type cd ~ and you’ll get back home in a jiffy. ~ will also stand in for your home directory in paths, so for instance~/Desktop is the same as /home/jae/Desktop. This only works if it isthe first character in the path: here/there/~/elsewhere is not /home/jae/elsewhere. Additional tips Okay. These exercises help us get to know about cd command, but not very exciting. Let’s do something more concrete and potentially useful. Let’s say you downloaded a file using your web browser and locate that file. How could you do that? Your first step should be learning more about ls command. You can do that by Googling or typing ls --help. By taking a look at the documentation, you can recognize that you need to add -t (sort by time). Then, what’s |? It’s called pipe and it chains commands. For instance, if &lt;command 1&gt; | &lt;command 2&gt;, then command1’s output will be command2’s input. head list the first ten lines of a file. -n1 flag makes it show only the first line of the output (n1). # Don&#39;t forget using TAB completion cd Downloads/ ls -t | head -n1 Yeah! We can actually do more cool things. How can you find the most recently downloaded PDF file? You can do this by combining the two neat tricks you learned earlier. ls -t | find *.pdf | head -n1 3.1.2.14 Creating, copying, removing, and renaming files 3.1.2.14.1 Creating files First, let’s create an empty directory named exercise mkdir exercise You can check whether the directory is created by typing ls. If the print format is difficult to read, add -l flag. Did you notice the difference? Let’s move to exercise subdirectory and create a file named test cd exercise ; touch test ; ls Read test cat test Hmn. It’s empty. Let’s add something there. &gt; = overwrite echo &quot;something&quot; &gt; test ; cat test Yeah! Can you add more? &gt;&gt; = append echo &quot;anything&quot; &gt;&gt; test ; cat test Removing “anything” from test is a little bit more complex because you need to know how to use grep (remember that we used this command in the very first example). Here, I just demonstrate that you can do this task using Bash and let’s dig into this more when we talk about working with text files. grep -v &#39;anything&#39; test 3.1.2.14.2 Copying and Removing Files Can we make a copy of test? Yes! cp test test_1; cat Can we make 100 copies of test? Yes! You can do this cp test test_1 cp test test_2 cp test test_3 ... or for i in {1..100}; do cp test &quot;test_$i&quot;; done Which one do you like? (Again, don’t focus on for loop. We’ll learn it and other similar tools to deal with iterations in the later chapters.) Can you remove all of test_ files? You can do this rm test_1 rm test_2 rm test_3 ... or rm test_* Which one do you like? Let’s remove the directory. cd .. rm exercise/ The rm command should not work because exercise is not a file. Type rm --help and see which flag is going to be helpful. It might be -d (remove empty directories). rm -d exercise/ Oops. Still not working because the directory is not empty. Try this. Now, it works. rm -r exercise/ What’s -r? It stands for recursion (e.g., . Recursion is a very powerful idea in programming and helps solving complex problems. We’ll come back to it many times (e.g., purrr::reduce in R). 3.1.2.14.3 Renaming files Using mv First, we will learn how to move files and see how it’s relevant for renaming files. # Create two directories mkdir exercise_1 ; mkdir exercise_2 # Check whether they were indeed created find exer* # Create an empty file touch exercise_1/test # Move to exercise_1 and check cd exercise_1 ; ls # Move this file to exercise_2 mv test ../exercise_2 # Move to exercise_2 and check cd exercise_2 ; ls What mv has something to do with renaming? [mv] [source] [destination] mv test new_test ; ls Using rename mv is a good tool to rename one file. But how about renaming many files? (Note that your pwd is still exercise_2 where you have new_test file.) for i in {1..100}; do cp new_test &quot;test_$i.csv&quot;; done Then install rename. Either sudo apt-get install -y rename or brew install rename (MacOS). Basic syntax: rename [flags] perlexpr (Perl Expression) files. Note that Perl is another programming language. # Rename every csv file to txt file rename &#39;s/.csv/.txt/&#39; *.csv # Check ls -l The key part is s/.csv/.txt/ = s/FIND/REPLACE Can you perform the same task using GUI? Yes, you can but it would be more time-consuming. Using the command line, you did this via just one-liner(!). Keith Brandnam wrote a wonderful book titled UNIX and Perl to the Rescue! (Cambridge University Press 2012) that discusses how to use UNIX and Perl to deal with massively large datasets. 3.1.2.15 Working with CSV and text files Download a CSV file (Forbes World’s Billionaires lists from 1996-2014). For more on the data source, see this site. wget https://corgis-edu.github.io/corgis/datasets/csv/billionaires/billionaires.csv Read the first two lines. We have learned cat, |, and head already. So, there’s nothing new here. cat billionaires.csv | head -n2 Check the size of the dataset (2615 rows). So, there are 2014 observations (n-1 because of the header). wc prints newline, word, and byte counts for each file. If you run wc without -l flag, you get the following: 2615 (line) 20433 (word) 607861 (byte) billionaires.csv wc -l billionaires.csv How about the number of columns? sed is a stream editor and very powerful when it’s used to filter text in a pipeline. For more information, see this article. You’ve already seen s/FIND/REPLACE. Here, the pattern we are using is s/delimiter/\\n/g. We’ve seen that the delimiter is ,, so that’s what I plugged in the command below. head -1 billionaires.csv | sed &#39;s/,/\\n/g&#39; | nl 3.1.2.16 User roles and file permissions If you need admin access, use sudo. For instance, sudo apt-get install &lt;package name&gt;. To run a Shell script (.sh), you need to change its file mode. You can make the script executable by typing chmod +x &lt;Shell script&gt;. Then, you can run it by typing ./pdf_copy_sh. . refers the current working directory. Other options: sh pdf_copy_sh. or bash pdf_copy_sh. I use ./pdf_copy_sh. 3.1.2.17 Writing your first Shell script (.sh) Finally, we’re learning how to write a Shell script (a file that ends with .sh). Here I show how to write a Shell script that creates a subdirectory called /pdfs under /Download directory, then find PDF files in /Download and copy those files to pdfs. Essentially, this Shell script creates a backup. Name this Shell script as ‘pdf_copy.sh.’ Pay attention to this example as a similar exercise is going to be part of your second assignment. #!/bin/sh # Stating this is a Shell script. mkdir /home/jae/Downloads/pdfs # Obviously, in your case this file path should be incorrect. cd Download cp *.pdf pdfs/ echo &quot;Copied pdfs&quot; Additional tips Using Make [TBD] 3.1.3 References The Unix Workbench by Sean Kross The Unix Shell, Software Carpentry Data Science at the Command Line by Jeroen Janssens Shell Tools and Scripting, ./missing-semester, MIT Command-line Environment, ./missing-semester, MIT 3.2 Git and GitHub 3.2.1 Version control system Why you should do version control According to GitHub Guides, a version control system “tracks the history of changes as people and teams collaborate on projects together.” Specifically, it helps to track the following information: Which changes were made? Who made the changes? When were the changes made? Why were changes needed? Git is a case of a distributed version control system, common in open source and commercial software development. This is no surprising given that Git was originally created to deal with Linux kernal development. If you’re curious about how the Internet works, learn one of the key ideas of the Internet: end-to-end principle. This also explains why net neutrality matters. The following images, from Pro Git, show how a centralized (e.g., CVS, Subversion, and Perforce) and decentralized VCS (e.g., Git, Mercurial, Bazzar or Darcs) works differently. Centralized version control system Figure 2. Centralized VCS. Decentralized version control system Figure 3. Decentralized VCS. For more information on the varieties of version control systems, please read Petr Baudis’s review on that subject. Figure 2.1. A schematic git workflow from Healy’s “The Plain Person’s Guide to Plain Text Social Science” 3.2.2 Setup We’ll start with telling Git who you are. $ git config --global user.name &quot;Firstname Lastname&quot; $ git config --global user.email username@company.extension 3.2.3 Making a repository Create a new directory and move to it. $ mkdir code_exercise $ cd code_exercise $ git init Alternatively, you can create a Git repository via GitHub and then clone it on your local machine. $ git clone /path/to/repository If you’re unfamiliar with basic Git commands, then please refer to this Git cheet sheet. 3.2.4 Commit changes These feature show how Git works as a version control system. If you edited files or added new ones, then you need to update your repository. In Git terms, this action is called committing changes. $ git add . # update every change. In Git terms, you&#39;re staging. $ git add file_name # or stage a specific file. $ git commit -m &quot;your comment&quot; # your comment for the commit. $ git push origin master # commit the change. Origin is a default name given to a server by Git. Another image from Pro Git well illustrates this process. Git Workflow 3.2.5 Other useful commands for tracking history $ git diff # to see what changed (e.g., inside a file) $ git log # to track who committed what $ git checkout the commit hash (e.g., a5e556) file name (fruit_list.txt) # to recover old files $ git revert 1q84 # revert to the previous commit 3.2.6 Doing other than adding $ git rm file_name # remove $ git mv old_file_name new_file_name # rename a file 3.2.7 Push and pull (or fetch) These features show how Git works as a collaboration tool. If you have not already done, let’s clone PS239T directory on your local machine. $ git clone https://GitHub.com/jaeyk/PS239T # clone Then, let’s learn more about the repository. $ git remote -v Previously, we learned how to send your data save in the local machine to the remote (the GitHub server). You can do that by editing or creating files, committing, and then typing git push. Instead, if you want to update your local data with the remote data, then you can type git pull origin (something like pwd in bash). Alternatively, you can use fetch (retrieve data from a remote). When you do that, Git retrieves the data and merge it into your local data. $ git fetch origin 3.2.8 Branching It’s an advanced feature of Git’s version control system that allows developers to “diverge from the main line of development and continue to do work without messing with that main line” according to Scott Chacon and Ben Straub. If you start working on a new feature, then create a new branch. $ git branch new_features $ git checkout new_features You can see the newly created branch by typing git branch. In short, branching makes Git works like a mini file system. 3.2.9 Collaborations Two options. Sharing a repository (suitable for a private project). Fork and pull (suitable for an open source project). ​ * The one who maintains the repository becomes the maintainer. ​ * The others can fork, make changes, and even pull them back. 3.2.9.1 Other stuff $ git status # show the status of changes $ git branch # show the branch being worked on locally $ git merge # merge branches $ git reset --hard # restore the pristine version $ git commit -a -m &quot;additional backup&quot; # to save the state again 3.2.10 Deployment: GitHub Pages 3.2.11 Tracking progress: GitHub Issues 3.2.12 Project management: GitHub Dashboards 3.3 Getting started in R 3.3.1 RStudio There are two main ways of interacting with R: using the console or by using script files (plain text files that contain your code). If R is ready to accept commands, the R console shows a &gt; prompt. If it receives a command (by typing, copy-pasting or sent from the script editor using Ctrl-Enter; Command-Enter will also work on Macs), R will try to execute it, and when ready, show the results and come back with a new &gt;-prompt to wait for new commands. This is the equivalent of the $ in your terminal. 3.3.2 Basic Syntax Comments Use # signs to comment. Comment liberally in your R scripts. Anything to the right of a # is ignored by R. For those of you familiar with other languages, there is no doc string, or equivalent to \"\"\" in R. Assignment operator &lt;- is the assignment operator. It assigns values on the right to objects on the left. So, after executing x &lt;- 3, the value of x is 3. The arrow can be read as 3 goes into x. You can also use = for assignments. USweird &lt;- &quot;Why use lb for pound!&quot; # Use this &quot;Why use lb for pound!&quot; = USweird Nonetheless, can does not mean you should. It is good practice to use &lt;- for assignments. = should only be used to specify the values of arguments of functions. This is what Google and Hadley Wickham recommend as well. If they don’t convince you enough, here’s a real example. mean(x = 1:10) # Does it save x? ## [1] 5.5 rm(x) ## Warning in rm(x): object &#39;x&#39; not found mean(x &lt;- 1:10) # Does it save x? ## [1] 5.5 rm(x) Printing In R, the contents of an object can be printed by either simply executing the the object name or calling the print() function. Help ? + object opens a help page for that specific object ?? + object searches help pages containing the name of the object ?mean ??mean help(mean) # The above three will do same. example(ls) # provides example for how to use ls help.search(&quot;visualization&quot;) # search functions and packages that have &quot;visualization&quot; in their descriptions 3.3.3 Environment Environment = a collection of pairs 3.3.3.1 Objects List objects in your current environment # Create a numeric object x &lt;- c(1,2,3,4,5) # List the object ls() # Remove the object rm(x) Remove objects from your current environment # Create an object x &lt;- 5 # Remove the object rm(x) Remove all objects from your current environment # Create an object a &lt;- 7 b &lt;- 3 # Remove the object rm(list = ls()) Force memory release # Garbage collect; for more information, type ?gc() gc() 3.3.3.2 Packages install.packages(package-name) will download a package from one of the CRAN mirrors assuming that a binary is available for your operating system. # From CRAN install.packages(&quot;dplyr&quot;) # Load package library(dplyr) # From GitHub devtools::install_GitHub(&quot;jaeyk/tidytweetjson&quot;) # my own package # Unload package # detach(&quot;package:stats&quot;, unload=TRUE) Tips If you have multiple packages to install, then please consider using pacman package. The following is the example. First, you install pacman. Then, you load several libraries by using p_load() method. install.packages(&quot;pacman&quot;) pacman::p_load( ggplot2, dplyr, broom ) If you don’t like to use pacman, then the other option is to create a list (we’re going to learn what is list soon). pkgs &lt;- c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;broom&quot;) install.packages(pkgs) Still, we have to write two lines. The simpler, the better, right? Here’s another approach that can simplify the code further. Note that lapply() applies (there’s a family of apply functions) a function to a list. In this case, library to pkgs. apply is an advanced concept, which is related to anonymous functions. We will learn about it later when we study functions. inst &lt;- lapply(pkgs, library, character.only = TRUE) 3.4 Project-oriented research 3.4.1 Computational reproducibility Replication = code + data Computational reproduciblity = code + data + environment + distribution Reproducibility checklist by Roger Peng Start with science (avoid vague questions and concepts) Don’t do things by hand (not only about automation but also documentation) Don’t point and click (same problem) Teach a computer (automation also solves documentation to some extent) Use some version control Don’t save output (instead keep the input and code) Set your seed Think about the entire pipeline 3.4.1.1 Setup pacman::p_load( tidyverse, # tidyverse here # computational reproducibility ) 3.4.1.2 Motivation Why do you need to make your research project computationally reproducible? For your self-interest and public benefits. 3.4.1.3 How to organize files in a project You won’t be able to reproduce your project unless it is efficiently organized. Step 1. Environment is part of your project. If someone can’t reproduce your environment, they won’t be able to run your code. Launch R Studio. Choose Tools &gt; Global Options. You should not check Restor .RData into workspace at startup and set saving workspace option to NEVER. Step 2. For each project, create a project directory named after the project. name_of_the_project data: raw processed (all processed, cleaned, and tided) figures packrat (optional) reports (PDF, HTML, TEX, etc.,) results (model outcomes, etc.,) scripts (i.e., functions) .gitignore (for Git) name_of_project.Rproj (for R) README.md (for Git) Working directory structure example # Don&#39;t name it a project. Use a name that&#39;s more informative. For instance, us_election not my_project. dir.create(&quot;../us_election&quot;) Step 3. Launch R Studio. Choose File &gt; New project &gt; Browse existing directories &gt; Create project This allows each project has its own workspace. Step 4. Organize files by putting them in separate subdirectories and naming them in a sensible way. Treat raw data as read only (raw data should be RAW!) and put in the data subdirectory. Note that version control does not need replace backup. You still need to backup your raw data. dir.create(here::here(&quot;us_election&quot;, &quot;data&quot;)) Separate read-only data from processed data and put in the processed_data subdirectory. dir.create(here::here(&quot;us_election&quot;, &quot;processed_data&quot;)) Put your code in the src subdirectory. dir.create(here::here(&quot;us_election&quot;, &quot;src&quot;)) Put generated outputs (e.g., tables, figures) in the outputs subdirectory and treat them as disposable. dir.create(here::here(&quot;us_election&quot;, &quot;outputs&quot;)) Put your custom functions in the functions subdirectory. You can gather some of these functions and distribute them as an open-source library. dir.create(here::here(&quot;us_election&quot;, &quot;src&quot;)) Challenge Set a project structure for a project named “starwars.” 3.4.1.4 How to organize code in a R markdown file In addition to environment, workflow is an important component of project efficiency and reproducibility. What is R markdown? An R package, developed by Yihui Xie, that provides an authoring framework for data science. Xie is also a developer of many widely popular R packages such as knitr, xaringan (cool kids use xaringan not Beamer these days), blogdown (used to create my personal website), and bookdown (used to create this book) among many others. Many applications: reports, presentations, dashboards, websites Check out Communicating with R markdown workshop by Alison Hill (RStudio) Alison Hill is a co-author of blogdown: Creating Websites with R Markdown. Key strengths: dynamic reporting + reproducible science + easy deployment Concept map for R Markdown. By Gabriela Sandoval, Florencia D’Andrea, Yanina Bellini Saibene, Monica Alonso. R Markdown The bigger picture - Garrett Grolemund R-Ladies Oslo (English) - Reports to impress your boss! Rmarkdown magic - Athanasia Mowinckel R Markdown basic syntax # Header 1 ## Header 2 ### Header 3 Use these section headers to indicate workflow. # Import packages and data # Tidy data # Wrangle data # Model data # Visualize data Press ctrl + shift + o. You can see a document outline based on these headers. This is a nice feature for finding code you need to focus. If your project’s scale is large, then divide these sections into files, number, and save them in code subdirectory. 01_wrangling.Rmd 02_modeling.Rmd … 3.4.1.5 Making a project computationally reproducible setwd(): set a working directory. Note that using setwd() is not a reproducible way to set up your project. For instance, none will be able to run the following code except me. # Set a working directory setwd(&quot;/home/jae/starwars&quot;) # Do something ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point() # Export the object. # dot means the working directory set by setwd() ggsave(&quot;./outputs/example.png&quot;) # This is called relative path Instead, learn how to use here()’. Key idea: separate workflow (e.g., workspace information) from products (code and data). For more information, read Jenny Bryan’s wonderful piece on project-oriented workflow. Example # New: Reproducible ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point() ggsave(here(&quot;project&quot;, &quot;outputs&quot;, &quot;example.png&quot;)) How here works here() function shows what’s the top-level project directory. here::here() Build a path including subdirectories here::here(&quot;project&quot;, &quot;outputs&quot;) #depth 1 #depth 2 How here defines the top-level project directory. The following list came from the here package vignette). Is a file named .here present? Is this an RStudio Project? (Note that we already set up an RStudio Project! So, if you use RStudio’s project feature, then you are ready to use here.) Is this an R package? Does it have a DESCRIPTION file? Is this a remake project? Does it have a file named remake.yml? Is this a projectile project? Does it have a file named .projectile? Is this a checkout from a version control system? Does it have a directory named .git or .svn? Currently, only Git and Subversion are supported. If there’s no match then use set_here() to create an empty .here file. Challenge Can you define computational reproducibility? Can you explain why sharing code and data is not enough for computational reproducibility 3.4.2 References Code and data management “Code and Data for the Social Sciences: A Practitioner’s Guide” by Matthew Gentkow and Jesse M. Shapiro Project-oriented research Computational reproducibility “Good Enough Practices in Scientific Computing” by PLOS Project Management with RStudio by Software Carpentry Initial steps toward reproducible research by Karl Broman Version control Version Control with Git by Software Carpentry The Plain Person’s Guide to Plain Text Social Science by Kieran Healy 3.5 Writing code: How to code like a professional 3.5.1 Write readable code What is code style? Every major open-source project has its own style guide: a set of conventions (sometimes arbitrary) about how to write code for that project. It is much easier to understand a large codebase when all the code in it is in a consistent style. - Google Style Guides 10 Tips For Clean Code - Michael Toppa How to avoid smelly code? Check out the code-smells Git repository by Jenny Bryan. Code smells and feels - Jenny Bryan \"Code smell\" is an evocative term for that vague feeling of unease we get when reading certain bits of code. It's not necessarily wrong, but neither is it obviously correct. We may be reluctant to work on such code, because past experience suggests it's going to be fiddly and bug-prone. In contrast, there's another type of code that just feels good to read and work on. What's the difference? If we can be more precise about code smells and feels, we can be intentional about writing code that is easier and more pleasant to work on. I've been fortunate to spend the last couple years embedded in a group of developers working on the tidyverse and r-lib packages. Based on this experience, I'll talk about specific code smells and deodorizing strategies for R. - Jenny Bryan Naming matters When naming files, remember the following three rules: Machine readable (avoid spaces, punctuation, periods, and any other special characters except _ and -) Human readable (should be meaningful. No text1, image1, etc.,) Ordering (e.g., 01, 02, 03, … ) # Good fit_models.R # Bad fit models.R When naming objects: Don’t use special characters. Don’t capitalize. # Good day_one # Bad DayOne When naming functions: Don’t use special characters. Don’t capitalize. Use verbs instead of nouns. (Functions do something!) # Good run_rdd # Bad rdd Spacing Some people do spacing by pressing the Tab key and others do it by pressing the Space key multiple times (and this is a serious subject). Tabs versus Spaces # Good x[, 1] mean(x, na.rm = TRUE) # Bad x[,1] mean (x, na.rm = TRUE) Indenting Indent at least 4 spaces. Note that some people, including none other than Roger Peng, indent 8 spaces. The below example shows how you can change the default indentation setting using RStudio configuration. Roger Peng’s tweet # Good if (y &lt; 0) { message(&quot;y is negative&quot;) } # Bad if (y &lt; 0) { message(&quot;Y is negative&quot;)} Long lines # Good do_something_very_complicated( something = &quot;that&quot;, requires = many, arguments = &quot;some of which may be long&quot; ) # Bad do_something_very_complicated(&quot;that&quot;, requires = many, arguments = &quot;some of which may be long&quot; ) Comments Use comments to explain your decisions. But, show your code; Do not try to explain your code by comments. Also, try to comment out rather than delete the code that you experiment with. # Average sleep hours of Jae jae %&gt;% # By week group_by(week) %&gt;% # Mean sleep hours summarise(week_sleep = mean(sleep, na.rm = TRUE)) Pipes (chaining commands) # Good iris %&gt;% group_by(Species) %&gt;% summarize_if(is.numeric, mean) %&gt;% ungroup() %&gt;% gather(measure, value, -Species) %&gt;% arrange(value) # Bad iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) %&gt;% ungroup %&gt;% gather(measure, value, -Species) %&gt;% arrange(value) Additional tips Use lintr to check whether your code complies with a recommended style guideline (e.g., tidyverse) and styler package to format your code according to the style guideline. how lintr works 3.5.2 Write reusable code Pasting Copy-and-paste programming, sometimes referred to as just pasting, is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), or certain programming idioms, and it is supported by some source code editors in the form of snippets. - Wikipedia It’s okay for pasting for the first attempt to solve a problem. But if you copy and paste three times (a.k.a. Rule of Three in programming), something’s wrong. You’re working too hard. You need to be lazy. What do I mean and how can you do that? The following exercise was inspired by Wickham’s example. Let’s imagine df is a survey dataset. a, b, c, d = Survey questions -99: non-responses Your goal: replace -99 with NA # Data set.seed(1234) # for reproducibility df &lt;- tibble(&quot;a&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;b&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;c&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;d&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE)) # Copy and paste df$a[df$a == -99] &lt;- NA df$b[df$b == -99] &lt;- NA df$c[df$c == -99] &lt;- NA df$d[df$d == -99] &lt;- NA df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Using a function function: input + computation + output If you write a function, you gain efficiency because you don’t need to copy and paste the computation part. # Create a custom function fix_missing &lt;- function(x) { # INPUT x[x == -99] &lt;- NA # COMPUTATION x # OUTPUT } # Apply the function to each column (vector) # This iterated part can and should be automated. df$a &lt;- fix_missing(df$a) df$b &lt;- fix_missing(df$b) df$c &lt;- fix_missing(df$c) df$d &lt;- fix_missing(df$d) df Automation Many options for automation in R: for loop, apply family, etc. Here’s a tidy solution comes from purrr package. The power and joy of one-liner. df &lt;- purrr::map_df(df, fix_missing) # What is this magic? We will unpack the blackbox (`map_df()`) later. df Takeaways Your code becomes more reusable, when it’s easier to change, debug, and scale up. Don’t repeat yourself and embrace the power of lazy programming. Lazy, because only lazy programmers will want to write the kind of tools that might replace them in the end. Lazy, because only a lazy programmer will avoid writing monotonous, repetitive code—thus avoiding redundancy, the enemy of software maintenance and flexible refactoring. Mostly, the tools and processes that come out of this endeavor fired by laziness will speed up the production. - Philipp Lenssen Only when your code becomes reusable, you would become efficient in your data work. Otherwise, you need to start from scratch or copy and paste, when you work on a new project. Code reuse aims to save time and resources and reduce redundancy by taking advantage of assets that have already been created in some form within the software product development process.[2] The key idea in reuse is that parts of a computer program written at one time can be or should be used in the construction of other programs written at a later time. - Wikipedia 3.5.3 Test your code systematically 3.5.4 Asking questions: Minimal reproducible example Chances are you’re going to use StackOverFlow a lot to solve a pressing problem you face. However, other can’t understand/be interested in your problem unless you can provide an example which they can understand with minimal efforts. Such example is called a minimal reproducible example. Read this StackOverFlow post to understand the concept and best practices. Simply put, a MRE consists of the following items: A minimal dataset The minimal burnable code The necessary information on package, R version, system (use sessionInfo()) A seed for reproducibility (set.seed()), if you used a random process. 3.5.5 References Writing code Style guides R Google’s R style guide R code style guide by Hadley Wickham The tidyverse style guide by Hadley Wickham Python Google Python Style Guide Code Style by the Hitchhiker’s Guide to Python "],["tidy-data.html", "Chapter 4 Tidy data and its friends 4.1 Setup 4.2 R Data structures 4.3 1D data: Vectors 4.4 2D data: Matrices and dataframes 4.5 Subset 4.6 Tidyverse 4.7 Tidy data 4.8 Tidying (tidyr) 4.9 Manipulating (dplyr) 4.10 Modeling (broom) 4.11 Visualizing (ggplot2)", " Chapter 4 Tidy data and its friends 4.1 Setup Check your dplyr package is up-to-date by typing packageVersion(\"dplyr\"). If the current installed version is less than 1.0, then update by typing update.packages(\"dplyr\"). You may need to restart R to make it work. ifelse(packageVersion(&quot;dplyr&quot;) &gt;= 1, &quot;The installed version of dplyr package is greater than or equal to 1.0.0&quot;, update.packages(&quot;dplyr&quot;) ) ## [1] &quot;The installed version of dplyr package is greater than or equal to 1.0.0&quot; if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman pacman::p_load( tidyverse, # the tidyverse framework skimr, # skimming data here, # computational reproducibility infer, # statistical inference tidymodels, # statistical modeling gapminder, # toy data nycflights13, # for exercise ggthemes, # additional themes ggrepel, # arranging ggplots patchwork, # arranging ggplots broom, # tidying model outputs waldo # side-by-side code comparison ) The rest of the chapter follows the basic structure in the Data Wrangling Cheat Sheet created by RStudio. 4.2 R Data structures To make the best use of the R language, you’ll need a strong understanding of the basic data types and data structures and how to operate on those. R is an object-oriented language, so the importance of this cannot be understated. It is critical to understand because these are the objects you will manipulate on a day-to-day basis in R, and they are not always as easy to work with as they sound at the outset. Dealing with object conversions is one of the most common sources of frustration for beginners. To understand computations in R, two slogans are helpful: - Everything that exists is an object. - Everything that happens is a function call. __John Chambers__the creator of S (the mother of R) Main Classes introduces you to R’s one-dimensional or atomic classes and data structures. R has five basic atomic classes: logical, integer, numeric, complex, character. Social scientists don’t use complex class. (Also, remember that we rarely use trigonometry.) Attributes takes a small detour to discuss attributes, R’s flexible metadata specification. Here you’ll learn about factors, an important data structure created by setting attributes of an atomic vector. R has many data structures: vector, list, matrix, data frame, factors, tables. Concept map for data types. By Meghan Sposato, Brendan Cullen, Monica Alonso. 4.3 1D data: Vectors 4.3.1 Atomic classes R’s main atomic classes are: character (or a “string” in Python and Stata) numeric (integer or float) integer (just integer) logical (booleans) Example Type “a,” “swc” character 2, 15.5 numeric 2 (Must add a L at end to denote integer) integer TRUE, FALSE logical Like Python, R is dynamically typed. There are a few differences in terminology, however, that are pertinent. First, “types” in Python are referred to as “classes” in R. What is a class? from https://brilliant.org/ Second, R has some different names for the types string, integer, and float — specifically character, integer (not different), and numeric. Because there is no “float” class in R, users tend to default to the “numeric” class when they want to work with numerical data. The function for recovering object classes is class(). L suffix to qualify any number with the intent of making it an explicit integer. See more from the R language definition. class(3) ## [1] &quot;numeric&quot; class(3L) ## [1] &quot;integer&quot; class(&quot;Three&quot;) ## [1] &quot;character&quot; class(F) ## [1] &quot;logical&quot; 4.3.2 Data structures R’s base data structures can be organized by their dimensionality (1d, 2d, or nd) and whether they’re homogeneous (all contents must be of the same type) or heterogeneous (the contents can be of different types). This gives rise to the five data types most often used in data analysis: Homogeneous Heterogeneous 1d Atomic vector List 2d Matrix Data frame nd Array Each data structure has its own specifications and behavior. For our purposes, an important thing to remember is that R is always faster (more efficient) working with homogeneous (vectorized) data. 4.3.2.1 Vector properties Vectors have three common properties: Class, class(), or what type of object it is (same as type() in Python). Length, length(), how many elements it contains (same as len() in Python). Attributes, attributes(), additional arbitrary metadata. They differ in the types of their elements: all elements of an atomic vector must be the same type, whereas the elements of a list can have different types. 4.3.2.2 Creating different types of atomic vectors Remember, there are four common types of vectors: * logical * integer * numeric (same as double) * character. You can create an empty vector with vector() (By default the mode is logical. You can be more explicit as shown in the examples below.) It is more common to use direct constructors such as character(), numeric(), etc. x &lt;- vector() # with a length and type vector(&quot;character&quot;, length = 10) ## [1] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## character vector of length 5 character(5) ## [1] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; numeric(5) ## [1] 0 0 0 0 0 logical(5) ## [1] FALSE FALSE FALSE FALSE FALSE Atomic vectors are usually created with c(), which is short for concatenate: x &lt;- c(1, 2, 3) x ## [1] 1 2 3 length(x) ## [1] 3 x is a numeric vector. These are the most common kind. You can also have logical vectors. y &lt;- c(TRUE, TRUE, FALSE, FALSE) y ## [1] TRUE TRUE FALSE FALSE Finally you can have character vectors: kim_family &lt;- c(&quot;Jae&quot;, &quot;Sun&quot;, &quot;Jane&quot;) is.integer(kim_family) # integer? ## [1] FALSE is.character(kim_family) # character? ## [1] TRUE is.atomic(kim_family) # atomic? ## [1] TRUE typeof(kim_family) # what&#39;s the type? ## [1] &quot;character&quot; Short exercise: Create and examine your vector Create a character vector called fruit that contain 4 of your favorite fruits. Then evaluate its structure using the commands below. # First create your fruit vector # YOUR CODE HERE fruit &lt;- # Examine your vector length(fruit) class(fruit) str(fruit) Add elements You can add elements to the end of a vector by passing the original vector into the c function, like so: z &lt;- c(&quot;Beyonce&quot;, &quot;Kelly&quot;, &quot;Michelle&quot;, &quot;LeToya&quot;) z &lt;- c(z, &quot;Farrah&quot;) z ## [1] &quot;Beyonce&quot; &quot;Kelly&quot; &quot;Michelle&quot; &quot;LeToya&quot; &quot;Farrah&quot; More examples of vectors x &lt;- c(0.5, 0.7) x &lt;- c(TRUE, FALSE) x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) x &lt;- 9:100 You can also create vectors as a sequence of numbers: series &lt;- 1:10 series ## [1] 1 2 3 4 5 6 7 8 9 10 seq(10) ## [1] 1 2 3 4 5 6 7 8 9 10 seq(1, 10, by = 0.1) ## [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 ## [16] 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 ## [31] 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0 5.1 5.2 5.3 5.4 ## [46] 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 ## [61] 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 8.2 8.3 8.4 ## [76] 8.5 8.6 8.7 8.8 8.9 9.0 9.1 9.2 9.3 9.4 9.5 9.6 9.7 9.8 9.9 ## [91] 10.0 Atomic vectors are always flat, even if you nest c()’s: c(1, c(2, c(3, 4))) ## [1] 1 2 3 4 # the same as c(1, 2, 3, 4) ## [1] 1 2 3 4 Types and Tests Given a vector, you can determine its class with class, or check if it’s a specific type with an “is” function: is.character(), is.numeric(), is.integer(), is.logical(), or, more generally, is.atomic(). char_var &lt;- c(&quot;harry&quot;, &quot;sally&quot;) class(char_var) ## [1] &quot;character&quot; is.character(char_var) ## [1] TRUE is.atomic(char_var) ## [1] TRUE num_var &lt;- c(1, 2.5, 4.5) class(num_var) ## [1] &quot;numeric&quot; is.numeric(num_var) ## [1] TRUE is.atomic(num_var) ## [1] TRUE NB: is.vector() does not test if an object is a vector. Instead it returns TRUE only if the object is a vector with no attributes apart from names. Use is.atomic(x) || is.list(x) to test if an object is actually a vector. Coercion All elements of an atomic vector must be the same type, so when you attempt to combine different types they will be coerced to the most flexible type. Types from least to most flexible are: logical, integer, double, and character. For example, combining a character and an integer yields a character: str(c(&quot;a&quot;, 1)) ## chr [1:2] &quot;a&quot; &quot;1&quot; Guess what the following do without running them first c(1.7, &quot;a&quot;) c(TRUE, 2) c(&quot;a&quot;, TRUE) Notice that when a logical vector is coerced to an integer or double, TRUE becomes 1 and FALSE becomes 0. This is very useful in conjunction with sum() and mean() x &lt;- c(FALSE, FALSE, TRUE) as.numeric(x) ## [1] 0 0 1 # Total number of TRUEs sum(x) ## [1] 1 # Proportion that are TRUE mean(x) ## [1] 0.3333333 Coercion often happens automatically. This is called implicit coercion. Most mathematical functions (+, log, abs, etc.) will coerce to a numeric or integer, and most logical operations (&amp;, |, any, etc) will coerce to a logical. You will usually get a warning message if the coercion might lose information. 1 &lt; &quot;2&quot; ## [1] TRUE &quot;1&quot; &gt; 2 ## [1] FALSE You can also coerce vectors explicitly coerce with as.character(), as.numeric(), as.integer(), or as.logical(). Example: x &lt;- 0:6 as.numeric(x) ## [1] 0 1 2 3 4 5 6 as.logical(x) ## [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE as.character(x) ## [1] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; Sometimes coercions, especially nonsensical ones, won’t work. x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) as.numeric(x) ## Warning: NAs introduced by coercion ## [1] NA NA NA as.logical(x) ## [1] NA NA NA Short Exercise # 1. Create a vector of a sequence of numbers between 1 to 10. # 2. Coerce that vector into a character vector # 3. Add the element &quot;11&quot; to the end of the vector # 4. Coerce it back to a numeric vector. 4.3.2.3 Lists Lists are also vectors, but different from atomic vectors because their elements can be of any type. In short, they are generic vectors. You construct lists by using list() instead of c(): Lists are sometimes called recursive vectors, because a list can contain other lists. This makes them fundamentally different from atomic vectors. x &lt;- list(1, &quot;a&quot;, TRUE, c(4, 5, 6)) x ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;a&quot; ## ## [[3]] ## [1] TRUE ## ## [[4]] ## [1] 4 5 6 You can coerce other objects using as.list(). You can test for a list with is.list() x &lt;- 1:10 x &lt;- as.list(x) is.list(x) ## [1] TRUE length(x) ## [1] 10 c() will combine several lists into one. If given a combination of atomic vectors and lists, c() (concatenate) will coerce the vectors to lists before combining them. Compare the results of list() and c(): x &lt;- list(list(1, 2), c(3, 4)) y &lt;- c(list(1, 2), c(3, 4)) str(x) ## List of 2 ## $ :List of 2 ## ..$ : num 1 ## ..$ : num 2 ## $ : num [1:2] 3 4 str(y) ## List of 4 ## $ : num 1 ## $ : num 2 ## $ : num 3 ## $ : num 4 You can turn a list into an atomic vector with unlist(). If the elements of a list have different types, unlist() uses the same coercion rules as c(). x &lt;- list(list(1, 2), c(3, 4)) x ## [[1]] ## [[1]][[1]] ## [1] 1 ## ## [[1]][[2]] ## [1] 2 ## ## ## [[2]] ## [1] 3 4 unlist(x) ## [1] 1 2 3 4 Lists are used to build up many of the more complicated data structures in R. For example, both data frames and linear models objects (as produced by lm()) are lists: is.list(mtcars) ## [1] TRUE mod &lt;- lm(mpg ~ wt, data = mtcars) is.list(mod) ## [1] TRUE For this reason, lists are extremely useful inside functions. You can “staple” together lots of different kinds of results into a single object that a function can return. A list does not print to the console like a vector. Instead, each element of the list starts on a new line. x.vec &lt;- c(1, 2, 3) x.list &lt;- list(1, 2, 3) x.vec ## [1] 1 2 3 x.list ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 For lists, elements are indexed by double brackets. Single brackets will still return a(nother) list. (We’ll talk more about subsetting and indexing in the fourth lesson.) Exercises What are the four basic types of atomic vector? How does a list differ from an atomic vector? Why is 1 == \"1\" true? Why is -1 &lt; FALSE true? Why is \"one\" &lt; 2 false? Create three vectors and then combine them into a list. If x is a list, what is the class of x[1]? How about x[[1]]? 4.3.3 Attributes Attributes provide additional information about the data to you, the user, and to R. We’ve already seen the following three attributes in action: Names (names(x)), a character vector giving each element a name. Dimensions (dim(x)), used to turn vectors into matrices. Class (class(x)), used to implement the S3 object system. Additional tips In an object-oriented system, a class (an extensible problem-code-template) defines a type of objects like what its properties are, how it behaves, and how it relates to other types of objects. Therefore, technically, an object is an instance (or occurrence) of a class. A method is a function associated with a particular type of object. 4.3.3.1 Names You can name a vector when you create it: x &lt;- c(a = 1, b = 2, c = 3) You can also modifying an existing vector: x &lt;- 1:3 names(x) ## NULL names(x) &lt;- c(&quot;e&quot;, &quot;f&quot;, &quot;g&quot;) names(x) ## [1] &quot;e&quot; &quot;f&quot; &quot;g&quot; Names don’t have to be unique. However, character subsetting, described in the next lesson, is the most important reason to use names and it is most useful when the names are unique. (For Python users: when names are unique, a vector behaves kind of like a Python dictionary key.) Not all elements of a vector need to have a name. If some names are missing, names() will return an empty string for those elements. If all names are missing, names() will return NULL. y &lt;- c(a = 1, 2, 3) names(y) ## [1] &quot;a&quot; &quot;&quot; &quot;&quot; z &lt;- c(1, 2, 3) names(z) ## NULL You can create a new vector without names using unname(x), or remove names in place with names(x) &lt;- NULL. 4.3.3.2 Factors Factors are special vectors that represent categorical data. Factors can be ordered (ordinal variable) or unordered (nominal or categorical variable) and are important for modeling functions such as lm() and glm() and also in plot methods. Quiz 1. If you want to enter dummy variables (Democrats = 1, Non-democrats = 0) in your regression model, should you use numeric or factor variable? Factors can only contain pre-defined values. Set allowed values using the levels() attribute. Note that a factor’s levels will always be character values. x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;) x &lt;- factor(c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;)) x ## [1] a b b a ## Levels: a b class(x) ## [1] &quot;factor&quot; levels(x) ## [1] &quot;a&quot; &quot;b&quot; # You can&#39;t use values that are not in the levels x[2] &lt;- &quot;c&quot; ## Warning in `[&lt;-.factor`(`*tmp*`, 2, value = &quot;c&quot;): invalid factor level, NA ## generated # NB: you can&#39;t combine factors c(factor(&quot;a&quot;), factor(&quot;b&quot;)) ## [1] 1 1 rep(1:5, rep(6, 5)) ## [1] 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 4 5 5 5 5 5 5 Factors are pretty much integers that have labels on them. Underneath, it’s really numbers (1, 2, 3…). x &lt;- factor(c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;)) str(x) ## Factor w/ 2 levels &quot;a&quot;,&quot;b&quot;: 1 2 2 1 They are better than using simple integer labels because factors are what are called self describing. For example, democrat and republican is more descriptive than 1s and 2s. Factors are useful when you know the possible values a variable may take, even if you don’t see all values in a given dataset. Using a factor instead of a character vector makes it obvious when some groups contain no observations: party_char &lt;- c(&quot;democrat&quot;, &quot;democrat&quot;, &quot;democrat&quot;) party_char ## [1] &quot;democrat&quot; &quot;democrat&quot; &quot;democrat&quot; party_factor &lt;- factor(party_char, levels = c(&quot;democrat&quot;, &quot;republican&quot;)) party_factor ## [1] democrat democrat democrat ## Levels: democrat republican table(party_char) # shows only democrats ## party_char ## democrat ## 3 table(party_factor) # shows republicans too ## party_factor ## democrat republican ## 3 0 Sometimes factors can be left unordered. Example: democrat, republican. Other times you might want factors to be ordered (or ranked). Example: low, medium, high. x &lt;- factor(c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;)) str(x) ## Factor w/ 3 levels &quot;high&quot;,&quot;low&quot;,&quot;medium&quot;: 2 3 1 is.ordered(x) ## [1] FALSE y &lt;- ordered(c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), levels = c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;)) is.ordered(y) ## [1] TRUE While factors look (and often behave) like character vectors, they are actually integers. Be careful when treating them like strings. Some string methods (like gsub() and grepl()) will coerce factors to strings, while others (like nchar()) will throw an error, and still others (like c()) will use the underlying integer values. x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;) x ## [1] &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;a&quot; is.factor(x) ## [1] FALSE x &lt;- as.factor(x) x ## [1] a b b a ## Levels: a b c(x, &quot;c&quot;) ## [1] &quot;1&quot; &quot;2&quot; &quot;2&quot; &quot;1&quot; &quot;c&quot; For this reason, it’s usually best to explicitly convert factors to character vectors if you need string-like behavior. In early versions of R, there was a memory advantage to using factors instead of character vectors, but this is no longer the case. Unfortunately, most data loading functions in R automatically convert character vectors to factors. This is suboptimal, because there’s no way for those functions to know the set of all possible levels or their optimal order. If this becomes a problem, use the argument stringsAsFactors = FALSE to suppress this behavior, and then manually convert character vectors to factors using your knowledge of the data. More attributes All R objects can have arbitrary additional attributes, used to store metadata about the object. Attributes can be thought of as a named list (with unique names). Attributes can be accessed individually with attr() or all at once (as a list) with attributes(). y &lt;- 1:10 attr(y, &quot;my_attribute&quot;) &lt;- &quot;This is a vector&quot; attr(y, &quot;my_attribute&quot;) ## [1] &quot;This is a vector&quot; # str returns a new object with modified information str(attributes(y)) ## List of 1 ## $ my_attribute: chr &quot;This is a vector&quot; Exercises What happens to a factor when you modify its levels? f1 &lt;- factor(letters) levels(f1) &lt;- rev(levels(f1)) f1 ## [1] z y x w v u t s r q p o n m l k j i h g f e d c b a ## Levels: z y x w v u t s r q p o n m l k j i h g f e d c b a What does this code do? How do f2 and f3 differ from f1? f2 &lt;- rev(factor(letters)) f3 &lt;- factor(letters, levels = rev(letters)) 4.4 2D data: Matrices and dataframes Matrices: data structures for storing 2d data that is all the same class. Dataframes: teaches you about the dataframe, the most important data structure for storing data in R, because it stores different kinds of (2d) data. 4.4.1 Matrices Matrices are created when we combine multiple vectors that all have the same class (e.g., numeric). This creates a dataset with rows and columns. By definition, if you want to combine multiple classes of vectors, you want a dataframe. You can coerce a matrix to become a dataframe, and vice-versa, but as with all vector coercions, the results can be unpredictable, so be sure you know how each variable (column) will convert. m &lt;- matrix(nrow = 2, ncol = 2) m ## [,1] [,2] ## [1,] NA NA ## [2,] NA NA dim(m) ## [1] 2 2 Matrices are filled column-wise. m &lt;- matrix(1:6, nrow = 2, ncol = 3) m ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Other ways to construct a matrix m &lt;- 1:10 dim(m) &lt;- c(2, 5) m ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 dim(m) &lt;- c(5, 2) m ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 You can transpose a matrix (or dataframe) with t() m &lt;- 1:10 dim(m) &lt;- c(2, 5) m ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 t(m) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 ## [4,] 7 8 ## [5,] 9 10 Another way is to bind columns or rows using cbind() and rbind(). x &lt;- 1:3 y &lt;- 10:12 cbind(x, y) ## x y ## [1,] 1 10 ## [2,] 2 11 ## [3,] 3 12 # or rbind(x, y) ## [,1] [,2] [,3] ## x 1 2 3 ## y 10 11 12 You can also use the byrow argument to specify how the matrix is filled. From R’s own documentation: mdat &lt;- matrix(c(1, 2, 3, 11, 12, 13), nrow = 2, ncol = 3, byrow = TRUE, dimnames = list( c(&quot;row1&quot;, &quot;row2&quot;), c(&quot;C.1&quot;, &quot;C.2&quot;, &quot;C.3&quot;) ) ) mdat ## C.1 C.2 C.3 ## row1 1 2 3 ## row2 11 12 13 Notice that we gave names to the dimensions in mdat. dimnames(mdat) ## [[1]] ## [1] &quot;row1&quot; &quot;row2&quot; ## ## [[2]] ## [1] &quot;C.1&quot; &quot;C.2&quot; &quot;C.3&quot; rownames(mdat) ## [1] &quot;row1&quot; &quot;row2&quot; colnames(mdat) ## [1] &quot;C.1&quot; &quot;C.2&quot; &quot;C.3&quot; 4.4.2 Dataframes A data frame is a very important data type in R. It’s pretty much the de facto data structure for most tabular data and what we use for statistics. 4.4.2.1 Creation You create a data frame using data.frame(), which takes named vectors as input: vec1 &lt;- 1:3 vec2 &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) df &lt;- data.frame(vec1, vec2) df ## vec1 vec2 ## 1 1 a ## 2 2 b ## 3 3 c str(df) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ vec1: int 1 2 3 ## $ vec2: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; Beware: data.frame()’s default behavior which turns strings into factors. Remember to use stringAsFactors = FALSE to suppress this behavior as needed: df &lt;- data.frame( x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), stringsAsFactors = FALSE ) str(df) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ x: int 1 2 3 ## $ y: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; In reality, we rarely type up our datasets ourselves, and certainly not in R. The most common way to make a data.frame is by calling a file using read.csv (which relies on the foreign package), read.dta (if you’re using a Stata file), or some other kind of data file input. 4.4.2.2 Structure and Attributes Under the hood, a data frame is a list of equal-length vectors. This makes it a 2-dimensional structure, so it shares properties of both the matrix and the list. vec1 &lt;- 1:3 vec2 &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) df &lt;- data.frame(vec1, vec2) str(df) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ vec1: int 1 2 3 ## $ vec2: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; This means that a dataframe has names(), colnames(), and rownames(), although names() and colnames() are the same thing. ** Summary ** Set column names: names() in data frame, colnames() in matrix Set row names: row.names() in data frame, rownames() in matrix vec1 &lt;- 1:3 vec2 &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) df &lt;- data.frame(vec1, vec2) # these two are equivalent names(df) ## [1] &quot;vec1&quot; &quot;vec2&quot; colnames(df) ## [1] &quot;vec1&quot; &quot;vec2&quot; # change the colnames colnames(df) &lt;- c(&quot;Number&quot;, &quot;Character&quot;) df ## Number Character ## 1 1 a ## 2 2 b ## 3 3 c names(df) &lt;- c(&quot;Number&quot;, &quot;Character&quot;) df ## Number Character ## 1 1 a ## 2 2 b ## 3 3 c # change the rownames rownames(df) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; rownames(df) &lt;- c(&quot;donut&quot;, &quot;pickle&quot;, &quot;pretzel&quot;) df ## Number Character ## donut 1 a ## pickle 2 b ## pretzel 3 c The length() of a dataframe is the length of the underlying list and so is the same as ncol(); nrow() gives the number of rows. vec1 &lt;- 1:3 vec2 &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) df &lt;- data.frame(vec1, vec2) # these two are equivalent - number of columns length(df) ## [1] 2 ncol(df) ## [1] 2 # get number of rows nrow(df) ## [1] 3 # get number of both columns and rows dim(df) ## [1] 3 2 4.4.2.3 Testing and coercion To check if an object is a dataframe, use class() or test explicitly with is.data.frame(): class(df) ## [1] &quot;data.frame&quot; is.data.frame(df) ## [1] TRUE You can coerce an object to a dataframe with as.data.frame(): A vector will create a one-column dataframe. A list will create one column for each element; it’s an error if they’re not all the same length. A matrix will create a data frame with the same number of columns and rows as the matrix. 4.4.2.4 Combining dataframes You can combine dataframes using cbind() and rbind(): df &lt;- data.frame( x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), stringsAsFactors = FALSE ) cbind(df, data.frame(z = 3:1)) ## x y z ## 1 1 a 3 ## 2 2 b 2 ## 3 3 c 1 rbind(df, data.frame(x = 10, y = &quot;z&quot;)) ## x y ## 1 1 a ## 2 2 b ## 3 3 c ## 4 10 z When combining column-wise, the number of rows must match, but row names are ignored. When combining row-wise, both the number and names of columns must match. (If you want to combine rows that don’t have the same columns, there are other functions / packages in R that can help.) It’s a common mistake to try and create a dataframe by cbind()ing vectors together. This doesn’t work because cbind() will create a matrix unless one of the arguments is already a dataframe. Instead use data.frame() directly: bad &lt;- (cbind(x = 1:2, y = c(&quot;a&quot;, &quot;b&quot;))) bad ## x y ## [1,] &quot;1&quot; &quot;a&quot; ## [2,] &quot;2&quot; &quot;b&quot; str(bad) ## chr [1:2, 1:2] &quot;1&quot; &quot;2&quot; &quot;a&quot; &quot;b&quot; ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:2] &quot;x&quot; &quot;y&quot; good &lt;- data.frame( x = 1:2, y = c(&quot;a&quot;, &quot;b&quot;), stringsAsFactors = FALSE ) good ## x y ## 1 1 a ## 2 2 b str(good) ## &#39;data.frame&#39;: 2 obs. of 2 variables: ## $ x: int 1 2 ## $ y: chr &quot;a&quot; &quot;b&quot; The conversion rules for cbind() are complicated and best avoided by ensuring all inputs are of the same type. Other objects Missing values are specified with NA, which is a logical vector of length 1. NA will always be coerced to the correct type if used inside c() x &lt;- c(NA, 1) x ## [1] NA 1 typeof(NA) ## [1] &quot;logical&quot; typeof(x) ## [1] &quot;double&quot; Inf is infinity. You can have either positive or negative infinity. 1 / 0 ## [1] Inf 1 / Inf ## [1] 0 NaN means Not a number. It’s an undefined value. 0 / 0 ## [1] NaN 4.5 Subset When working with data, you’ll need to subset objects early and often. Luckily, R’s subsetting operators are powerful and fast. Mastery of subsetting allows you to succinctly express complex operations in a way that few other languages can match. Subsetting is hard to learn because you need to master a number of interrelated concepts: The three subsetting operators, [, [[, and $. Important differences in behavior for different objects (e.g., vectors, lists, factors, matrices, and data frames). The use of subsetting in conjunction with assignment. This unit helps you master subsetting by starting with the simplest type of subsetting: subsetting an atomic vector with [. It then gradually extends your knowledge, first to more complicated data types (like dataframes and lists), and then to the other subsetting operators, [[ and $. You’ll then learn how subsetting and assignment can be combined to modify parts of an object, and, finally, you’ll see a large number of useful applications. 4.5.1 Atomic vectors Let’s explore the different types of subsetting with a simple vector, x. x &lt;- c(2.1, 4.2, 3.3, 5.4) Note that the number after the decimal point gives the original position in the vector. NB: In R, positions start at 1, unlike Python, which starts at 0. Fun!** There are five things that you can use to subset a vector: 4.5.1.1 Positive integers x &lt;- c(2.1, 4.2, 3.3, 5.4) x ## [1] 2.1 4.2 3.3 5.4 x[1] ## [1] 2.1 x[c(3, 1)] ## [1] 3.3 2.1 # `order(x)` gives the positions of smallest to largest values. order(x) ## [1] 1 3 2 4 x[order(x)] ## [1] 2.1 3.3 4.2 5.4 x[c(1, 3, 2, 4)] ## [1] 2.1 3.3 4.2 5.4 # Duplicated indices yield duplicated values x[c(1, 1)] ## [1] 2.1 2.1 4.5.1.2 Negative integers x &lt;- c(2.1, 4.2, 3.3, 5.4) x[-1] ## [1] 4.2 3.3 5.4 x[-c(3, 1)] ## [1] 4.2 5.4 You can’t mix positive and negative integers in a single subset: x &lt;- c(2.1, 4.2, 3.3, 5.4) x[c(-1, 2)] ## Error in x[c(-1, 2)]: only 0&#39;s may be mixed with negative subscripts 4.5.1.3 Logical vectors x &lt;- c(2.1, 4.2, 3.3, 5.4) x[c(TRUE, TRUE, FALSE, FALSE)] ## [1] 2.1 4.2 This is probably the most useful type of subsetting because you write the expression that creates the logical vector x &lt;- c(2.1, 4.2, 3.3, 5.4) # this returns a logical vector x &gt; 3 ## [1] FALSE TRUE TRUE TRUE x ## [1] 2.1 4.2 3.3 5.4 # use a conditional statement to create an implicit logical vector x[x &gt; 3] ## [1] 4.2 3.3 5.4 You can combine conditional statements with &amp; (and), | (or), and ! (not) x &lt;- c(2.1, 4.2, 3.3, 5.4) # combing two conditional statements with &amp; x &gt; 3 &amp; x &lt; 5 ## [1] FALSE TRUE TRUE FALSE x[x &gt; 3 &amp; x &lt; 5] ## [1] 4.2 3.3 # combing two conditional statements with | x &lt; 3 | x &gt; 5 ## [1] TRUE FALSE FALSE TRUE x[x &lt; 3 | x &gt; 5] ## [1] 2.1 5.4 # combining conditional statements with ! !x &gt; 5 ## [1] TRUE TRUE TRUE FALSE x[!x &gt; 5] ## [1] 2.1 4.2 3.3 Another way to generate implicit conditional statements is using the %in% operator, which works like the in keywords in Python. # generate implicit logical vectors through the %in% operator x %in% c(3.3, 4.2) ## [1] FALSE TRUE TRUE FALSE x ## [1] 2.1 4.2 3.3 5.4 x[x %in% c(3.3, 4.2)] ## [1] 4.2 3.3 4.5.1.4 Character vectors x &lt;- c(2.1, 4.2, 3.3, 5.4) # apply names names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) x ## a b c d ## 2.1 4.2 3.3 5.4 # subset using names x[c(&quot;d&quot;, &quot;c&quot;, &quot;a&quot;)] ## d c a ## 5.4 3.3 2.1 # Like integer indices, you can repeat indices x[c(&quot;a&quot;, &quot;a&quot;, &quot;a&quot;)] ## a a a ## 2.1 2.1 2.1 # Careful! names are always matched exactly x &lt;- c(abc = 1, def = 2) x ## abc def ## 1 2 x[c(&quot;a&quot;, &quot;d&quot;)] ## &lt;NA&gt; &lt;NA&gt; ## NA NA 4.5.1.4.0.1 More on string operations firstName &lt;- &quot;Jae Yeon&quot; lastName &lt;- &quot;Kim&quot; Unlike in Python, R does not have a reserved operator for string concatenation such as +. Furthermore, using the usual concatenation operator c() on two or more character strings will not create a single character string, but rather a vector of character strings. fullName &lt;- c(firstName, lastName) print(fullName) ## [1] &quot;Jae Yeon&quot; &quot;Kim&quot; length(fullName) ## [1] 2 In order to combine two or more character strings into one larger character string, we use the paste() function. This function takes character strings or vectors and collapses their values into a single character string, with each value separated by a character string selected by the user. fullName &lt;- paste(firstName, lastName) print(fullName) fullName &lt;- paste(firstName, lastName, sep = &quot;+&quot;) print(fullName) fullName &lt;- paste(firstName, lastName, sep = &quot;___&quot;) print(fullName) As with Python, R can also extract substrings based on the index position of its characters. There are, however, two critical differences. First, index positions in R start at 1. This is in contrast to Python, where indexation starts at 0. Second, object subsets using index positions in R contain all the elements in the specified range. If some object called data contains five elements, data[2:4] will return the elements at the second, third, and fourth positions. By contrast, the same subset in Python would return the objects at the third and fourth positions (or second and third positions, depending upon whether your index starts at 0 or 1). Third, R does not allow indexing of character strings*. Instead, you must use the substr() function. Note that this function must receive both the start and stop arguments. So if you want to get all the characters between some index and the end of the string, you must make use of the nchar() function, which will tell you the length of a character string. fullName &lt;- paste(firstName, lastName) # this won&#39;t work like in Python fullName[1] # R sees the string as a unitary object - it can&#39;t be indexed this way ## [1] &quot;Jae Yeon Kim&quot; fullName[1:4] ## [1] &quot;Jae Yeon Kim&quot; NA NA NA # So use this instead substr(x = fullName, start = 1, stop = 2) ## [1] &quot;Ja&quot; substr(x = fullName, start = 5, stop = 5) ## [1] &quot;Y&quot; substr(x = fullName, start = 1, stop = 10) ## [1] &quot;Jae Yeon K&quot; substr(x = fullName, start = 11, stop = nchar(fullName)) ## [1] &quot;im&quot; Like Python, R has a number of string methods, though these exist as individual rather than “mix-and-match” functions. For example: toupper(x = fullName) ## [1] &quot;JAE YEON KIM&quot; tolower(x = fullName) ## [1] &quot;jae yeon kim&quot; strsplit(x = fullName, split = &quot; &quot;) ## [[1]] ## [1] &quot;Jae&quot; &quot;Yeon&quot; &quot;Kim&quot; strsplit(x = fullName, split = &quot;n&quot;) ## [[1]] ## [1] &quot;Jae Yeo&quot; &quot; Kim&quot; gsub(pattern = &quot;Kim&quot;, replacement = &quot;Choi&quot;, x = fullName) ## [1] &quot;Jae Yeon Choi&quot; gsub(pattern = &quot;Jae Yeon&quot;, replacement = &quot;Danny&quot;, x = fullName) ## [1] &quot;Danny Kim&quot; # Note the importance of cases! This doesn&#39;t throw an error, so you won&#39;t realize your function didn&#39;t work unless you double-check several entries gsub(pattern = &quot; &quot;, replacement = &quot;&quot;, x = fullName) # The same function is used for replacements and stripping ## [1] &quot;JaeYeonKim&quot; 4.5.2 Lists Subsetting a list works in the same way as subsetting an atomic vector. Using [ will always return a list; [[ and $, as described below, let you pull out the components of the list. l &lt;- list(&quot;a&quot; = 1, &quot;b&quot; = 2) l ## $a ## [1] 1 ## ## $b ## [1] 2 l[1] ## $a ## [1] 1 l[[1]] ## [1] 1 l[&quot;a&quot;] ## $a ## [1] 1 4.5.3 Matrices The most common way of subsetting matrices (2d) is a simple generalization of 1d subsetting: you supply a 1d index for each dimension, separated by a comma. Blank subsetting is now useful because it lets you keep all rows or all columns. a &lt;- matrix(1:9, nrow = 3) colnames(a) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) a ## A B C ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 # rows come first, then columns a[c(1, 2), ] ## A B C ## [1,] 1 4 7 ## [2,] 2 5 8 a[c(T, F, T), c(&quot;B&quot;, &quot;A&quot;)] ## B A ## [1,] 4 1 ## [2,] 6 3 a[0, -2] ## A C a[c(1, 2), -2] ## A C ## [1,] 1 7 ## [2,] 2 8 4.5.4 Data frames Data from data frames can be addressed like matrices (with row and column indicators separated by a comma). df &lt;- data.frame(x = 4:6, y = 3:1, z = letters[1:3]) df ## x y z ## 1 4 3 a ## 2 5 2 b ## 3 6 1 c # return only the rows where x == 6 df[df$x == 6, ] ## x y z ## 3 6 1 c # return the first and third row df[c(1, 3), ] ## x y z ## 1 4 3 a ## 3 6 1 c # return the first and third row, and the first and second column df[c(1, 3), c(1, 2)] ## x y ## 1 4 3 ## 3 6 1 Data frames possess the characteristics of both lists and matrices: if you subset with a single vector, they behave like lists, and return only the columns. # There are two ways to select columns from a data frame # Like a list: df[c(&quot;x&quot;, &quot;z&quot;)] ## x z ## 1 4 a ## 2 5 b ## 3 6 c # Like a matrix df[, c(&quot;x&quot;, &quot;z&quot;)] ## x z ## 1 4 a ## 2 5 b ## 3 6 c But there’s an important difference when you select a single column: matrix subsetting simplifies by default, list subsetting does not. (df[&quot;x&quot;]) ## x ## 1 4 ## 2 5 ## 3 6 class((df[&quot;x&quot;])) ## [1] &quot;data.frame&quot; (df[, &quot;x&quot;]) ## [1] 4 5 6 class((df[, &quot;x&quot;])) ## [1] &quot;integer&quot; See the bottom section on Simplying and Preserving to know more 4.5.5 Subsetting operators There are two other subsetting operators: [[ and $. [[ is similar to [, except it can only return a single value and it allows you to pull pieces out of a list. $ is a useful shorthand for [[ combined with character subsetting. 4.5.5.0.1 [[ You need [[ when working with lists. This is because when [ is applied to a list it always returns a list: it never gives you the contents of the list. To get the contents, you need [[: “If list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.” — (RLangTip?) Because data frames are lists of columns, you can use [[ to extract a column from data frames: mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 # these two are equivalent mtcars[[1]] ## [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 ## [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 ## [31] 15.0 21.4 mtcars[, 1] ## [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 ## [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 ## [31] 15.0 21.4 # which differs from this: mtcars[1] ## mpg ## Mazda RX4 21.0 ## Mazda RX4 Wag 21.0 ## Datsun 710 22.8 ## Hornet 4 Drive 21.4 ## Hornet Sportabout 18.7 ## Valiant 18.1 ## Duster 360 14.3 ## Merc 240D 24.4 ## Merc 230 22.8 ## Merc 280 19.2 ## Merc 280C 17.8 ## Merc 450SE 16.4 ## Merc 450SL 17.3 ## Merc 450SLC 15.2 ## Cadillac Fleetwood 10.4 ## Lincoln Continental 10.4 ## Chrysler Imperial 14.7 ## Fiat 128 32.4 ## Honda Civic 30.4 ## Toyota Corolla 33.9 ## Toyota Corona 21.5 ## Dodge Challenger 15.5 ## AMC Javelin 15.2 ## Camaro Z28 13.3 ## Pontiac Firebird 19.2 ## Fiat X1-9 27.3 ## Porsche 914-2 26.0 ## Lotus Europa 30.4 ## Ford Pantera L 15.8 ## Ferrari Dino 19.7 ## Maserati Bora 15.0 ## Volvo 142E 21.4 4.5.5.0.2 $ $ is a shorthand operator, where x$y is equivalent to x[[\"y\", exact = FALSE]]. It’s often used to access variables in a data frame: # these two are equivalent mtcars[[&quot;cyl&quot;]] ## [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 mtcars$cyl ## [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 One common mistake with $ is to try and use it when you have the name of a column stored in a variable: var &lt;- &quot;cyl&quot; # Doesn&#39;t work - mtcars$var translated to mtcars[[&quot;var&quot;]] mtcars$var ## NULL # Instead use [[ mtcars[[var]] ## [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 4.5.6 Subassignment All subsetting operators can be combined with assignment to modify selected values of the input vector. x &lt;- 1:5 x ## [1] 1 2 3 4 5 x[c(1, 2)] &lt;- 2:3 x ## [1] 2 3 3 4 5 # The length of the LHS needs to match the RHS! x[-1] &lt;- 4:1 x ## [1] 2 4 3 2 1 x[1] &lt;- 4:1 ## Warning in x[1] &lt;- 4:1: number of items to replace is not a multiple of ## replacement length # This is mostly useful when conditionally modifying vectors df &lt;- data.frame(a = c(1, 10, NA)) df ## a ## 1 1 ## 2 10 ## 3 NA df$a[df$a &lt; 5] &lt;- 0 df ## a ## 1 0 ## 2 10 ## 3 NA 4.6 Tidyverse I adapted the following content from Wickham’s R for Data Science, his earlier paper published in the Journal of Statistical Software, Efficient R Programming by Gillespie and Lovelace, and R Programming for Data Science by Roger P. Peng. Tidyverse design guide Human centered Consistent Composable (modualized) Inclusive Influenced by the Basics of the Unix Philosophy, The Zen of Python, and the Design Principles Behind Smalltalk 4.7 Tidy data “Tidy data sets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.” - Hadley Wickham Variables -&gt; Columns Observations -&gt; Rows Values -&gt; Cells Tidy Data Example (Source: R for Data Science) If dataframes are tidy, it’s easy to transform, visualize, model, and program them using tidyverse packages (a whole workflow). Tidyverse: an opinionated collection of R packages Nevertheless, don’t be religious. In summary, tidy data is a useful conceptual idea and is often the right way to go for general, small data sets, but may not be appropriate for all problems. - Jeff Leek For instance, in many data science applications, linear algebra-based computations are essential (e.g., Principal Component Analysis). These computations are optimized to work on matrices, not tidy data frames (for more information, read Jeff Leek’s blog post). This is what a tidy data looks like. library(tidyverse) table1 ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Additional tips There are so many different ways of taking a look at data in R. Can you discuss the pros and cons of each approach? Which one do you prefer and why? str(table1) glimpse(table1): similar to str() cleaner output skim(table1): str() + summary() + more The big picture Tidying data with tidyr Processing data with dplyr These two packages don’t do anything new, but simplify most common tasks in data manipulation. Plus, they are fast, consistent, and more readable. Practically, this approach is good because you’re going to have consistency in the format of data across all the projects you’re working on. Also, tidy data works well with key packages (e.g., dplyr, ggplot2) in R. Computationally, this approach is useful for vectorized programming because “different variables from the same observation are always paired.” Vectorized means a function applies to a vector treats each element individually (=operations working in parallel). 4.8 Tidying (tidyr) 4.8.1 Reshaping Signs of messy datasets Column headers are values, not variable names. Multiple variables are not stored in one column. Variables are stored in both rows and columns. Multiple types of observational units are stored in the same table. A single observational unit is stored in multiple tables. Let’s take a look at the cases of untidy data. Messy Data Case 1 (Source: R for Data Science) Make It Longer Col1 Col2 Col3 Challenge: Why this data is not tidy? table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 Let’s pivot (rotate by 90 degree). Concept map for pivoting. By Florian Schmoll, Monica Alonso. pivot_longer() increases the number of rows (longer) and decreases the number of columns. The inverse function is pivot_wider(). These functions improve the usability of gather() and spread(). What pivot_longer() does (Source: https://www.storybench.org) Concept map for pipe operator. By Jeroen Janssens, Monica Alonso. The pipe operator %&gt;% originally comes from the magrittr package. The idea behind the pipe operator is similar to what we learned about chaining functions in high school. f: B -&gt; C and g: A -&gt; B can be expressed as \\(f(g(x))\\). Basically, the pipe operator chains operations. When you read pipe operator, read as “and then” (Wickham’s recommendation). The keyboard shortcut is ctrl + shift + M. The key idea here is not creating temporary variables and focusing on verbs (functions). We’ll learn more about this functional programming paradigm later on. table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 # Old way, less intuitive table4a %&gt;% gather( key = &quot;year&quot;, # Current column names value = &quot;cases&quot;, # The values matched to cases c(&quot;1999&quot;, &quot;2000&quot;) ) # Selected columns ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Brazil 1999 37737 ## 3 China 1999 212258 ## 4 Afghanistan 2000 2666 ## 5 Brazil 2000 80488 ## 6 China 2000 213766 # New way, more intuitive table4a %&gt;% pivot_longer( cols = c(&quot;1999&quot;, &quot;2000&quot;), # Selected columns names_to = &quot;year&quot;, # Shorter columns (the columns going to be in one column called year) values_to = &quot;cases&quot; ) # Longer rows (the values are going to be in a separate column called named cases) ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 There’s another problem, did you catch it? The data type of year variable should be numeric not character. By default, pivot_longer() transforms uninformative columns to character. You can fix this problem by using names_transform argument. table4a %&gt;% pivot_longer( cols = c(&quot;1999&quot;, &quot;2000&quot;), # Put two columns together names_to = &quot;year&quot;, # Shorter columns (the columns going to be in one column called year) values_to = &quot;cases&quot;, # Longer rows (the values are going to be in a separate column called named cases) names_transform = list(year = readr::parse_number) ) # Transform the variable ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 Additional tips parse_number() also keeps only numeric information in a variable. parse_number(&quot;reply1994&quot;) ## [1] 1994 A flat file (e.g., CSV) is a rectangular shaped combination of strings. Parsing determines the type of each column and turns into a vector of a more specific type. Tidyverse has parse_ functions (from readr package) that are flexible and fast (e.g., parse_integer(), parse_double(), parse_logical(), parse_datetime(), parse_date(), parse_time(), parse_factor(), etc). Let’s do another practice. Challenge Why this data is not tidy? (This exercise comes from pivot function vigenette.) Too long or too wide? billboard ## # A tibble: 317 x 79 ## artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Pac Baby… 2000-02-26 87 82 72 77 87 94 99 NA ## 2 2Ge+h… The … 2000-09-02 91 87 92 NA NA NA NA NA ## 3 3 Doo… Kryp… 2000-04-08 81 70 68 67 66 57 54 53 ## 4 3 Doo… Loser 2000-10-21 76 76 72 69 67 65 55 59 ## 5 504 B… Wobb… 2000-04-15 57 34 25 17 17 31 36 49 ## 6 98^0 Give… 2000-08-19 51 39 34 26 26 19 2 2 ## 7 A*Tee… Danc… 2000-07-08 97 97 96 95 100 NA NA NA ## 8 Aaliy… I Do… 2000-01-29 84 62 51 41 38 35 35 38 ## 9 Aaliy… Try … 2000-03-18 59 53 38 28 21 18 16 14 ## 10 Adams… Open… 2000-08-26 76 76 74 69 68 67 61 58 ## # … with 307 more rows, and 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, ## # wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;, wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, ## # wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;, wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, ## # wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;, wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, ## # wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;, wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, ## # wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;, wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, ## # wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, ## # wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, wk49 &lt;dbl&gt;, wk50 &lt;dbl&gt;, wk51 &lt;dbl&gt;, wk52 &lt;dbl&gt;, ## # wk53 &lt;dbl&gt;, wk54 &lt;dbl&gt;, wk55 &lt;dbl&gt;, wk56 &lt;dbl&gt;, wk57 &lt;dbl&gt;, wk58 &lt;dbl&gt;, ## # wk59 &lt;dbl&gt;, wk60 &lt;dbl&gt;, wk61 &lt;dbl&gt;, wk62 &lt;dbl&gt;, wk63 &lt;dbl&gt;, wk64 &lt;dbl&gt;, ## # wk65 &lt;dbl&gt;, wk66 &lt;lgl&gt;, wk67 &lt;lgl&gt;, wk68 &lt;lgl&gt;, wk69 &lt;lgl&gt;, wk70 &lt;lgl&gt;, ## # wk71 &lt;lgl&gt;, wk72 &lt;lgl&gt;, wk73 &lt;lgl&gt;, wk74 &lt;lgl&gt;, wk75 &lt;lgl&gt;, wk76 &lt;lgl&gt; How can you fix it? Which pivot? # Old way billboard %&gt;% gather( key = &quot;week&quot;, value = &quot;rank&quot;, starts_with(&quot;wk&quot;) ) %&gt;% # Use regular expressions drop_na() # Drop NAs ## # A tibble: 5,307 x 5 ## artist track date.entered week rank ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk1 87 ## 2 2Ge+her The Hardest Part Of ... 2000-09-02 wk1 91 ## 3 3 Doors Down Kryptonite 2000-04-08 wk1 81 ## 4 3 Doors Down Loser 2000-10-21 wk1 76 ## 5 504 Boyz Wobble Wobble 2000-04-15 wk1 57 ## 6 98^0 Give Me Just One Nig... 2000-08-19 wk1 51 ## 7 A*Teens Dancing Queen 2000-07-08 wk1 97 ## 8 Aaliyah I Don&#39;t Wanna 2000-01-29 wk1 84 ## 9 Aaliyah Try Again 2000-03-18 wk1 59 ## 10 Adams, Yolanda Open My Heart 2000-08-26 wk1 76 ## # … with 5,297 more rows Note that pivot_longer() is more versatile than gather(). # New way billboard %&gt;% pivot_longer( cols = starts_with(&quot;wk&quot;), # Use regular expressions names_to = &quot;week&quot;, values_to = &quot;rank&quot;, values_drop_na = TRUE # Drop NAs ) ## # A tibble: 5,307 x 5 ## artist track date.entered week rank ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk1 87 ## 2 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk2 82 ## 3 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk3 72 ## 4 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk4 77 ## 5 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk5 87 ## 6 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk6 94 ## 7 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk7 99 ## 8 2Ge+her The Hardest Part Of ... 2000-09-02 wk1 91 ## 9 2Ge+her The Hardest Part Of ... 2000-09-02 wk2 87 ## 10 2Ge+her The Hardest Part Of ... 2000-09-02 wk3 92 ## # … with 5,297 more rows Make It Wider Why this data is not tidy? table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 Each observation is spread across two rows. How can you fix it?: pivot_wider(). Two differences between pivot_longer() and pivot_wider() In pivot_longer(), the arguments are named names_to and values_to (to). In pivot_wider(), this pattern is opposite. The arguments are named names_from and values_from (from). The number of required arguments for pivot_longer() is 3 (col, names_to, values_to). The number of required arguments for pivot_wider() is 2 (names_from, values_from). What pivot_wider() does (Source: https://www.storybench.org) # Old way table2 %&gt;% spread( key = type, value = count ) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 # New way table2 %&gt;% pivot_wider( names_from = type, # first values_from = count # second ) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Sometimes, a consultee came to me and asked: “I don’t have missing values in my original dataframe. Then R said that I have missing values after I’ve done some data transformations. What happened?” Here’s an answer. R defines missing values in two ways. Implicit missing values: simply not present in the data. Explicit missing values: flagged with NA Challenge The example comes from R for Data Science. stocks &lt;- tibble( year = c(2019, 2019, 2019, 2020, 2020, 2020), qtr = c(1, 2, 3, 2, 3, 4), return = c(1, 2, 3, NA, 2, 3) ) stocks ## # A tibble: 6 x 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2019 1 1 ## 2 2019 2 2 ## 3 2019 3 3 ## 4 2020 2 NA ## 5 2020 3 2 ## 6 2020 4 3 Where is explicit missing value? Does stocks have implicit missing values? # implicit missing values become explicit stocks %&gt;% pivot_wider( names_from = year, values_from = return ) ## # A tibble: 4 x 3 ## qtr `2019` `2020` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 NA ## 2 2 2 NA ## 3 3 3 2 ## 4 4 NA 3 Challenge This exercise comes from pivot function vigenette. Could you make station a series of dummy variables using pivot_wider()? fish_encounters ## # A tibble: 114 x 3 ## fish station seen ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 4842 Release 1 ## 2 4842 I80_1 1 ## 3 4842 Lisbon 1 ## 4 4842 Rstr 1 ## 5 4842 Base_TD 1 ## 6 4842 BCE 1 ## 7 4842 BCW 1 ## 8 4842 BCE2 1 ## 9 4842 BCW2 1 ## 10 4842 MAE 1 ## # … with 104 more rows Which pivot you should use? Are there explicit missing values? How could you turn these NAs into 0s? Check values_fill argument in the pivot_wider() function. Separate Messy Data Case 2 (Source: R for Data Science) # Toy example df &lt;- data.frame(x = c(NA, &quot;Dad.apple&quot;, &quot;Mom.orange&quot;, &quot;Daughter.banana&quot;)) df ## x ## 1 &lt;NA&gt; ## 2 Dad.apple ## 3 Mom.orange ## 4 Daughter.banana # Separate df %&gt;% separate(x, into = c(&quot;Name&quot;, &quot;Preferred_fruit&quot;)) ## Name Preferred_fruit ## 1 &lt;NA&gt; &lt;NA&gt; ## 2 Dad apple ## 3 Mom orange ## 4 Daughter banana # Don&#39;t need the first variable df %&gt;% separate(x, into = c(NA, &quot;Preferred_fruit&quot;)) ## Preferred_fruit ## 1 &lt;NA&gt; ## 2 apple ## 3 orange ## 4 banana Practice table3 ## # A tibble: 6 x 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 Note sep argument. You can specify how to separate joined values. table3 %&gt;% separate(rate, into = c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot; ) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Note convert argument. You can specify whether automatically convert the new values or not. table3 %&gt;% separate(rate, into = c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;, convert = TRUE ) # cases and population become integers ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Unite pivot_longer() &lt;-&gt; pivot_wider() separate() &lt;-&gt; unite() # Create a toy example df &lt;- data.frame( name = c(&quot;Jae&quot;, &quot;Sun&quot;, &quot;Jane&quot;, NA), birthmonth = c(&quot;April&quot;, &quot;April&quot;, &quot;June&quot;, NA) ) # Include missing values df %&gt;% unite( &quot;contact&quot;, c(&quot;name&quot;, &quot;birthmonth&quot;) ) ## contact ## 1 Jae_April ## 2 Sun_April ## 3 Jane_June ## 4 NA_NA # Do not include missing values df %&gt;% unite(&quot;contact&quot;, c(&quot;name&quot;, &quot;birthmonth&quot;), na.rm = TRUE ) ## contact ## 1 Jae_April ## 2 Sun_April ## 3 Jane_June ## 4 4.8.2 Filling This is a relatively less-known function of the tidyr package. I found this function super useful to complete time-series data. For instance, how can you replace NA in the following example (this use case is drawn from the tidyr package vignette.)? # Example stock &lt;- tibble::tribble( ~ quarter, ~ year, ~stock_price, &quot;Q1&quot;, 2000, 10000, &quot;Q2&quot;, NA, 10001, # Replace NA with 2000 &quot;Q3&quot;, NA, 10002, # Replace NA with 2000 &quot;Q4&quot;, NA, 10003, # Replace NA with 2000 &quot;Q1&quot;, 2001, 10004, &quot;Q2&quot;, NA, 10005, # Replace NA with 2001 &quot;Q3&quot;, NA, 10006, # Replace NA with 2001 &quot;Q4&quot;, NA, 10007, # Replace NA with 2001 ) fill(stock, year) ## # A tibble: 8 x 3 ## quarter year stock_price ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Q1 2000 10000 ## 2 Q2 2000 10001 ## 3 Q3 2000 10002 ## 4 Q4 2000 10003 ## 5 Q1 2001 10004 ## 6 Q2 2001 10005 ## 7 Q3 2001 10006 ## 8 Q4 2001 10007 Let’s take a slightly more complex example. # Example yelp_rate &lt;- tibble::tribble( ~ neighborhood, ~restraurant_type, ~popularity_rate, &quot;N1&quot;, &quot;Chinese&quot;, 5, &quot;N2&quot;, NA, 4, &quot;N3&quot;, NA, 3, &quot;N4&quot;, NA, 2, &quot;N1&quot;, &quot;Indian&quot;, 1, &quot;N2&quot;, NA, 2, &quot;N3&quot;, NA, 3, &quot;N4&quot;, NA, 4, &quot;N1&quot;, &quot;Mexican&quot;, 5 ) fill(yelp_rate, restraurant_type) # default is direction = .down ## # A tibble: 9 x 3 ## neighborhood restraurant_type popularity_rate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 N1 Chinese 5 ## 2 N2 Chinese 4 ## 3 N3 Chinese 3 ## 4 N4 Chinese 2 ## 5 N1 Indian 1 ## 6 N2 Indian 2 ## 7 N3 Indian 3 ## 8 N4 Indian 4 ## 9 N1 Mexican 5 fill(yelp_rate, restraurant_type, .direction = &quot;up&quot;) ## # A tibble: 9 x 3 ## neighborhood restraurant_type popularity_rate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 N1 Chinese 5 ## 2 N2 Indian 4 ## 3 N3 Indian 3 ## 4 N4 Indian 2 ## 5 N1 Indian 1 ## 6 N2 Mexican 2 ## 7 N3 Mexican 3 ## 8 N4 Mexican 4 ## 9 N1 Mexican 5 4.9 Manipulating (dplyr) Concept map for dplyr. By Monica Alonso, Greg Wilson. dplyr is better than the base R approaches to data processing: fast to run (due to the C++ backed) and intuitive to type works well with tidy data and databases (thanks to dbplyr) 4.9.1 Rearranging Arrange Order rows dplyr::arrange(mtcars, mpg) # Low to High (default) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 dplyr::arrange(mtcars, desc(mpg)) # High to Row ## mpg cyl disp hp drat wt qsec vs am gear carb ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Rename Rename columns df &lt;- tibble(y = c(2011, 2012, 2013)) df %&gt;% rename( Year = # NEW name y ) # OLD name ## # A tibble: 3 x 1 ## Year ## &lt;dbl&gt; ## 1 2011 ## 2 2012 ## 3 2013 4.9.2 Subset observations (rows) Choose row by logical condition Single condition starwars %&gt;% filter(gender == &quot;feminine&quot;) %&gt;% arrange(desc(height)) ## # A tibble: 17 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Taun… 213 NA none grey black NA fema… femin… ## 2 Adi … 184 50 none dark blue NA fema… femin… ## 3 Ayla… 178 55 none blue hazel 48 fema… femin… ## 4 Shaa… 178 57 none red, blue… black NA fema… femin… ## 5 Lumi… 170 56.2 black yellow blue 58 fema… femin… ## 6 Zam … 168 55 blonde fair, gre… yellow NA fema… femin… ## 7 Joca… 167 NA white fair blue NA fema… femin… ## 8 Barr… 166 50 black yellow blue 40 fema… femin… ## 9 Beru… 165 75 brown light blue 47 fema… femin… ## 10 Dormé 165 NA brown light brown NA fema… femin… ## 11 Padm… 165 45 brown light brown 46 fema… femin… ## 12 Shmi… 163 NA black fair brown 72 fema… femin… ## 13 Cordé 157 NA brown light brown NA fema… femin… ## 14 Leia… 150 49 brown light brown 19 fema… femin… ## 15 Mon … 150 NA auburn fair blue 48 fema… femin… ## 16 R4-P… 96 NA none silver, r… red, blue NA none femin… ## 17 Rey NA NA brown light hazel NA fema… femin… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; The following filtering example was inspired by the suzanbert’s dplyr blog post. Multiple conditions (numeric) # First example starwars %&gt;% filter(height &lt; 180, height &gt; 160) %&gt;% nrow() ## [1] 24 # Same as above starwars %&gt;% filter(height &lt; 180 &amp; height &gt; 160) %&gt;% nrow() ## [1] 24 # Not same as above starwars %&gt;% filter(height &lt; 180 | height &gt; 160) %&gt;% nrow() ## [1] 81 Challenge Use filter(between()) to find characters whose heights are between 180 and 160 and (2) count the number of these observations. Minimum reproducible example df &lt;- tibble( heights = c(160:180), char = rep(&quot;none&quot;, length(c(160:180))) ) df %&gt;% filter(between(heights, 161, 179)) ## # A tibble: 19 x 2 ## heights char ## &lt;int&gt; &lt;chr&gt; ## 1 161 none ## 2 162 none ## 3 163 none ## 4 164 none ## 5 165 none ## 6 166 none ## 7 167 none ## 8 168 none ## 9 169 none ## 10 170 none ## 11 171 none ## 12 172 none ## 13 173 none ## 14 174 none ## 15 175 none ## 16 176 none ## 17 177 none ## 18 178 none ## 19 179 none Multiple conditions (character) # Filter names include ars; `grepl` is a base R function starwars %&gt;% filter(grepl(&quot;ars&quot;, tolower(name))) ## # A tibble: 4 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Owen… 178 120 brown, gr… light blue 52 male mascu… ## 2 Beru… 165 75 brown light blue 47 fema… femin… ## 3 Quar… 183 NA black dark brown 62 &lt;NA&gt; &lt;NA&gt; ## 4 Clie… 183 NA brown fair blue 82 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # Or, if you prefer dplyr way starwars %&gt;% filter(str_detect(tolower(name), &quot;ars&quot;)) ## # A tibble: 4 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Owen… 178 120 brown, gr… light blue 52 male mascu… ## 2 Beru… 165 75 brown light blue 47 fema… femin… ## 3 Quar… 183 NA black dark brown 62 &lt;NA&gt; &lt;NA&gt; ## 4 Clie… 183 NA brown fair blue 82 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # Filter brown and black hair_color starwars %&gt;% filter(hair_color %in% c(&quot;black&quot;, &quot;brown&quot;)) ## # A tibble: 31 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Leia… 150 49 brown light brown 19 fema… femin… ## 2 Beru… 165 75 brown light blue 47 fema… femin… ## 3 Bigg… 183 84 black light brown 24 male mascu… ## 4 Chew… 228 112 brown unknown blue 200 male mascu… ## 5 Han … 180 80 brown fair brown 29 male mascu… ## 6 Wedg… 170 77 brown fair hazel 21 male mascu… ## 7 Jek … 180 110 brown fair blue NA male mascu… ## 8 Boba… 183 78.2 black fair brown 31.5 male mascu… ## 9 Land… 177 79 black dark brown 31 male mascu… ## 10 Arve… NA NA brown fair brown NA male mascu… ## # … with 21 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; Challenge Use str_detect() to find characters whose names include “Han.” Choose row by position (row index) starwars %&gt;% arrange(desc(height)) %&gt;% slice(1:6) ## # A tibble: 6 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Yara… 264 NA none white yellow NA male mascu… ## 2 Tarf… 234 136 brown brown blue NA male mascu… ## 3 Lama… 229 88 none grey black NA male mascu… ## 4 Chew… 228 112 brown unknown blue 200 male mascu… ## 5 Roos… 224 82 none grey orange NA male mascu… ## 6 Grie… 216 159 none brown, wh… green, y… NA male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Sample by fraction # For reproducibility set.seed(1234) # Old way starwars %&gt;% sample_frac(0.10, replace = FALSE ) # Without replacement ## # A tibble: 9 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Arve… NA NA brown fair brown NA male mascu… ## 2 Sly … 178 48 none pale white NA &lt;NA&gt; &lt;NA&gt; ## 3 IG-88 200 140 none metal red 15 none mascu… ## 4 Bigg… 183 84 black light brown 24 male mascu… ## 5 Leia… 150 49 brown light brown 19 fema… femin… ## 6 Watto 137 NA black blue, grey yellow NA male mascu… ## 7 Jabb… 175 1358 &lt;NA&gt; green-tan… orange 600 herm… mascu… ## 8 Dart… 202 136 none white yellow 41.9 male mascu… ## 9 Taun… 213 NA none grey black NA fema… femin… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # New way starwars %&gt;% slice_sample( prop = 0.10, replace = FALSE ) ## # A tibble: 8 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Raym… 188 79 brown light brown NA male mascu… ## 2 Tarf… 234 136 brown brown blue NA male mascu… ## 3 Han … 180 80 brown fair brown 29 male mascu… ## 4 Mas … 196 NA none blue blue NA male mascu… ## 5 Barr… 166 50 black yellow blue 40 fema… femin… ## 6 Dart… 202 136 none white yellow 41.9 male mascu… ## 7 Finn NA NA black dark dark NA male mascu… ## 8 Boba… 183 78.2 black fair brown 31.5 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Sample by number # Old way starwars %&gt;% sample_n(20, replace = FALSE ) # Without replacement ## # A tibble: 20 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Quar… 183 NA black dark brown 62 &lt;NA&gt; &lt;NA&gt; ## 2 Poe … NA NA brown light brown NA male mascu… ## 3 Mas … 196 NA none blue blue NA male mascu… ## 4 Zam … 168 55 blonde fair, gre… yellow NA fema… femin… ## 5 Leia… 150 49 brown light brown 19 fema… femin… ## 6 Jang… 183 79 black tan brown 66 male mascu… ## 7 Ben … 163 65 none grey, gre… orange NA male mascu… ## 8 Padm… 165 45 brown light brown 46 fema… femin… ## 9 Mace… 188 84 none dark brown 72 male mascu… ## 10 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 11 Shmi… 163 NA black fair brown 72 fema… femin… ## 12 Ratt… 79 15 none grey, blue unknown NA male mascu… ## 13 Nute… 191 90 none mottled g… red NA male mascu… ## 14 Dart… 175 80 none red yellow 54 male mascu… ## 15 Bib … 180 NA none pale pink NA male mascu… ## 16 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… ## 17 Yara… 264 NA none white yellow NA male mascu… ## 18 Ki-A… 198 82 white pale yellow 92 male mascu… ## 19 BB8 NA NA none none black NA none mascu… ## 20 Eeth… 171 NA black brown brown NA male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # New way starwars %&gt;% slice_sample( n = 20, replace = FALSE ) # Without replacement ## # A tibble: 20 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Owen… 178 120 brown, gr… light blue 52 male mascu… ## 2 Ki-A… 198 82 white pale yellow 92 male mascu… ## 3 Capt… NA NA unknown unknown unknown NA &lt;NA&gt; &lt;NA&gt; ## 4 Greg… 185 85 black dark brown NA male mascu… ## 5 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… ## 6 Ackb… 180 83 none brown mot… orange 41 male mascu… ## 7 Wedg… 170 77 brown fair hazel 21 male mascu… ## 8 Dormé 165 NA brown light brown NA fema… femin… ## 9 Rey NA NA brown light hazel NA fema… femin… ## 10 IG-88 200 140 none metal red 15 none mascu… ## 11 Roos… 224 82 none grey orange NA male mascu… ## 12 Shmi… 163 NA black fair brown 72 fema… femin… ## 13 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 14 Poe … NA NA brown light brown NA male mascu… ## 15 Obi-… 182 77 auburn, w… fair blue-gray 57 male mascu… ## 16 Plo … 188 80 none orange black 22 male mascu… ## 17 Tarf… 234 136 brown brown blue NA male mascu… ## 18 Lobot 175 79 none light blue 37 male mascu… ## 19 San … 191 NA none grey gold NA male mascu… ## 20 Kit … 196 87 none green black NA male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Top 10 rows orderd by height # Old way starwars %&gt;% top_n(10, height) ## # A tibble: 10 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Dart… 202 136 none white yellow 41.9 male mascu… ## 2 Chew… 228 112 brown unknown blue 200 male mascu… ## 3 Roos… 224 82 none grey orange NA male mascu… ## 4 Rugo… 206 NA none green orange NA male mascu… ## 5 Yara… 264 NA none white yellow NA male mascu… ## 6 Lama… 229 88 none grey black NA male mascu… ## 7 Taun… 213 NA none grey black NA fema… femin… ## 8 Grie… 216 159 none brown, wh… green, y… NA male mascu… ## 9 Tarf… 234 136 brown brown blue NA male mascu… ## 10 Tion… 206 80 none grey black NA male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # New way starwars %&gt;% slice_max(height, n = 10) # Variable first, Argument second ## # A tibble: 10 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Yara… 264 NA none white yellow NA male mascu… ## 2 Tarf… 234 136 brown brown blue NA male mascu… ## 3 Lama… 229 88 none grey black NA male mascu… ## 4 Chew… 228 112 brown unknown blue 200 male mascu… ## 5 Roos… 224 82 none grey orange NA male mascu… ## 6 Grie… 216 159 none brown, wh… green, y… NA male mascu… ## 7 Taun… 213 NA none grey black NA fema… femin… ## 8 Rugo… 206 NA none green orange NA male mascu… ## 9 Tion… 206 80 none grey black NA male mascu… ## 10 Dart… 202 136 none white yellow 41.9 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; 4.9.3 Subset variables (columns) names(msleep) ## [1] &quot;name&quot; &quot;genus&quot; &quot;vore&quot; &quot;order&quot; &quot;conservation&quot; ## [6] &quot;sleep_total&quot; &quot;sleep_rem&quot; &quot;sleep_cycle&quot; &quot;awake&quot; &quot;brainwt&quot; ## [11] &quot;bodywt&quot; Select only numeric columns # Only numeric msleep %&gt;% dplyr::select(where(is.numeric)) ## # A tibble: 83 x 6 ## sleep_total sleep_rem sleep_cycle awake brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.1 NA NA 11.9 NA 50 ## 2 17 1.8 NA 7 0.0155 0.48 ## 3 14.4 2.4 NA 9.6 NA 1.35 ## 4 14.9 2.3 0.133 9.1 0.00029 0.019 ## 5 4 0.7 0.667 20 0.423 600 ## 6 14.4 2.2 0.767 9.6 NA 3.85 ## 7 8.7 1.4 0.383 15.3 NA 20.5 ## 8 7 NA NA 17 NA 0.045 ## 9 10.1 2.9 0.333 13.9 0.07 14 ## 10 3 NA NA 21 0.0982 14.8 ## # … with 73 more rows Challenge Use select(where()) to find only non-numeric columns Select the columns that include “sleep” in their names msleep %&gt;% dplyr::select(contains(&quot;sleep&quot;)) ## # A tibble: 83 x 3 ## sleep_total sleep_rem sleep_cycle ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.1 NA NA ## 2 17 1.8 NA ## 3 14.4 2.4 NA ## 4 14.9 2.3 0.133 ## 5 4 0.7 0.667 ## 6 14.4 2.2 0.767 ## 7 8.7 1.4 0.383 ## 8 7 NA NA ## 9 10.1 2.9 0.333 ## 10 3 NA NA ## # … with 73 more rows Select the columns that include either “sleep” or “wt” in their names Basic R way grepl is one of the R base pattern matching functions. msleep[grepl(&quot;sleep|wt&quot;, names(msleep))] ## # A tibble: 83 x 5 ## sleep_total sleep_rem sleep_cycle brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.1 NA NA NA 50 ## 2 17 1.8 NA 0.0155 0.48 ## 3 14.4 2.4 NA NA 1.35 ## 4 14.9 2.3 0.133 0.00029 0.019 ## 5 4 0.7 0.667 0.423 600 ## 6 14.4 2.2 0.767 NA 3.85 ## 7 8.7 1.4 0.383 NA 20.5 ## 8 7 NA NA NA 0.045 ## 9 10.1 2.9 0.333 0.07 14 ## 10 3 NA NA 0.0982 14.8 ## # … with 73 more rows Challenge Use select(match()) to find columns whose names include either “sleep” or “wt.” Select the columns that starts with “b” msleep %&gt;% dplyr::select(starts_with(&quot;b&quot;)) ## # A tibble: 83 x 2 ## brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 NA 50 ## 2 0.0155 0.48 ## 3 NA 1.35 ## 4 0.00029 0.019 ## 5 0.423 600 ## 6 NA 3.85 ## 7 NA 20.5 ## 8 NA 0.045 ## 9 0.07 14 ## 10 0.0982 14.8 ## # … with 73 more rows Select the columns that ends with “wt” msleep %&gt;% dplyr::select(ends_with(&quot;wt&quot;)) ## # A tibble: 83 x 2 ## brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 NA 50 ## 2 0.0155 0.48 ## 3 NA 1.35 ## 4 0.00029 0.019 ## 5 0.423 600 ## 6 NA 3.85 ## 7 NA 20.5 ## 8 NA 0.045 ## 9 0.07 14 ## 10 0.0982 14.8 ## # … with 73 more rows Select the columns using both beginning and end string patterns The key idea is you can use Boolean operators (!, &amp;, |)to combine different string pattern matching statements. msleep %&gt;% dplyr::select(starts_with(&quot;b&quot;) &amp; ends_with(&quot;wt&quot;)) ## # A tibble: 83 x 2 ## brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 NA 50 ## 2 0.0155 0.48 ## 3 NA 1.35 ## 4 0.00029 0.019 ## 5 0.423 600 ## 6 NA 3.85 ## 7 NA 20.5 ## 8 NA 0.045 ## 9 0.07 14 ## 10 0.0982 14.8 ## # … with 73 more rows Select order and move it before everything # By specifying a column msleep %&gt;% dplyr::select(order, everything()) ## # A tibble: 83 x 11 ## order name genus vore conservation sleep_total sleep_rem sleep_cycle awake ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Carn… Chee… Acin… carni lc 12.1 NA NA 11.9 ## 2 Prim… Owl … Aotus omni &lt;NA&gt; 17 1.8 NA 7 ## 3 Rode… Moun… Aplo… herbi nt 14.4 2.4 NA 9.6 ## 4 Sori… Grea… Blar… omni lc 14.9 2.3 0.133 9.1 ## 5 Arti… Cow Bos herbi domesticated 4 0.7 0.667 20 ## 6 Pilo… Thre… Brad… herbi &lt;NA&gt; 14.4 2.2 0.767 9.6 ## 7 Carn… Nort… Call… carni vu 8.7 1.4 0.383 15.3 ## 8 Rode… Vesp… Calo… &lt;NA&gt; &lt;NA&gt; 7 NA NA 17 ## 9 Carn… Dog Canis carni domesticated 10.1 2.9 0.333 13.9 ## 10 Arti… Roe … Capr… herbi lc 3 NA NA 21 ## # … with 73 more rows, and 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt; Select variables from a character vector. msleep %&gt;% dplyr::select(any_of(c(&quot;name&quot;, &quot;order&quot;))) %&gt;% colnames() ## [1] &quot;name&quot; &quot;order&quot; Select the variables named in the character + number pattern msleep$week8 &lt;- NA msleep$week12 &lt;- NA msleep$week_extra &lt;- 0 msleep %&gt;% dplyr::select(num_range(&quot;week&quot;, c(1:12))) ## # A tibble: 83 x 2 ## week8 week12 ## &lt;lgl&gt; &lt;lgl&gt; ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA ## 7 NA NA ## 8 NA NA ## 9 NA NA ## 10 NA NA ## # … with 73 more rows Additional tips msleep data has nicely cleaned column names. But real world data are usually messier. The janitor package is useful to fix this kind of problem. messy_df &lt;- tibble::tribble(~&quot;ColNum1&quot;, ~&quot;COLNUM2&quot;, ~ &quot;COL &amp; NUM3&quot;, 1, 2, 3) messy_df ## # A tibble: 1 x 3 ## ColNum1 COLNUM2 `COL &amp; NUM3` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 3 pacman::p_load(janitor) janitor::clean_names(messy_df) ## # A tibble: 1 x 3 ## col_num1 colnum2 col_num3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 3 janitor::tabyl() is helpful for doing crosstabulation and a nice alternative to table() function. # Frequency table; The default output class is table table(gapminder$country) ## ## Afghanistan Albania Algeria ## 12 12 12 ## Angola Argentina Australia ## 12 12 12 ## Austria Bahrain Bangladesh ## 12 12 12 ## Belgium Benin Bolivia ## 12 12 12 ## Bosnia and Herzegovina Botswana Brazil ## 12 12 12 ## Bulgaria Burkina Faso Burundi ## 12 12 12 ## Cambodia Cameroon Canada ## 12 12 12 ## Central African Republic Chad Chile ## 12 12 12 ## China Colombia Comoros ## 12 12 12 ## Congo, Dem. Rep. Congo, Rep. Costa Rica ## 12 12 12 ## Cote d&#39;Ivoire Croatia Cuba ## 12 12 12 ## Czech Republic Denmark Djibouti ## 12 12 12 ## Dominican Republic Ecuador Egypt ## 12 12 12 ## El Salvador Equatorial Guinea Eritrea ## 12 12 12 ## Ethiopia Finland France ## 12 12 12 ## Gabon Gambia Germany ## 12 12 12 ## Ghana Greece Guatemala ## 12 12 12 ## Guinea Guinea-Bissau Haiti ## 12 12 12 ## Honduras Hong Kong, China Hungary ## 12 12 12 ## Iceland India Indonesia ## 12 12 12 ## Iran Iraq Ireland ## 12 12 12 ## Israel Italy Jamaica ## 12 12 12 ## Japan Jordan Kenya ## 12 12 12 ## Korea, Dem. Rep. Korea, Rep. Kuwait ## 12 12 12 ## Lebanon Lesotho Liberia ## 12 12 12 ## Libya Madagascar Malawi ## 12 12 12 ## Malaysia Mali Mauritania ## 12 12 12 ## Mauritius Mexico Mongolia ## 12 12 12 ## Montenegro Morocco Mozambique ## 12 12 12 ## Myanmar Namibia Nepal ## 12 12 12 ## Netherlands New Zealand Nicaragua ## 12 12 12 ## Niger Nigeria Norway ## 12 12 12 ## Oman Pakistan Panama ## 12 12 12 ## Paraguay Peru Philippines ## 12 12 12 ## Poland Portugal Puerto Rico ## 12 12 12 ## Reunion Romania Rwanda ## 12 12 12 ## Sao Tome and Principe Saudi Arabia Senegal ## 12 12 12 ## Serbia Sierra Leone Singapore ## 12 12 12 ## Slovak Republic Slovenia Somalia ## 12 12 12 ## South Africa Spain Sri Lanka ## 12 12 12 ## Sudan Swaziland Sweden ## 12 12 12 ## Switzerland Syria Taiwan ## 12 12 12 ## Tanzania Thailand Togo ## 12 12 12 ## Trinidad and Tobago Tunisia Turkey ## 12 12 12 ## Uganda United Kingdom United States ## 12 12 12 ## Uruguay Venezuela Vietnam ## 12 12 12 ## West Bank and Gaza Yemen, Rep. Zambia ## 12 12 12 ## Zimbabwe ## 12 # Frequency table (unique value, n, percentage) janitor::tabyl(gapminder$country) ## gapminder$country n percent ## Afghanistan 12 0.007042254 ## Albania 12 0.007042254 ## Algeria 12 0.007042254 ## Angola 12 0.007042254 ## Argentina 12 0.007042254 ## Australia 12 0.007042254 ## Austria 12 0.007042254 ## Bahrain 12 0.007042254 ## Bangladesh 12 0.007042254 ## Belgium 12 0.007042254 ## Benin 12 0.007042254 ## Bolivia 12 0.007042254 ## Bosnia and Herzegovina 12 0.007042254 ## Botswana 12 0.007042254 ## Brazil 12 0.007042254 ## Bulgaria 12 0.007042254 ## Burkina Faso 12 0.007042254 ## Burundi 12 0.007042254 ## Cambodia 12 0.007042254 ## Cameroon 12 0.007042254 ## Canada 12 0.007042254 ## Central African Republic 12 0.007042254 ## Chad 12 0.007042254 ## Chile 12 0.007042254 ## China 12 0.007042254 ## Colombia 12 0.007042254 ## Comoros 12 0.007042254 ## Congo, Dem. Rep. 12 0.007042254 ## Congo, Rep. 12 0.007042254 ## Costa Rica 12 0.007042254 ## Cote d&#39;Ivoire 12 0.007042254 ## Croatia 12 0.007042254 ## Cuba 12 0.007042254 ## Czech Republic 12 0.007042254 ## Denmark 12 0.007042254 ## Djibouti 12 0.007042254 ## Dominican Republic 12 0.007042254 ## Ecuador 12 0.007042254 ## Egypt 12 0.007042254 ## El Salvador 12 0.007042254 ## Equatorial Guinea 12 0.007042254 ## Eritrea 12 0.007042254 ## Ethiopia 12 0.007042254 ## Finland 12 0.007042254 ## France 12 0.007042254 ## Gabon 12 0.007042254 ## Gambia 12 0.007042254 ## Germany 12 0.007042254 ## Ghana 12 0.007042254 ## Greece 12 0.007042254 ## Guatemala 12 0.007042254 ## Guinea 12 0.007042254 ## Guinea-Bissau 12 0.007042254 ## Haiti 12 0.007042254 ## Honduras 12 0.007042254 ## Hong Kong, China 12 0.007042254 ## Hungary 12 0.007042254 ## Iceland 12 0.007042254 ## India 12 0.007042254 ## Indonesia 12 0.007042254 ## Iran 12 0.007042254 ## Iraq 12 0.007042254 ## Ireland 12 0.007042254 ## Israel 12 0.007042254 ## Italy 12 0.007042254 ## Jamaica 12 0.007042254 ## Japan 12 0.007042254 ## Jordan 12 0.007042254 ## Kenya 12 0.007042254 ## Korea, Dem. Rep. 12 0.007042254 ## Korea, Rep. 12 0.007042254 ## Kuwait 12 0.007042254 ## Lebanon 12 0.007042254 ## Lesotho 12 0.007042254 ## Liberia 12 0.007042254 ## Libya 12 0.007042254 ## Madagascar 12 0.007042254 ## Malawi 12 0.007042254 ## Malaysia 12 0.007042254 ## Mali 12 0.007042254 ## Mauritania 12 0.007042254 ## Mauritius 12 0.007042254 ## Mexico 12 0.007042254 ## Mongolia 12 0.007042254 ## Montenegro 12 0.007042254 ## Morocco 12 0.007042254 ## Mozambique 12 0.007042254 ## Myanmar 12 0.007042254 ## Namibia 12 0.007042254 ## Nepal 12 0.007042254 ## Netherlands 12 0.007042254 ## New Zealand 12 0.007042254 ## Nicaragua 12 0.007042254 ## Niger 12 0.007042254 ## Nigeria 12 0.007042254 ## Norway 12 0.007042254 ## Oman 12 0.007042254 ## Pakistan 12 0.007042254 ## Panama 12 0.007042254 ## Paraguay 12 0.007042254 ## Peru 12 0.007042254 ## Philippines 12 0.007042254 ## Poland 12 0.007042254 ## Portugal 12 0.007042254 ## Puerto Rico 12 0.007042254 ## Reunion 12 0.007042254 ## Romania 12 0.007042254 ## Rwanda 12 0.007042254 ## Sao Tome and Principe 12 0.007042254 ## Saudi Arabia 12 0.007042254 ## Senegal 12 0.007042254 ## Serbia 12 0.007042254 ## Sierra Leone 12 0.007042254 ## Singapore 12 0.007042254 ## Slovak Republic 12 0.007042254 ## Slovenia 12 0.007042254 ## Somalia 12 0.007042254 ## South Africa 12 0.007042254 ## Spain 12 0.007042254 ## Sri Lanka 12 0.007042254 ## Sudan 12 0.007042254 ## Swaziland 12 0.007042254 ## Sweden 12 0.007042254 ## Switzerland 12 0.007042254 ## Syria 12 0.007042254 ## Taiwan 12 0.007042254 ## Tanzania 12 0.007042254 ## Thailand 12 0.007042254 ## Togo 12 0.007042254 ## Trinidad and Tobago 12 0.007042254 ## Tunisia 12 0.007042254 ## Turkey 12 0.007042254 ## Uganda 12 0.007042254 ## United Kingdom 12 0.007042254 ## United States 12 0.007042254 ## Uruguay 12 0.007042254 ## Venezuela 12 0.007042254 ## Vietnam 12 0.007042254 ## West Bank and Gaza 12 0.007042254 ## Yemen, Rep. 12 0.007042254 ## Zambia 12 0.007042254 ## Zimbabwe 12 0.007042254 # If you want to add percentage ... gapminder %&gt;% tabyl(country) %&gt;% adorn_pct_formatting(digits = 0, affix_sign = TRUE) ## country n percent ## Afghanistan 12 1% ## Albania 12 1% ## Algeria 12 1% ## Angola 12 1% ## Argentina 12 1% ## Australia 12 1% ## Austria 12 1% ## Bahrain 12 1% ## Bangladesh 12 1% ## Belgium 12 1% ## Benin 12 1% ## Bolivia 12 1% ## Bosnia and Herzegovina 12 1% ## Botswana 12 1% ## Brazil 12 1% ## Bulgaria 12 1% ## Burkina Faso 12 1% ## Burundi 12 1% ## Cambodia 12 1% ## Cameroon 12 1% ## Canada 12 1% ## Central African Republic 12 1% ## Chad 12 1% ## Chile 12 1% ## China 12 1% ## Colombia 12 1% ## Comoros 12 1% ## Congo, Dem. Rep. 12 1% ## Congo, Rep. 12 1% ## Costa Rica 12 1% ## Cote d&#39;Ivoire 12 1% ## Croatia 12 1% ## Cuba 12 1% ## Czech Republic 12 1% ## Denmark 12 1% ## Djibouti 12 1% ## Dominican Republic 12 1% ## Ecuador 12 1% ## Egypt 12 1% ## El Salvador 12 1% ## Equatorial Guinea 12 1% ## Eritrea 12 1% ## Ethiopia 12 1% ## Finland 12 1% ## France 12 1% ## Gabon 12 1% ## Gambia 12 1% ## Germany 12 1% ## Ghana 12 1% ## Greece 12 1% ## Guatemala 12 1% ## Guinea 12 1% ## Guinea-Bissau 12 1% ## Haiti 12 1% ## Honduras 12 1% ## Hong Kong, China 12 1% ## Hungary 12 1% ## Iceland 12 1% ## India 12 1% ## Indonesia 12 1% ## Iran 12 1% ## Iraq 12 1% ## Ireland 12 1% ## Israel 12 1% ## Italy 12 1% ## Jamaica 12 1% ## Japan 12 1% ## Jordan 12 1% ## Kenya 12 1% ## Korea, Dem. Rep. 12 1% ## Korea, Rep. 12 1% ## Kuwait 12 1% ## Lebanon 12 1% ## Lesotho 12 1% ## Liberia 12 1% ## Libya 12 1% ## Madagascar 12 1% ## Malawi 12 1% ## Malaysia 12 1% ## Mali 12 1% ## Mauritania 12 1% ## Mauritius 12 1% ## Mexico 12 1% ## Mongolia 12 1% ## Montenegro 12 1% ## Morocco 12 1% ## Mozambique 12 1% ## Myanmar 12 1% ## Namibia 12 1% ## Nepal 12 1% ## Netherlands 12 1% ## New Zealand 12 1% ## Nicaragua 12 1% ## Niger 12 1% ## Nigeria 12 1% ## Norway 12 1% ## Oman 12 1% ## Pakistan 12 1% ## Panama 12 1% ## Paraguay 12 1% ## Peru 12 1% ## Philippines 12 1% ## Poland 12 1% ## Portugal 12 1% ## Puerto Rico 12 1% ## Reunion 12 1% ## Romania 12 1% ## Rwanda 12 1% ## Sao Tome and Principe 12 1% ## Saudi Arabia 12 1% ## Senegal 12 1% ## Serbia 12 1% ## Sierra Leone 12 1% ## Singapore 12 1% ## Slovak Republic 12 1% ## Slovenia 12 1% ## Somalia 12 1% ## South Africa 12 1% ## Spain 12 1% ## Sri Lanka 12 1% ## Sudan 12 1% ## Swaziland 12 1% ## Sweden 12 1% ## Switzerland 12 1% ## Syria 12 1% ## Taiwan 12 1% ## Tanzania 12 1% ## Thailand 12 1% ## Togo 12 1% ## Trinidad and Tobago 12 1% ## Tunisia 12 1% ## Turkey 12 1% ## Uganda 12 1% ## United Kingdom 12 1% ## United States 12 1% ## Uruguay 12 1% ## Venezuela 12 1% ## Vietnam 12 1% ## West Bank and Gaza 12 1% ## Yemen, Rep. 12 1% ## Zambia 12 1% ## Zimbabwe 12 1% 4.9.4 Create variables 4.9.4.1 Change values using conditions You can think of case_when() (multiple conditions) as an extended version of ifelse() (binary conditions). mtcars &lt;- mtcars %&gt;% mutate(cyl_dummy = case_when(cyl &gt; median(cyl) ~ &quot;High&quot;, # if condition cyl &lt; median(cyl) ~ &quot;Low&quot;, # else if condition TRUE ~ &#39;Median&#39;)) # else condition mtcars %&gt;% pull(cyl_dummy) ## [1] &quot;Median&quot; &quot;Median&quot; &quot;Low&quot; &quot;Median&quot; &quot;High&quot; &quot;Median&quot; &quot;High&quot; &quot;Low&quot; ## [9] &quot;Low&quot; &quot;Median&quot; &quot;Median&quot; &quot;High&quot; &quot;High&quot; &quot;High&quot; &quot;High&quot; &quot;High&quot; ## [17] &quot;High&quot; &quot;Low&quot; &quot;Low&quot; &quot;Low&quot; &quot;Low&quot; &quot;High&quot; &quot;High&quot; &quot;High&quot; ## [25] &quot;High&quot; &quot;Low&quot; &quot;Low&quot; &quot;Low&quot; &quot;High&quot; &quot;Median&quot; &quot;High&quot; &quot;Low&quot; 4.9.4.2 Change values manually mtcars %&gt;% mutate(cyl_dummy = recode(cyl_dummy, # Target column &quot;High&quot; = &quot;2&quot;, # Old - New &quot;Low&quot; = &quot;0&quot;, &quot;Median&quot; = &quot;1&quot;)) %&gt;% pull(cyl_dummy) ## [1] &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;1&quot; &quot;2&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;1&quot; &quot;2&quot; &quot;2&quot; &quot;2&quot; &quot;2&quot; &quot;2&quot; &quot;2&quot; &quot;0&quot; &quot;0&quot; ## [20] &quot;0&quot; &quot;0&quot; &quot;2&quot; &quot;2&quot; &quot;2&quot; &quot;2&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;2&quot; &quot;1&quot; &quot;2&quot; &quot;0&quot; 4.9.5 Counting How may countries in each continent? gapminder %&gt;% count(continent) ## # A tibble: 5 x 2 ## continent n ## * &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 Let’s arrange the result. # Just add a new argument `sort = TRUE` gapminder %&gt;% count(continent, sort = TRUE) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Asia 396 ## 3 Europe 360 ## 4 Americas 300 ## 5 Oceania 24 # Same as above; How nice! gapminder %&gt;% count(continent) %&gt;% arrange(desc(n)) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Asia 396 ## 3 Europe 360 ## 4 Americas 300 ## 5 Oceania 24 Challenge Count the number of observations per continent as well as year and arrange them with descending order. Let’s take a deeper look at how things work under the hood. tally() works similar to nrow(): Calculate the total number of cases in a dataframe count = group_by() + tally() gapminder %&gt;% tally() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1704 add_tally() = mutate(n = n()) Challenge What does n in the below example represent? gapminder %&gt;% dplyr::select(continent, country) %&gt;% add_tally() ## # A tibble: 1,704 x 3 ## continent country n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Asia Afghanistan 1704 ## 2 Asia Afghanistan 1704 ## 3 Asia Afghanistan 1704 ## 4 Asia Afghanistan 1704 ## 5 Asia Afghanistan 1704 ## 6 Asia Afghanistan 1704 ## 7 Asia Afghanistan 1704 ## 8 Asia Afghanistan 1704 ## 9 Asia Afghanistan 1704 ## 10 Asia Afghanistan 1704 ## # … with 1,694 more rows add_count Add count as a column # Add count as a column gapminder %&gt;% group_by(continent) %&gt;% add_count(year) ## # A tibble: 1,704 x 7 ## # Groups: continent [5] ## country continent year lifeExp pop gdpPercap n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 33 ## 2 Afghanistan Asia 1957 30.3 9240934 821. 33 ## 3 Afghanistan Asia 1962 32.0 10267083 853. 33 ## 4 Afghanistan Asia 1967 34.0 11537966 836. 33 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 33 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 33 ## 7 Afghanistan Asia 1982 39.9 12881816 978. 33 ## 8 Afghanistan Asia 1987 40.8 13867957 852. 33 ## 9 Afghanistan Asia 1992 41.7 16317921 649. 33 ## 10 Afghanistan Asia 1997 41.8 22227415 635. 33 ## # … with 1,694 more rows Challenge Do the cases 1 and 2 in the below code chunk produce same outputs? If so, why? # Case 1 gapminder %&gt;% group_by(continent, year) %&gt;% count() ## # A tibble: 60 x 3 ## # Groups: continent, year [60] ## continent year n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 Africa 1952 52 ## 2 Africa 1957 52 ## 3 Africa 1962 52 ## 4 Africa 1967 52 ## 5 Africa 1972 52 ## 6 Africa 1977 52 ## 7 Africa 1982 52 ## 8 Africa 1987 52 ## 9 Africa 1992 52 ## 10 Africa 1997 52 ## # … with 50 more rows # Case 2 gapminder %&gt;% group_by(continent) %&gt;% count(year) ## # A tibble: 60 x 3 ## # Groups: continent [5] ## continent year n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 Africa 1952 52 ## 2 Africa 1957 52 ## 3 Africa 1962 52 ## 4 Africa 1967 52 ## 5 Africa 1972 52 ## 6 Africa 1977 52 ## 7 Africa 1982 52 ## 8 Africa 1987 52 ## 9 Africa 1992 52 ## 10 Africa 1997 52 ## # … with 50 more rows count() is a simple function, but it is still helpful to learn a very important concept underlying complex data wrangling: split-apply-combine strategy. For more information, read Wickham’s article (2011) “The Split-Apply-Combine Strategy for Data Analysis” published in the Journal of Statistical Software (especially pages 7-8). plyr was the package (retired) that demonstrated this idea, which has evolved into two directions: dplyr (for data frames) and purrr (for lists) 4.9.6 Summarizing 4.9.6.1 Basic Create a summary Think of summarise() as an extended version of count(). gapminder %&gt;% group_by(continent) %&gt;% summarise( n = n(), mean_gdp = mean(gdpPercap), sd_gdp = sd(gdpPercap) ) ## # A tibble: 5 x 4 ## continent n mean_gdp sd_gdp ## * &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 624 2194. 2828. ## 2 Americas 300 7136. 6397. ## 3 Asia 396 7902. 14045. ## 4 Europe 360 14469. 9355. ## 5 Oceania 24 18622. 6359. tablea &lt;- gapminder %&gt;% group_by(continent) %&gt;% summarise( n = n(), mean_gdp = mean(gdpPercap), sd_gdp = sd(gdpPercap) ) Produce publishable tables # For HTML and LaTeX tablea %&gt;% kableExtra::kable() continent n mean_gdp sd_gdp Africa 624 2193.755 2827.930 Americas 300 7136.110 6396.764 Asia 396 7902.150 14045.373 Europe 360 14469.476 9355.213 Oceania 24 18621.609 6358.983 # For HTML and MS Office suite tablea %&gt;% flextable::flextable() .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 5px; margin-bottom: 5px; table-layout: fixed; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-f5d3b2da{border-collapse:collapse;}.cl-f5ce2298{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f5ce3652{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:3pt;padding-top:3pt;padding-left:3pt;padding-right:3pt;line-height: 1;background-color:transparent;}.cl-f5ce3670{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:3pt;padding-top:3pt;padding-left:3pt;padding-right:3pt;line-height: 1;background-color:transparent;}.cl-f5ce719e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f5ce71c6{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f5ce71d0{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f5ce71da{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f5ce71e4{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 2pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f5ce71ee{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(0, 0, 0, 1.00);border-top: 2pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} continentnmean_gdpsd_gdpAfrica6242,1942,828Americas3007,1366,397Asia3967,90214,045Europe36014,4699,355Oceania2418,6226,359 4.9.6.2 Scoped summaries Old way summarise_all() # Create a wide-shaped data example wide_gapminder &lt;- gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% pivot_wider( names_from = country, values_from = gdpPercap ) # Apply summarise_all wide_gapminder %&gt;% dplyr::select(-c(1:4)) %&gt;% summarise_all(mean, na.rm = TRUE) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; summarise_if(): using a logical condition wide_gapminder %&gt;% summarise_if(is.double, mean, na.rm = TRUE) ## # A tibble: 1 x 31 ## lifeExp Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 71.9 3255. 20412. 19901. 3485. 6384. 9332. ## # … with 24 more variables: `Czech Republic` &lt;dbl&gt;, Denmark &lt;dbl&gt;, ## # Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, ## # Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, ## # Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, ## # Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, ## # Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United ## # Kingdom` &lt;dbl&gt; summarise_at() vars() = select() wide_gapminder %&gt;% summarise_at(vars(-c(1:4)), mean, na.rm = TRUE ) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; wide_gapminder %&gt;% summarise_at(vars(contains(&quot;life&quot;)), mean, na.rm = TRUE ) ## # A tibble: 1 x 1 ## lifeExp ## &lt;dbl&gt; ## 1 71.9 Additional tips Concept map for regular expressions. By Monica Alonso, Greg Wilson. New way summarise() + across() Concept map for across. By Emma Vestesson If you find using summarise_all(), summarise_if() and summarise_at() confusing, here’s a solution: use summarise() with across(). summarise_all() wide_gapminder %&gt;% summarise(across(Albania:`United Kingdom`, mean, na.rm = TRUE)) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; wide_gapminder %&gt;% summarise(across(-c(1:4), mean, na.rm = TRUE)) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; summarise_if() wide_gapminder %&gt;% summarise(across(is.double, mean, na.rm = TRUE)) ## Warning: Predicate functions must be wrapped in `where()`. ## ## # Bad ## data %&gt;% select(is.double) ## ## # Good ## data %&gt;% select(where(is.double)) ## ## ℹ Please update your code. ## This message is displayed once per session. ## # A tibble: 1 x 31 ## lifeExp Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 71.9 3255. 20412. 19901. 3485. 6384. 9332. ## # … with 24 more variables: `Czech Republic` &lt;dbl&gt;, Denmark &lt;dbl&gt;, ## # Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, ## # Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, ## # Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, ## # Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, ## # Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United ## # Kingdom` &lt;dbl&gt; summarise_at() wide_gapminder %&gt;% summarise(across(-c(1:4), mean, na.rm = TRUE )) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; wide_gapminder %&gt;% summarise(across(contains(&quot;life&quot;), mean, na.rm = TRUE )) ## # A tibble: 1 x 1 ## lifeExp ## &lt;dbl&gt; ## 1 71.9 wide_gapminder %&gt;% summarise(across(contains(&quot;A&quot;, ignore.case = FALSE))) ## # A tibble: 360 x 2 ## Albania Austria ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1601. NA ## 2 1942. NA ## 3 2313. NA ## 4 2760. NA ## 5 3313. NA ## 6 3533. NA ## 7 3631. NA ## 8 3739. NA ## 9 2497. NA ## 10 3193. NA ## # … with 350 more rows Note that this workshop does not cover creating and manipulating variables using mutate() because many techniques you learned from playing with summarise() can be directly applied to mutate(). Challenge Summarize average GDP of countries whose names starting with alphabet “A.” Turn the summary dataframe into a publishable table using either kableExtra or flextable package. 4.9.6.3 Tabulation (TBD) 4.9.7 Grouping 4.9.7.1 Grouped summaries Calculate the mean of gdpPercap. Some functions are designed to work together. For instance, the group_by function defines the strata that you’re going to use for summary statistics. Then, use summarise() or summarize() for producing summary statistics. gapminder %&gt;% group_by(continent) %&gt;% # summarise(mean_gdp = mean(gdpPercap)) ## # A tibble: 5 x 2 ## continent mean_gdp ## * &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 2194. ## 2 Americas 7136. ## 3 Asia 7902. ## 4 Europe 14469. ## 5 Oceania 18622. Calculate multiple summary statistics. gapminder %&gt;% group_by(continent) %&gt;% # summarise( mean_gdp = mean(gdpPercap), count = n() ) ## # A tibble: 5 x 3 ## continent mean_gdp count ## * &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 2194. 624 ## 2 Americas 7136. 300 ## 3 Asia 7902. 396 ## 4 Europe 14469. 360 ## 5 Oceania 18622. 24 Optional Other summary statistics Measures of spread: median(x), sd(x), IQR(x), mad(x) (the median absolute deviation) # The Interquartile Range = The Difference Between 75t and 25t Percentiles gapminder %&gt;% group_by(continent) %&gt;% # summarise(IQR_gdp = IQR(gdpPercap)) ## # A tibble: 5 x 2 ## continent IQR_gdp ## * &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 1616. ## 2 Americas 4402. ## 3 Asia 7492. ## 4 Europe 13248. ## 5 Oceania 8072. Measures of rank: min(x), quantile(x, 0.25), max(x) gapminder %&gt;% group_by(continent) %&gt;% # summarise( min_gdp = min(gdpPercap), max_gdp = max(gdpPercap) ) ## # A tibble: 5 x 3 ## continent min_gdp max_gdp ## * &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 241. 21951. ## 2 Americas 1202. 42952. ## 3 Asia 331 113523. ## 4 Europe 974. 49357. ## 5 Oceania 10040. 34435. Measures of position: first(x), last(x), nth(x, 2) gapminder %&gt;% group_by(continent) %&gt;% summarise( first_gdp = first(gdpPercap), last_gdp = last(gdpPercap) ) ## # A tibble: 5 x 3 ## continent first_gdp last_gdp ## * &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 2449. 470. ## 2 Americas 5911. 11416. ## 3 Asia 779. 2281. ## 4 Europe 1601. 33203. ## 5 Oceania 10040. 25185. gapminder %&gt;% group_by(continent) %&gt;% arrange(gdpPercap) %&gt;% # Adding arrange summarise( first_gdp = first(gdpPercap), last_gdp = last(gdpPercap) ) ## # A tibble: 5 x 3 ## continent first_gdp last_gdp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 241. 21951. ## 2 Americas 1202. 42952. ## 3 Asia 331 113523. ## 4 Europe 974. 49357. ## 5 Oceania 10040. 34435. Measures of counts: n(x) (all rows), sum(!is.na(x)) (only non-missing rows) = n_distinct(x) gapminder %&gt;% group_by(continent) %&gt;% summarise(ns = n()) ## # A tibble: 5 x 2 ## continent ns ## * &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 Counts and proportions of logical values: sum(condition about x) (the number of TRUEs in x), mean(condition about x) (the proportion of TRUEs in x) gapminder %&gt;% group_by(continent) %&gt;% summarise(rich_countries = mean(gdpPercap &gt; 20000)) ## # A tibble: 5 x 2 ## continent rich_countries ## * &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 0.00481 ## 2 Americas 0.05 ## 3 Asia 0.111 ## 4 Europe 0.261 ## 5 Oceania 0.333 Additional tips Also, check out window functions such as cumsum() and lag(). Window functions are a variant of aggregate functions that take a vector as an input then returns a vector of the same length as an output. vec &lt;- c(1:10) # Typical aggregate function sum(vec) # The output length is one ## [1] 55 # Window function cumsum(vec) # The output length is ten ## [1] 1 3 6 10 15 21 28 36 45 55 # Let&#39;s compare them side-by-side compare( sum(vec), cumsum(vec) ) ## `old`: 55 ## `new`: 1 3 6 10 15 21 28 36 45 55 4.9.8 Joining Relational data = multiple tables of data Relational data example Key ideas A primary key “uniquely identifies an observation in its own table” # Example planes$tailnum %&gt;% head() ## [1] &quot;N10156&quot; &quot;N102UW&quot; &quot;N103US&quot; &quot;N104UW&quot; &quot;N10575&quot; &quot;N105UW&quot; Verify primary key tailnum should be unique. Challenge What do you expect the outcome? planes %&gt;% count(tailnum) %&gt;% filter(n &gt; 1) ## # A tibble: 0 x 2 ## # … with 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt; Optional If a dataframe doesn’t have primary key, you can add one called a surrogate key. # Toy example df &lt;- tibble( x = c(1:3), y = c(4:6) ) # Add a row_index column df &lt;- df %&gt;% rowid_to_column(&quot;ID&quot;) A foreign key “uniquely identifies an observation in another table.” flights$tailnum %&gt;% head() ## [1] &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; &quot;N668DN&quot; &quot;N39463&quot; For joining, don’t be distracted by other details and focus on KEYS! 4.9.8.1 Mutating joins Add new variables to one data frame from matching observations in another\" Using a simple toy example is great because it is easy to see how things work in that much narrow context. Toy example # Table 1 x &lt;- tibble( key = c(1:4), val_x = c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;) ) # Table 2 y &lt;- tibble( key = c(1:5), val_y = c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;, &quot;y5&quot;) ) Inner Join inner_join() keeps the matched values in both tables. If the left table is a subset of the right table, then the result of left_join() is same as inner_join(). Challenge What are going to be the shared keys? inner_join(x, y) ## Joining, by = &quot;key&quot; ## # A tibble: 4 x 3 ## key val_x val_y ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 Mutating joins Left Join left_join(), right_join() and full_join() are outer join functions. Unlike inner_join(), outer join functions keep observations that appear in at least one of the tables. left_join() keeps only the matched observations in the right table. left_join(x, y) ## Joining, by = &quot;key&quot; ## # A tibble: 4 x 3 ## key val_x val_y ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 Right Join right_join() does the opposite. right_join(x, y) ## Joining, by = &quot;key&quot; ## # A tibble: 5 x 3 ## key val_x val_y ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 ## 5 5 &lt;NA&gt; y5 Full Join full_join() keeps the observations from both tables. If they were unmatched, then NAs were recoded in one of the two tables. full_join(x, y) ## Joining, by = &quot;key&quot; ## # A tibble: 5 x 3 ## key val_x val_y ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 ## 5 5 &lt;NA&gt; y5 4.9.8.2 Filtering joins Filter observations from one data frame based on whether or not they match an observation in the other table. Semi Join In SQL, this type of query is also called subqueries. Filtering without joining # Create the list of the top 10 destinations top_dest &lt;- flights %&gt;% count(dest, sort = TRUE) %&gt;% top_n(10) ## Selecting by n # Filter filtered &lt;- flights %&gt;% filter(dest %in% top_dest$dest) Using semi join: only keep (INCLUDE) the rows that were matched between the two tables joined &lt;- flights %&gt;% semi_join(top_dest) ## Joining, by = &quot;dest&quot; head(filtered == joined) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## [1,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [4,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [5,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [6,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## [1,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [4,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [5,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [6,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## time_hour ## [1,] TRUE ## [2,] TRUE ## [3,] TRUE ## [4,] TRUE ## [5,] TRUE ## [6,] TRUE Anti Join anti_join() dose the opposite. Exclude the rows that were matched between the two tables. Great technique to filter stopwords when you do a computational text analysis. flights %&gt;% anti_join(planes, by = &quot;tailnum&quot;) %&gt;% count(tailnum, sort = TRUE) ## # A tibble: 722 x 2 ## tailnum n ## &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; 2512 ## 2 N725MQ 575 ## 3 N722MQ 513 ## 4 N723MQ 507 ## 5 N713MQ 483 ## 6 N735MQ 396 ## 7 N0EGMQ 371 ## 8 N534MQ 364 ## 9 N542MQ 363 ## 10 N531MQ 349 ## # … with 712 more rows 4.10 Modeling (broom) 4.10.1 Nesting 4.10.1.1 nest The following example comes from R for Data Science by by Garrett Grolemund and Hadley Wickham. How can you run multiple models simultaneously? Using a nested data frame. Hadley Wickham: Managing many models with R Grouped data: each row = an observation Nested data: each row = a group Challenge In the following example, why did we use country and continent for nesting variables? nested &lt;- gapminder %&gt;% group_by(country, continent) %&gt;% nest() head(nested) ## # A tibble: 6 x 3 ## # Groups: country, continent [6] ## country continent data ## &lt;fct&gt; &lt;fct&gt; &lt;list&gt; ## 1 Afghanistan Asia &lt;tibble [12 × 4]&gt; ## 2 Albania Europe &lt;tibble [12 × 4]&gt; ## 3 Algeria Africa &lt;tibble [12 × 4]&gt; ## 4 Angola Africa &lt;tibble [12 × 4]&gt; ## 5 Argentina Americas &lt;tibble [12 × 4]&gt; ## 6 Australia Oceania &lt;tibble [12 × 4]&gt; nested$data %&gt;% pluck(1) ## # A tibble: 12 x 4 ## year lifeExp pop gdpPercap ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 8425333 779. ## 2 1957 30.3 9240934 821. ## 3 1962 32.0 10267083 853. ## 4 1967 34.0 11537966 836. ## 5 1972 36.1 13079460 740. ## 6 1977 38.4 14880372 786. ## 7 1982 39.9 12881816 978. ## 8 1987 40.8 13867957 852. ## 9 1992 41.7 16317921 649. ## 10 1997 41.8 22227415 635. ## 11 2002 42.1 25268405 727. ## 12 2007 43.8 31889923 975. Custom function lm_model &lt;- function(df) { lm(lifeExp ~ year, data = df) } Apply function to the nested data # Apply m_model to the nested data nested &lt;- nested %&gt;% mutate(models = map(data, lm_model)) # Add the list object as a new column head(nested) ## # A tibble: 6 x 4 ## # Groups: country, continent [6] ## country continent data models ## &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; ## 1 Afghanistan Asia &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 2 Albania Europe &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 3 Algeria Africa &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 4 Angola Africa &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 5 Argentina Americas &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 6 Australia Oceania &lt;tibble [12 × 4]&gt; &lt;lm&gt; S3 is part of R’s object oriented systems. If you need more information, check this section in Hadley’s Advanced R out. 4.10.1.2 unnest glance() glance() function from broom package inspects the quality of a statistical model. Additional tips broom::glance(model): for evaluating model quality and/or complexity broom::tidy(model): for extracting each coefficient in the model (the estimates + its variability) broom::augment(model, data): for getting extra values (residuals, and influence statistics). A really handy tool in case if you want to plot fitted values and raw data together. Broom: Converting Statistical Models to Tidy Data Frames by David Robinson glanced &lt;- nested %&gt;% mutate(glance = map(models, broom::glance)) # Pluck the first item on the list glanced$glance %&gt;% pluck(1) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.948 0.942 1.22 181. 9.84e-8 1 -18.3 42.7 44.1 ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # Pull p.value glanced$glance %&gt;% pluck(1) %&gt;% pull(p.value) ## value ## 9.835213e-08 unnest() unpacks the list objects stored in glance column glanced %&gt;% unnest(glance) %&gt;% arrange(r.squared) ## # A tibble: 142 x 16 ## # Groups: country, continent [142] ## country continent data models r.squared adj.r.squared sigma statistic ## &lt;fct&gt; &lt;fct&gt; &lt;lis&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rwanda Africa &lt;tib… &lt;lm&gt; 0.0172 -0.0811 6.56 0.175 ## 2 Botswa… Africa &lt;tib… &lt;lm&gt; 0.0340 -0.0626 6.11 0.352 ## 3 Zimbab… Africa &lt;tib… &lt;lm&gt; 0.0562 -0.0381 7.21 0.596 ## 4 Zambia Africa &lt;tib… &lt;lm&gt; 0.0598 -0.0342 4.53 0.636 ## 5 Swazil… Africa &lt;tib… &lt;lm&gt; 0.0682 -0.0250 6.64 0.732 ## 6 Lesotho Africa &lt;tib… &lt;lm&gt; 0.0849 -0.00666 5.93 0.927 ## 7 Cote d… Africa &lt;tib… &lt;lm&gt; 0.283 0.212 3.93 3.95 ## 8 South … Africa &lt;tib… &lt;lm&gt; 0.312 0.244 4.74 4.54 ## 9 Uganda Africa &lt;tib… &lt;lm&gt; 0.342 0.276 3.19 5.20 ## 10 Congo,… Africa &lt;tib… &lt;lm&gt; 0.348 0.283 2.43 5.34 ## # … with 132 more rows, and 8 more variables: p.value &lt;dbl&gt;, df &lt;dbl&gt;, ## # logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, ## # nobs &lt;int&gt; glanced %&gt;% unnest(glance) %&gt;% ggplot(aes(continent, r.squared)) + geom_jitter(width = 0.5) tidy() nested &lt;- gapminder %&gt;% group_by(continent) %&gt;% nest() nested &lt;- nested %&gt;% mutate(models = map(data, ~lm(lifeExp ~ year + country, data = .))) tidied &lt;- nested %&gt;% mutate(tidied = map(models, broom::tidy)) model_out &lt;- tidied %&gt;% unnest(tidied) %&gt;% mutate(term = str_replace(term, &quot;country&quot;, &quot;&quot;)) %&gt;% select(continent, term, estimate, p.value) %&gt;% mutate(p_threshold = ifelse(p.value &lt; 0.05, 1, 0)) model_out %&gt;% filter(p_threshold == 1) %&gt;% pull(term) %&gt;% unique() ## [1] &quot;(Intercept)&quot; &quot;year&quot; ## [3] &quot;Bahrain&quot; &quot;Bangladesh&quot; ## [5] &quot;Cambodia&quot; &quot;China&quot; ## [7] &quot;Hong Kong, China&quot; &quot;India&quot; ## [9] &quot;Indonesia&quot; &quot;Iran&quot; ## [11] &quot;Iraq&quot; &quot;Israel&quot; ## [13] &quot;Japan&quot; &quot;Jordan&quot; ## [15] &quot;Korea, Dem. Rep.&quot; &quot;Korea, Rep.&quot; ## [17] &quot;Kuwait&quot; &quot;Lebanon&quot; ## [19] &quot;Malaysia&quot; &quot;Mongolia&quot; ## [21] &quot;Myanmar&quot; &quot;Nepal&quot; ## [23] &quot;Oman&quot; &quot;Pakistan&quot; ## [25] &quot;Philippines&quot; &quot;Saudi Arabia&quot; ## [27] &quot;Singapore&quot; &quot;Sri Lanka&quot; ## [29] &quot;Syria&quot; &quot;Taiwan&quot; ## [31] &quot;Thailand&quot; &quot;Vietnam&quot; ## [33] &quot;West Bank and Gaza&quot; &quot;Yemen, Rep.&quot; ## [35] &quot;Austria&quot; &quot;Belgium&quot; ## [37] &quot;Croatia&quot; &quot;Czech Republic&quot; ## [39] &quot;Denmark&quot; &quot;Finland&quot; ## [41] &quot;France&quot; &quot;Germany&quot; ## [43] &quot;Greece&quot; &quot;Iceland&quot; ## [45] &quot;Ireland&quot; &quot;Italy&quot; ## [47] &quot;Montenegro&quot; &quot;Netherlands&quot; ## [49] &quot;Norway&quot; &quot;Poland&quot; ## [51] &quot;Portugal&quot; &quot;Slovak Republic&quot; ## [53] &quot;Slovenia&quot; &quot;Spain&quot; ## [55] &quot;Sweden&quot; &quot;Switzerland&quot; ## [57] &quot;Turkey&quot; &quot;United Kingdom&quot; ## [59] &quot;Angola&quot; &quot;Benin&quot; ## [61] &quot;Botswana&quot; &quot;Burkina Faso&quot; ## [63] &quot;Burundi&quot; &quot;Cameroon&quot; ## [65] &quot;Central African Republic&quot; &quot;Chad&quot; ## [67] &quot;Comoros&quot; &quot;Congo, Dem. Rep.&quot; ## [69] &quot;Congo, Rep.&quot; &quot;Cote d&#39;Ivoire&quot; ## [71] &quot;Djibouti&quot; &quot;Equatorial Guinea&quot; ## [73] &quot;Eritrea&quot; &quot;Ethiopia&quot; ## [75] &quot;Gabon&quot; &quot;Gambia&quot; ## [77] &quot;Ghana&quot; &quot;Guinea&quot; ## [79] &quot;Guinea-Bissau&quot; &quot;Kenya&quot; ## [81] &quot;Lesotho&quot; &quot;Liberia&quot; ## [83] &quot;Madagascar&quot; &quot;Malawi&quot; ## [85] &quot;Mali&quot; &quot;Mauritania&quot; ## [87] &quot;Mauritius&quot; &quot;Mozambique&quot; ## [89] &quot;Namibia&quot; &quot;Niger&quot; ## [91] &quot;Nigeria&quot; &quot;Reunion&quot; ## [93] &quot;Rwanda&quot; &quot;Senegal&quot; ## [95] &quot;Sierra Leone&quot; &quot;Somalia&quot; ## [97] &quot;South Africa&quot; &quot;Sudan&quot; ## [99] &quot;Swaziland&quot; &quot;Tanzania&quot; ## [101] &quot;Togo&quot; &quot;Uganda&quot; ## [103] &quot;Zambia&quot; &quot;Zimbabwe&quot; ## [105] &quot;Bolivia&quot; &quot;Brazil&quot; ## [107] &quot;Canada&quot; &quot;Colombia&quot; ## [109] &quot;Dominican Republic&quot; &quot;Ecuador&quot; ## [111] &quot;El Salvador&quot; &quot;Guatemala&quot; ## [113] &quot;Haiti&quot; &quot;Honduras&quot; ## [115] &quot;Mexico&quot; &quot;Nicaragua&quot; ## [117] &quot;Paraguay&quot; &quot;Peru&quot; ## [119] &quot;Puerto Rico&quot; &quot;Trinidad and Tobago&quot; ## [121] &quot;United States&quot; &quot;Venezuela&quot; ## [123] &quot;New Zealand&quot; model_out %&gt;% filter(p_threshold == 0) %&gt;% pull(term) %&gt;% unique() ## [1] &quot;Bosnia and Herzegovina&quot; &quot;Bulgaria&quot; &quot;Hungary&quot; ## [4] &quot;Romania&quot; &quot;Serbia&quot; &quot;Egypt&quot; ## [7] &quot;Libya&quot; &quot;Morocco&quot; &quot;Sao Tome and Principe&quot; ## [10] &quot;Tunisia&quot; &quot;Chile&quot; &quot;Costa Rica&quot; ## [13] &quot;Cuba&quot; &quot;Jamaica&quot; &quot;Panama&quot; ## [16] &quot;Uruguay&quot; 4.10.2 Mapping We tasted a little bit about how map() function works. Let’s dig into it deeper as this family of functions is really useful. For more information, see Rebecca Barter’s wonderful tutorial on the purrr package. In her words, this is “the tidyverse’s answer to apply functions for iteration.” map() function can take a vector (of any type), a list, and a dataframe for input. multiply &lt;- function(x) { x * x } df &lt;- list( first_obs = rnorm(7, 1, sd = 1), second_obs = rnorm(7, 2, sd = 2) ) # normal distribution Challenge Try map_df(.x = df, .f = multiply) and tell me what’s the difference between the output you got and what you saw earlier. If you want to know more about the power and joy of functional programming in R (e.g., purrr::map()), then please take “How to Automate Repeated Things in R” workshop. 4.10.3 Hypothesis testing Statistical inference: does the effect/difference in observed data occur by chance? Null hypothesis: everything was random Alternative hypothesis: everything was not random. Note that this does not mean that a particular factor influenced the outcome of interest. Statistical inference != Causal inference (causes and effects) \\(Y = X_{1} + X_{2} + X_{3} \\epsilon\\) infer is for tidyverse-friendly statistical inference. Workflow specify() specify a formula hypothesize() declare the null hypothesis generate() generate data based on the null hypothesis calculate() calculate a distribution of statistics from the generated data to form the null distribution From infer package gapminder &lt;- gapminder %&gt;% mutate(log_pop = log(pop)) ggplot(aes(x = log_pop, y = lifeExp), data = gapminder) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; # Calculate the observed statistic: Observed slopes observed_slopes &lt;- gapminder %&gt;% # specify(formula = lifeExp ~ log_pop) %&gt;% specify(formula = lifeExp ~ log_pop) %&gt;% calculate(stat = &quot;slope&quot;) # Generate the null distribution: Null slopes null_slopes &lt;- gapminder %&gt;% # Specify a formula specify(formula = lifeExp ~ log_pop) %&gt;% # Hypothesize (point estimation) hypothesize(null = &quot;point&quot;, mu = 0) %&gt;% # Generate sampling distributions (bootstrapping) generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% # Calculate statistics calculate(stat = &quot;slope&quot;) # Return data null_slopes %&gt;% # p-value is just the probability that observed pattern could arise if the null hypothesis was true # In social science convention, if alpha is below 0.005 (note: this is totally arbitrary), then the observed distribution is statistically significant. get_p_value(obs_stat = observed_slopes, direction = &quot;both&quot;) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.972 # Visualize output visualize(null_slopes) + shade_p_value(obs_stat = observed_slopes, direction = &quot;both&quot;) 4.10.4 Mixed models This part heavily draws on Gelman and Hill (2007), Michael Clark’s Mixed Models with R, and Basel R Bootcamp’s Statistics with R: Mixed Models. For a quick review on mixed models, I recommend Xavier et al.’s “A brief introduction to mixed effects modelling and multi-model inference in ecology” (2018). Why random effects model/mixed effects model? Fixed effects model assume that groups are independent from each other and sharing common residuals (same slope for fitted covariates). Limiting to a common slope can inflate Type I and Type II errors. pacman::p_load(# Necessary lme4, broom.mixed, # Optional merTools, glmmTMB, brms, modelr, nlme, sjstats) ## Installing package into &#39;/home/jae/R/x86_64-pc-linux-gnu-library/4.0&#39; ## (as &#39;lib&#39; is unspecified) ## also installing the dependency &#39;rstan&#39; ## Warning in utils::install.packages(package, ...): installation of package ## &#39;rstan&#39; had non-zero exit status ## Warning in utils::install.packages(package, ...): installation of package &#39;brms&#39; ## had non-zero exit status ## Warning in p_install(package, character.only = TRUE, ...): ## Warning in library(package, lib.loc = lib.loc, character.only = TRUE, ## logical.return = TRUE, : there is no package called &#39;brms&#39; ## Warning in pacman::p_load(lme4, broom.mixed, merTools, glmmTMB, brms, modelr, : Failed to install/load: ## brms # Base LM model lm_out &lt;- lm(lifeExp ~ pop + continent, data = gapminder) # Mixed effects re_out &lt;- lmer(lifeExp ~ pop + # Fixed effects # Random effects # (design matrix|group_variables) # (1|group_var) = random intercept # (1 + var | group_var) = random slope + random intercept (1|continent), data = gapminder, # Data REML = FALSE) # Default computation engine is restricted maximal likelihood; the other option is maximum likelihood estimation ## Warning: Some predictor variables are on very different scales: consider ## rescaling #re_out2 &lt;- lmer(lifeExp ~ pop + # Fixed effects # Random effects # (design matrix|group_variables) # (1|group_var) = random intercept # (1 + var | group_var) = random slope + random intercept # (country|continent), # data = gapminder, # Data # REML = FALSE) # Default computation engine is restricted maximal likelihood; the other option is maximum likelihood estimation summary(re_out) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: lifeExp ~ pop + (1 | continent) ## Data: gapminder ## ## AIC BIC logLik deviance df.resid ## 12441.4 12463.1 -6216.7 12433.4 1700 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.3584 -0.5915 0.0344 0.6585 2.9926 ## ## Random effects: ## Groups Name Variance Std.Dev. ## continent (Intercept) 82.02 9.056 ## Residual 85.01 9.220 ## Number of obs: 1704, groups: continent, 5 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 6.375e+01 4.072e+00 15.658 ## pop 4.504e-09 2.174e-09 2.072 ## ## Correlation of Fixed Effects: ## (Intr) ## pop -0.015 ## fit warnings: ## Some predictor variables are on very different scales: consider rescaling nlme::fixef(re_out) %&gt;% as_tibble() ## # A tibble: 2 x 1 ## value ## &lt;dbl&gt; ## 1 6.38e+1 ## 2 4.50e-9 nlme::ranef(re_out) %&gt;% as_tibble() ## # A tibble: 5 x 5 ## grpvar term grp condval condsd ## &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 continent (Intercept) Africa -14.9 0.369 ## 2 continent (Intercept) Americas 0.791 0.531 ## 3 continent (Intercept) Asia -4.03 0.463 ## 4 continent (Intercept) Europe 8.05 0.485 ## 5 continent (Intercept) Oceania 10.1 1.84 lm_out %&gt;% broom::tidy() %&gt;% filter(term == &quot;pop&quot;) %&gt;% pull(estimate) ## [1] 4.515688e-09 re_out %&gt;% broom.mixed::tidy() %&gt;% filter(term == &quot;pop&quot;) %&gt;% pull(estimate) ## [1] 4.503537e-09 4.10.5 Design Anaysis (optional) DeclareDesign provides a collection of packages (fabricatr, randomizr, estimatr) that are very useful for those interested in testing the strength of research design. DeclareDesign also helps share your research design along with your code and data. pacman::p_load(DeclareDesign, # fabricatr, # Fabricate mock (fake) data randomizr, # Random sampling and assignment estimatr, # Fast estimation tools (IV, etc) DesignLibrary) # Research design library Model-Inquire-Data Strategy-Answer Strategy (MIDA) by Graeme Blair, Jasper Cooper, Alex Coppock, and Macartan Humphreys (APSR 2019). The following instructions draw on the vignette available at the package homepage. Also, take a look at DeclareDesign cheatsheet. Setup Model: a speculatioin of how the world works (variables plus their relationships) population &lt;- declare_population(N = 1000, noise = rnorm(N)) population # class ## declare_population(N = 1000, noise = rnorm(N)) population() %&gt;% head() # instance ## ID noise ## 1 0001 0.5952464 ## 2 0002 0.5847477 ## 3 0003 0.9645900 ## 4 0004 0.3194326 ## 5 0005 -1.2011572 ## 6 0006 -1.7892308 # Note that Y_Z_0 and Y_Z_1 are argument names. potential_outcomes &lt;- declare_potential_outcomes(Y_Z_0 = 0, # Control group Y_Z_1 = 1 + noise) # Treatment group Inquiry: a description of the distributions of the variables (estimand = something we desire to know) You can add other estimators such as Conditional Average Treatment Effect (CATE). estimand_ate &lt;- declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) Data strategy: a description of sampling (or case selection) + intervention strategy sampling &lt;- declare_sampling(n = 250) # Sampling 250 units assignment &lt;- declare_assignment(m = 50) # Assign 50 units # If you don&#39;t do this, you will be missing Y reveal_Y &lt;- declare_reveal(Y, Z) Answer strategy: a description of how to use data to answer a question estimator_ate &lt;- declare_estimator(Y ~ Z, estimand = estimand_ate, model = difference_in_means) Design design &lt;- population + potential_outcomes + estimand_ate + sampling + assignment + reveal_Y + estimator_ate # This should work. Otherwise, something is wrong with your code. design ## ## Design Summary ## ## Step 1 (population): declare_population(N = 1000, noise = rnorm(N)) ------------ ## ## N = 1000 ## ## Added variable: ID ## N_missing N_unique class ## 0 1000 character ## ## Added variable: noise ## min median mean max sd N_missing N_unique ## -3.85 -0.07 -0.04 3.36 1.02 0 1000 ## ## Step 2 (potential outcomes): declare_potential_outcomes(Y_Z_0 = 0, Y_Z_1 = 1 + noise) ## ## Added variable: Y_Z_0 ## 0 ## 1000 ## 1.00 ## ## Added variable: Y_Z_1 ## min median mean max sd N_missing N_unique ## -2.85 0.93 0.96 4.36 1.02 0 1000 ## ## Step 3 (estimand): declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) ----------------- ## ## A single draw of the estimand: ## estimand_label estimand ## ATE 0.9600409 ## ## Step 4 (sampling): declare_sampling(n = 250) ----------------------------------- ## ## N = 250 (750 subtracted) ## ## Added variable: S_inclusion_prob ## 0.25 ## 250 ## 1.00 ## ## Altered variable: ID ## Before: ## N_missing N_unique class ## 0 1000 character ## ## After: ## N_missing N_unique class ## 0 250 character ## ## Altered variable: noise ## Before: ## min median mean max sd N_missing N_unique ## -3.85 -0.07 -0.04 3.36 1.02 0 1000 ## ## After: ## min median mean max sd N_missing N_unique ## -3.6 -0.08 -0.03 2.83 1.06 0 250 ## ## Altered variable: Y_Z_0 ## Before: ## 0 ## 1000 ## 1.00 ## ## After: ## 0 ## 250 ## 1.00 ## ## Altered variable: Y_Z_1 ## Before: ## min median mean max sd N_missing N_unique ## -2.85 0.93 0.96 4.36 1.02 0 1000 ## ## After: ## min median mean max sd N_missing N_unique ## -2.6 0.92 0.97 3.83 1.06 0 250 ## ## Step 5 (assignment): declare_assignment(m = 50) -------------------------------- ## ## Added variable: Z ## 0 1 ## 200 50 ## 0.80 0.20 ## ## Added variable: Z_cond_prob ## 0.2 0.8 ## 50 200 ## 0.20 0.80 ## ## Step 6 (reveal): declare_reveal(Y, Z) ------------------------------------------ ## ## Added variable: Y ## min median mean max sd N_missing N_unique ## -1.44 0 0.2 3.08 0.59 0 51 ## ## Step 7 (estimator): declare_estimator(Y ~ Z, estimand = estimand_ate, model = difference_in_means) ## ## Formula: Y ~ Z ## ## Model: difference_in_means ## ## A single draw of the estimator: ## estimator_label term estimate std.error statistic p.value conf.low ## estimator Z 1.010462 0.1370808 7.371286 1.768003e-09 0.7349878 ## conf.high df outcome estimand_label ## 1.285936 49 Y ATE Applications Simulating data fake_data &lt;- draw_data(design) head(fake_data) ## ID noise Y_Z_0 Y_Z_1 S_inclusion_prob Z Z_cond_prob Y ## 1 0001 -0.2533829 0 0.7466171 0.25 1 0.2 0.7466171 ## 2 0002 1.2077610 0 2.2077610 0.25 0 0.8 0.0000000 ## 3 0009 -0.5574354 0 0.4425646 0.25 0 0.8 0.0000000 ## 4 0010 1.2574286 0 2.2574286 0.25 1 0.2 2.2574286 ## 5 0011 -0.5511836 0 0.4488164 0.25 0 0.8 0.0000000 ## 6 0016 -1.1025132 0 -0.1025132 0.25 1 0.2 -0.1025132 Estimation estimates &lt;- draw_estimates(design) estimates ## estimator_label term estimate std.error statistic p.value conf.low ## 1 estimator Z 1.194527 0.1361679 8.77246 1.290575e-11 0.9208878 ## conf.high df outcome estimand_label ## 1 1.468167 49 Y ATE Diagnosis Diagnosis is done using a Monte Carlo approach (=simulating many times using bootstrapped samples). diagnosis &lt;- diagnose_design(design, sims = 1000, bootstrap_sims = 500) diagnosis ## ## Research design diagnosis based on 1000 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (500 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power ## design ATE estimator Z 1000 -0.00 0.13 1.00 ## (0.00) (0.00) (0.00) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.96 1.00 0.14 0.14 0.00 1.00 ## (0.01) (0.00) (0.00) (0.00) (0.00) (0.00) # Very weakly biased # Well powered. Power = 1 - Type II error (False Negative Rate). Therefore, higher statistical power means lower false negative rate. # Coverage (standard errors) is fine. 95% of the time right answers lie in the coverage. Shortcuts Doing the power analysis for a three-arm experiment. three_arm &lt;- multi_arm_designer(N = 1000, m = 3, outcome_means = c(0, 0.1, 0.2)) diagnose_design(three_arm) ## ## Research design diagnosis based on 500 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (100 replicates). ## ## Design Label Estimand Label Estimator Label N Sims Bias RMSE Power ## three_arm ate_Y_2_1 DIM (Z_2 - Z_1) 500 0.00 0.07 0.26 ## (0.00) (0.00) (0.02) ## three_arm ate_Y_3_1 DIM (Z_3 - Z_1) 500 0.00 0.07 0.75 ## (0.00) (0.00) (0.02) ## three_arm ate_Y_3_2 DIM (Z_3 - Z_2) 500 0.00 0.08 0.26 ## (0.00) (0.00) (0.02) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.97 0.10 0.07 0.08 0.00 0.10 ## (0.01) (0.00) (0.00) (0.00) (0.00) (0.00) ## 0.96 0.20 0.07 0.08 0.00 0.20 ## (0.01) (0.00) (0.00) (0.00) (0.00) (0.00) ## 0.96 0.10 0.08 0.08 0.00 0.10 ## (0.01) (0.00) (0.00) (0.00) (0.00) (0.00) 4.11 Visualizing (ggplot2) The following material is adapted from Kieran Healy’s wonderful book (2019) on data visualization and Hadley Wickham’s equally wonderful book on ggplot2. For more theoretical discussions, I recommend you to read The Grammar of Graphics by Leland Wilkinson. Why should we care data visualization? More precisely, why should we learn the grammar of statistical graphics? Sometimes, pictures are better tools than words in 1) exploring, 2) understanding, and 3) explaining data. 4.11.1 Motivation Anscombe’s quarter comprises four datasets, which are so alike in terms of their descriptive statistics but quite different when presented graphically. # Set theme theme_set(theme_minimal()) # Data anscombe ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 ## 7 6 6 6 8 7.24 6.13 6.08 5.25 ## 8 4 4 4 19 4.26 3.10 5.39 12.50 ## 9 12 12 12 8 10.84 9.13 8.15 5.56 ## 10 7 7 7 8 4.82 7.26 6.42 7.91 ## 11 5 5 5 8 5.68 4.74 5.73 6.89 # Correlation cor(anscombe)[c(1:4), c(5:8)] ## y1 y2 y3 y4 ## x1 0.8164205 0.8162365 0.8162867 -0.3140467 ## x2 0.8164205 0.8162365 0.8162867 -0.3140467 ## x3 0.8164205 0.8162365 0.8162867 -0.3140467 ## x4 -0.5290927 -0.7184365 -0.3446610 0.8165214 # gather and select anscombe_processed &lt;- anscombe %&gt;% gather(x_name, x_value, x1:x4) %&gt;% gather(y_name, y_value, y1:y4) # plot anscombe_processed %&gt;% ggplot(aes(x = x_value, y = y_value)) + geom_point() + geom_smooth(method = lm, se = FALSE) + facet_grid(x_name ~ y_name) + theme_bw() + labs( x = &quot;X values&quot;, y = &quot;Y values&quot;, title = &quot;Anscombe&#39;s quartet&quot; ) ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.11.2 The grammar of graphics the grammar of graphics data aesthetic attributes (color, shape, size) geometric objects (points, lines, bars) stats (summary stats) scales (map values in the data space) coord (data coordinates) facet (facetting specifications) No worries for new terms. We’re going to learn them by actually plotting. Workflow: Tidy data Mapping Geom Cor_ordinates and scales Labels and guides Themes Save files 4.11.3 mapping and geom aes (aesthetic mappings or aesthetics) tells which variables (x, y) in your data should be represented by which visual elements (color, shape, size) in the plot. geom_ tells the type of plot you are going to use 4.11.4 basic aes (x , y) p &lt;- ggplot( data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp) ) # ggplot or R in general takes positional arguments too. So, you don&#39;t need to name data, mapping each time you use ggplot2. p p + geom_point() p + geom_point() + geom_smooth() # geom_smooth has calculated a smoothed line; ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; # the shaded area is the standard error for the line 4.11.5 Univariate distribution geom_histogram(): For the probability distribution of a continuous variable. Bins divide the entire range of values into a series of intervals (see the Wiki entry). geom_density(): Also for the probability distribution of a continuous variable. It calculates a kernel density estimate of the underlying distribution. 4.11.5.1 Histogram data(midwest) # load midwest dataset midwest ## # A tibble: 437 x 28 ## PID county state area poptotal popdensity popwhite popblack popamerindian ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 561 ADAMS IL 0.052 66090 1271. 63917 1702 98 ## 2 562 ALEXA… IL 0.014 10626 759 7054 3496 19 ## 3 563 BOND IL 0.022 14991 681. 14477 429 35 ## 4 564 BOONE IL 0.017 30806 1812. 29344 127 46 ## 5 565 BROWN IL 0.018 5836 324. 5264 547 14 ## 6 566 BUREAU IL 0.05 35688 714. 35157 50 65 ## 7 567 CALHO… IL 0.017 5322 313. 5298 1 8 ## 8 568 CARRO… IL 0.027 16805 622. 16519 111 30 ## 9 569 CASS IL 0.024 13437 560. 13384 16 8 ## 10 570 CHAMP… IL 0.058 173025 2983. 146506 16559 331 ## # … with 427 more rows, and 19 more variables: popasian &lt;int&gt;, popother &lt;int&gt;, ## # percwhite &lt;dbl&gt;, percblack &lt;dbl&gt;, percamerindan &lt;dbl&gt;, percasian &lt;dbl&gt;, ## # percother &lt;dbl&gt;, popadults &lt;int&gt;, perchsd &lt;dbl&gt;, percollege &lt;dbl&gt;, ## # percprof &lt;dbl&gt;, poppovertyknown &lt;int&gt;, percpovertyknown &lt;dbl&gt;, ## # percbelowpoverty &lt;dbl&gt;, percchildbelowpovert &lt;dbl&gt;, percadultpoverty &lt;dbl&gt;, ## # percelderlypoverty &lt;dbl&gt;, inmetro &lt;int&gt;, category &lt;chr&gt; midwest %&gt;% ggplot(aes(x = area)) + geom_point() # not working. midwest %&gt;% ggplot(aes(x = area)) + geom_histogram() # stat_bin argument picks up 30 bins (or &quot;bucket&quot;) by default. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. midwest %&gt;% ggplot(aes(x = area)) + geom_histogram(bins = 10) # only 10 bins. ggplot( data = subset(midwest, state %in% c(&quot;OH&quot;, &quot;IN&quot;)), mapping = aes(x = percollege, fill = state) ) + geom_histogram(alpha = 0.7, bins = 20) + scale_fill_viridis_d() 4.11.5.2 Density midwest %&gt;% ggplot(aes(x = area, fill = state, color = state)) + geom_density(alpha = 0.3) + scale_color_viridis_d() + scale_fill_viridis_d() 4.11.6 Advanced aes (size, color) There’s also fill argument (mostly used in geom_bar()). Color aes affects the appearance of lines and points, fill is for the filled areas of bars, polygons, and in some cases, the interior of a smoother’s standard error ribbon. The property size/color/fill represents… ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, size = pop ) ) + geom_point() ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, size = pop, color = continent ) ) + geom_point() + scale_color_viridis_d() # try red instead of &quot;red&quot; ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, size = pop, color = &quot;red&quot; ) ) + geom_point() Aesthetics also can be mapped per Geom. p + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; p + geom_point(alpha = 0.3) + # alpha controls transparency geom_smooth(color = &quot;red&quot;, se = FALSE, size = 2) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; p + geom_point(alpha = 0.3) + # alpha controls transparency geom_smooth(color = &quot;red&quot;, se = FALSE, size = 2, method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, color = continent ) ) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;loess&quot;, color = &quot;red&quot;) + labs( x = &quot;log GDP&quot;, y = &quot;Life Expectancy&quot;, title = &quot;A Gapminder Plot&quot;, subtitle = &quot;Data points are country-years&quot;, caption = &quot;Source: Gapminder&quot; ) ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, color = continent, fill = continent ) ) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;loess&quot;, color = &quot;red&quot;) + labs( x = &quot;log GDP&quot;, y = &quot;Life Expectancy&quot;, title = &quot;A Gapminder Plot&quot;, subtitle = &quot;Data points are country-years&quot;, caption = &quot;Source: Gapminder&quot; ) + scale_color_viridis_d() + scale_fill_viridis_d() ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.11.7 Co-ordinates and scales p + geom_point() + coord_flip() # coord_type The data is heavily bunched up against the left side. p + geom_point() # without scaling p + geom_point() + scale_x_log10() # scales the axis of a plot to a log 10 basis p + geom_point() + geom_smooth(method = &quot;lm&quot;) + scale_x_log10() ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.11.8 Labels and guides scales package has some useful premade formatting functions. You can either load scales or just grab the function you need from the library using scales:: p + geom_point(alpha = 0.3) + geom_smooth(method = &quot;loess&quot;, color = &quot;red&quot;) + scale_x_log10(labels = scales::dollar) + labs( x = &quot;log GDP&quot;, y = &quot;Life Expectancy&quot;, title = &quot;A Gapminder Plot&quot;, subtitle = &quot;Data points are country-years&quot;, caption = &quot;Source: Gapminder&quot; ) ## `geom_smooth()` using formula &#39;y ~ x&#39; Themes p + geom_point(alpha = 0.3) + geom_smooth(method = &quot;loess&quot;, color = &quot;red&quot;) + scale_x_log10(labels = scales::dollar) + labs( x = &quot;log GDP&quot;, y = &quot;Life Expectancy&quot;, title = &quot;A Gapminder Plot&quot;, subtitle = &quot;Data points are country-years&quot;, caption = &quot;Source: Gapminder&quot; ) + theme_economist() ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.11.9 ggsave figure_example &lt;- p + geom_point(alpha = 0.3) + geom_smooth(method = &quot;gam&quot;, color = &quot;red&quot;) + scale_x_log10(labels = scales::dollar) + labs( x = &quot;log GDP&quot;, y = &quot;Life Expectancy&quot;, title = &quot;A Gapminder Plot&quot;, subtitle = &quot;Data points are country-years&quot;, caption = &quot;Source: Gapminder&quot; ) + theme_economist() ggsave(figure_example, here(&quot;outputs&quot;, &quot;figure_example.png&quot;)) 4.11.10 Many plots Basic ideas: Grouping: tell ggplot2 about the structure of your data Facetting: break up your data into pieces for a plot 4.11.10.1 Grouping Can you guess what’s wrong? p &lt;- ggplot(gapminder, aes(x = year, y = gdpPercap)) p + geom_point() p + geom_line() geom_line joins up all the lines for each particular year in the order they appear in the dataset. ggplot2 does not know the yearly observations in your data are grouped by country. Note that you need grouping when the grouping information you need to tell is not built into the variables being mapped (like continent). gapminder ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap log_pop ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 15.9 ## 2 Afghanistan Asia 1957 30.3 9240934 821. 16.0 ## 3 Afghanistan Asia 1962 32.0 10267083 853. 16.1 ## 4 Afghanistan Asia 1967 34.0 11537966 836. 16.3 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 16.4 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 16.5 ## 7 Afghanistan Asia 1982 39.9 12881816 978. 16.4 ## 8 Afghanistan Asia 1987 40.8 13867957 852. 16.4 ## 9 Afghanistan Asia 1992 41.7 16317921 649. 16.6 ## 10 Afghanistan Asia 1997 41.8 22227415 635. 16.9 ## # … with 1,694 more rows 4.11.10.2 Facetting Facetting is to make small multiples. facet_wrap: based on a single categorical variable like facet_wrap(~single_categorical_variable). Your panels will be laid out in order and then wrapped into a grid. facet_grid: when you want to cross-classify some data by two categorical variables like facet_grid(one_cat_variable ~ two_cat_variable). p &lt;- ggplot(gapminder, aes(x = year, y = gdpPercap)) p + geom_line(aes(group = country)) # group by, # The outlier is Kuwait. p + geom_line(aes(group = country)) + facet_wrap(~continent) # facetting p + geom_line(aes(group = country), color = &quot;gray70&quot;) + geom_smooth(size = 1.1, method = &quot;loess&quot;, se = FALSE) + scale_y_log10(labels = scales::dollar) + facet_wrap(~continent, ncol = 5) + # for single categorical variable; for multiple categorical variables use facet_grid() labs( x = &quot;Year&quot;, y = &quot;GDP per capita&quot;, title = &quot;GDP per capita on Five continents&quot; ) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) ## `geom_smooth()` using formula &#39;y ~ x&#39; p + geom_line(aes(group = country), color = &quot;gray70&quot;) + geom_smooth(size = 1.1, method = &quot;loess&quot;, se = FALSE) + scale_y_log10(labels = scales::dollar) + facet_grid(~continent) + # for single categorical variable; for multiple categorical variables use facet_grid() labs( x = &quot;Year&quot;, y = &quot;GDP per capita&quot;, title = &quot;GDP per capita on Five continents&quot; ) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.11.11 Transforming Transforming: perform some calculations on or summarize your data before producing the plot 4.11.11.1 Use pipes to summarize data Also, we experiment bar charts here. By default, geom_bar uses stat = “bins,” which makes the height of each bar equal to the number of cases in each group. If you have a y column, then you should use stat = \"identity\" argument. Alternatively, you can use geom_col(). gapminder_formatted &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the `.groups` argument. ggplot(data = gapminder_formatted, aes(x = year, y = lifeExp_mean, color = continent)) + geom_point() + labs( x = &quot;Year&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy on Five continents&quot; ) gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% group_by(country, year) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = year, y = lifeExp_mean, color = country)) + geom_point() + labs( x = &quot;Year&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy in Europe&quot; ) ## `summarise()` has grouped output by &#39;country&#39;. You can override using the `.groups` argument. # geom point gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% group_by(country, year) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = year, y = lifeExp_mean)) + geom_point() + labs( x = &quot;Year&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy in Europe&quot; ) + facet_wrap(~country) ## `summarise()` has grouped output by &#39;country&#39;. You can override using the `.groups` argument. # geom bar gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% group_by(country, year) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = year, y = lifeExp_mean)) + geom_bar(stat = &quot;identity&quot;) + labs( x = &quot;Year&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy in Europe&quot; ) + facet_wrap(~country) ## `summarise()` has grouped output by &#39;country&#39;. You can override using the `.groups` argument. # no facet gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% group_by(country, year) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = year, y = lifeExp_mean, fill = country)) + geom_bar(stat = &quot;identity&quot;) + # even if you not stack, still the plot looks messy or you can use geom_col() labs( x = &quot;Year&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy in Europe&quot; ) ## `summarise()` has grouped output by &#39;country&#39;. You can override using the `.groups` argument. gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% group_by(country, year) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = country, y = lifeExp_mean)) + geom_boxplot() + labs( x = &quot;Country&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy in Europe&quot; ) + coord_flip() ## `summarise()` has grouped output by &#39;country&#39;. You can override using the `.groups` argument. # without ordering gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% group_by(country, year) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = reorder(country, lifeExp_mean), y = lifeExp_mean)) + geom_boxplot() + labs( x = &quot;Country&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy in Europe&quot; ) + coord_flip() ## `summarise()` has grouped output by &#39;country&#39;. You can override using the `.groups` argument. # reorder gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% group_by(country, year) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = reorder(country, -lifeExp_mean), y = lifeExp_mean)) + geom_boxplot() + labs( x = &quot;Country&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy in Europe&quot; ) + coord_flip() ## `summarise()` has grouped output by &#39;country&#39;. You can override using the `.groups` argument. 4.11.11.2 Plotting text gapminder %&gt;% filter(continent == &quot;Asia&quot; | continent == &quot;Americas&quot;) %&gt;% group_by(continent, country) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = gdp_mean, y = lifeExp_mean)) + geom_point() + geom_text(aes(label = country)) + scale_x_log10() + facet_grid(~continent) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the `.groups` argument. # with label gapminder %&gt;% filter(continent == &quot;Asia&quot; | continent == &quot;Americas&quot;) %&gt;% group_by(continent, country) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = gdp_mean, y = lifeExp_mean)) + geom_point() + geom_label(aes(label = country)) + scale_x_log10() + facet_grid(~continent) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the `.groups` argument. # no overlaps gapminder %&gt;% filter(continent == &quot;Asia&quot; | continent == &quot;Americas&quot;) %&gt;% group_by(continent, country) %&gt;% summarize( gdp_mean = mean(gdpPercap), lifeExp_mean = mean(lifeExp) ) %&gt;% ggplot(aes(x = gdp_mean, y = lifeExp_mean)) + geom_point() + geom_text_repel(aes(label = country)) + # there&#39;s also geom_label_repel scale_x_log10() + facet_grid(~continent) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the `.groups` argument. ## Warning: ggrepel: 2 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps ## Warning: ggrepel: 1 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps 4.11.12 Ploting models In plotting models, we extensively use David Robinson’s broom package in R. The idea is to transform model outputs (i.e., predictions and estimations) into tidy objects so that we can easily combine, separate, and visualize these elements. 4.11.12.1 Plotting several fits at the same time model_colors &lt;- RColorBrewer::brewer.pal(3, &quot;Set1&quot;) # select three qualitatively different colors from a larger palette. gapminder %&gt;% ggplot(aes(x = log(gdpPercap), y = lifeExp)) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;, aes(color = &quot;OLS&quot;, fill = &quot;OLS&quot;)) + geom_smooth( method = &quot;lm&quot;, formula = y ~ splines::bs(x, df = 3), aes(color = &quot;Cubic Spline&quot;, fill = &quot;Cubic Spline&quot;) ) + geom_smooth(method = &quot;loess&quot;, aes(color = &quot;LOESS&quot;, fill = &quot;LOESS&quot;)) + theme(legend.position = &quot;top&quot;) + scale_color_manual(name = &quot;Models&quot;, values = model_colors) + scale_fill_manual(name = &quot;Models&quot;, values = model_colors) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.11.12.2 Extracting model outcomes # regression model out &lt;- lm( formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder ) tidy() is a method in the broom package. It “constructs a dataframe that summarizes the model’s statistical findings.” As the description states, tidy is a function that can be used for various models. For instance, a tidy can extract following information from a regression model. Term: a term being estimated p.value statistic: a test statistic used to compute p-value estimate conf.low: the low end of a confidence interval conf.high: the high end of a confidence interval df: degrees of freedom Challenge Try glance(out), what did you get from these commands? If you’re curious, you can try ?glance. The followings are to show your degree of confidence. 4.11.12.2.1 Coeffficients # estimates out_comp &lt;- tidy(out) p &lt;- out_comp %&gt;% ggplot(aes(x = term, y = estimate)) p + geom_point() + coord_flip() + theme_bw() 4.11.12.2.2 Confidence intervals # plus confidence intervals out_conf &lt;- tidy(out, conf.int = TRUE) # plotting coefficients using ggplot2 (pointrange) out_conf %&gt;% ggplot(aes(x = reorder(term, estimate), y = estimate, ymin = conf.low, ymax = conf.high)) + geom_pointrange() + coord_flip() + labs(x = &quot;&quot;, y = &quot;OLS Estimate&quot;) + theme_bw() # another way to do it (errorbar) out_conf %&gt;% ggplot(aes(x = estimate, y = reorder(term, estimate))) + geom_point() + geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) + labs(y = &quot;&quot;, x = &quot;OLS Estimate&quot;) + theme_bw() You can calculate marginal effects using margins package. For the sake of time, I’m not covering that here. "],["functional-programming.html", "Chapter 5 Automating repeated things 5.1 Flow control 5.2 purrr 5.3 Automote 2 or 2+ tasks 5.4 Automate plotting 5.5 Automate joining 5.6 Make automation slower or faster 5.7 Make error handling easier 5.8 Developing your own data products", " Chapter 5 Automating repeated things Anything that can be automated should be automated. Do as little as possible by hand. Do as much as possible with functions. - Hadley Wickham Setup # Install packages if (!require(&quot;pacman&quot;)) { install.packages(&quot;pacman&quot;) } pacman::p_load( tidyverse, # tidyverse pkgs including purrr bench, # performance test tictoc, # performance test broom, # tidy modeling glue, # paste string and objects furrr, # parallel processing rvest, # web scraping devtools, # dev tools usethis, # workflow roxygen2, # documentation testthat, # testing patchwork) # arranging ggplots 5.1 Flow control Control structures = putting logic in code to control flow (e.g., if, else, for, while, repeat, break, next) Almost all the conditional operators used in Python also work in R. The basic loop set up is also very similar, with some small syntax adjustments. if() is a function whose arguments must be specified inside parentheses. else, however, is a reserved operator that takes no arguments. Note that there is no elif option — one simply writes else if(). Whereas operations to be executed after conditional evaluations in Python come after a :, R operations must only be enclosed in curly brackets: {}. Furthermore, there is no requirement for indentation. x &lt;- 5 if (x &lt; 0) { # Condition print(&quot;x is negative&quot;) # Do something } x &lt;- -5 if (x &lt; 0) { print(&quot;x is negative&quot;) } ## [1] &quot;x is negative&quot; x &lt;- 5 if (x &lt; 0) { print(&quot;x is negative&quot;) } else{ print(&quot;x is positive&quot;) } ## [1] &quot;x is positive&quot; x &lt;- 0 if (x &lt; 0) { # Condition print(&quot;x is negative&quot;) # Do something } else if (x == 0) { print(&quot;x is zero&quot;) # Do something else } else {print(&quot;x is positive&quot;) # Do something else } ## [1] &quot;x is zero&quot; R also does some class coercion that makes Boolean evaluations harder to break than in Python. But be careful — R has a set of special coercion used for fast logical evaluation and subsetting. Specifically, TRUE is considered equal to 1, while FALSE is equal to 0. The Boolean logicals can also be specified as a full word in all caps, or simply as T or F. 1 &lt; 2 ## [1] TRUE &quot;1&quot; &lt; 2 ## [1] TRUE &quot;a&quot; &lt; 2 ## [1] FALSE TRUE &lt; 2 ## [1] TRUE TRUE == &quot;TRUE&quot; ## [1] TRUE T == &quot;TRUE&quot; ## [1] TRUE TRUE == &quot;T&quot; ## [1] FALSE TRUE == &quot;FALSE&quot; ## [1] FALSE TRUE == 0 ## [1] FALSE TRUE == 1 ## [1] TRUE FALSE == 0 ## [1] TRUE FALSE &lt;= 1 ## [1] TRUE 5.1.1 Functions While functions are defined in Python using the def reserved operator, R sees functions as just another type of named object. Thus, they require explicit assignment to an object. This is done using the function function(), which creates a function taking the arguments specified in parentheses. function = input + computation (begin -&gt; end) + output simple.function &lt;- function(x){ print(x + 1) } simple.function(x = 2) ## [1] 3 less.simple.function &lt;- function(x, y){ print(x - y + 1) } less.simple.function(x = 2, y = 10) ## [1] -7 With respect to returning function output, most of the same rules apply as with Python. Be sure to remember that return() will only process a single object, so multiple items must usually be returned as a list. Note that your ordering of the functions matters, too. dumbfun &lt;- function(x){ return(x) print(&quot;This will never print :(&quot;) } dumbfun(x = &quot;something&quot;) ## [1] &quot;something&quot; dumbfun &lt;- function(x){ print(&quot;Why did I print?&quot;) return(x) } dumbfun(x = &quot;something&quot;) ## [1] &quot;Why did I print?&quot; ## [1] &quot;something&quot; dumbfun &lt;- function(x,y){ thing1 &lt;- x thing2 &lt;- y return(list(thing1, thing2)) } dumbfun(x = &quot;some text&quot;, y = &quot;some data&quot;) ## [[1]] ## [1] &quot;some text&quot; ## ## [[2]] ## [1] &quot;some data&quot; dumbfun(x = c(5,10,15), y = &quot;some data&quot;) ## [[1]] ## [1] 5 10 15 ## ## [[2]] ## [1] &quot;some data&quot; R functions also allow you to set default argument values: less.simple.function &lt;- function(x, y = 0){ print(x - y + 1) } less.simple.function(x = 2) ## [1] 3 less.simple.function(x = 2, y = 10) ## [1] -7 With respect to specifying arguments, one can either use argument position specifications (i.e., the order) or argument name specifications. The latter is strongly preferred, as it is very easy to accidentally specify incorrect argument values. send &lt;- function(message, recipient, cc=NULL, bcc=NULL){ print(paste(message, recipient, sep = &quot;, &quot;)) print(paste(&quot;CC:&quot;, cc, sep = &quot; &quot;)) print(paste(&quot;BCC:&quot;, bcc, sep = &quot; &quot;)) } send(message = &quot;Hello&quot;, recipient = &quot;World&quot;, cc = &quot;Sun&quot;, bcc = &quot;Jane&quot;) ## [1] &quot;Hello, World&quot; ## [1] &quot;CC: Sun&quot; ## [1] &quot;BCC: Jane&quot; send(&quot;Hello&quot;, &quot;World&quot;, &quot;Sun&quot;, &quot;Jane&quot;) ## [1] &quot;Hello, World&quot; ## [1] &quot;CC: Sun&quot; ## [1] &quot;BCC: Jane&quot; send(&quot;Hello&quot;, &quot;Sun&quot;, &quot;Jane&quot;, &quot;World&quot;) ## [1] &quot;Hello, Sun&quot; ## [1] &quot;CC: Jane&quot; ## [1] &quot;BCC: World&quot; send(message = &quot;Hello&quot;, cc = &quot;Sun&quot;, bcc = c(&quot;Jane&quot;, &quot;Rochelle&quot;), recipient = &quot;World&quot;) ## [1] &quot;Hello, World&quot; ## [1] &quot;CC: Sun&quot; ## [1] &quot;BCC: Jane&quot; &quot;BCC: Rochelle&quot; Also, note that functions don’t have what CS people called side-effects. Functions only define local variables = They don’t change objects stored in the global environment. (Consider the difference between &lt;- and = for assignments.) That’s why you can use functions for reusable tasks since it does not interrupt other important things in your system. See the following example from Wilkinson. a = 1 b = 2 f &lt;- function(x) { a*x + b } f(2) ## [1] 4 g &lt;- function(x) { a = 2 b = 1 f(x) } g(2) # a equals still 1 ## [1] 4 Additional tips Nonstandard evaluation Nonstandard evaluation is an advanced subject. If you feel overwhelmed, you are more than welcome to skip this. But if you are serious in R programming, this is something you want to check out. For deeper understanding of this issue, I recommend reading Ren Kun’s very informative blog post carefully. This part draws on one of the [the dplyr package articles](https://dplyr.tidyverse.org/articles/programming.html. In tidyverse, calling a variable with or without quotation mark (string or not) does make little difference because tidyeval is a type of non-standard evaluation. This flexibility runs into the following problem when it comes to programming. # Using `mpg` instead of `mtcars$mpg` is called data masking. mtcars %&gt;% select(mpg) mtcars %&gt;% select(&quot;mpg&quot;) Data and env-variables # df = environment variable df &lt;- data.frame( x = c(1:5), y = c(6:10) ) # x, y = data variables df$x ## [1] 1 2 3 4 5 df$y ## [1] 6 7 8 9 10 Problem x &lt;- NULL var_summary &lt;- function(env_var, data_var){ env_var %&gt;% summarise(mean = mean(data_var)) } You may expect that the output is mean = 2.5 … but It’s because the mean() function doesn’t take df$x for data_var but x. You need to link x with environment variable var_summary(df, x) ## Warning in mean.default(data_var): argument is not numeric or logical: returning ## NA ## mean ## 1 NA This is how you can fix this. # Solution vs_fix &lt;- function(env_var, data_var){ env_var %&gt;% summarise(mean = mean({{data_var}})) } # You can also do this. vs_fix_enhanced &lt;- function(env_var, data_var){ env_var %&gt;% summarise(&quot;mean_{{data_var}}&quot; := mean({{data_var}})) # If you use the glue package, this syntax is very intuitive. } vs_fix_enhanced(df, x) ## mean_x ## 1 3 If you have a character vector input … mtcars_count &lt;- mtcars %&gt;% names() %&gt;% purrr::map(~count(mtcars, .data[[.x]])) # We&#39;re going to learn about map in the rest of this session. mtcars_count[[1]] ## mpg n ## 1 10.4 2 ## 2 13.3 1 ## 3 14.3 1 ## 4 14.7 1 ## 5 15.0 1 ## 6 15.2 2 ## 7 15.5 1 ## 8 15.8 1 ## 9 16.4 1 ## 10 17.3 1 ## 11 17.8 1 ## 12 18.1 1 ## 13 18.7 1 ## 14 19.2 2 ## 15 19.7 1 ## 16 21.0 2 ## 17 21.4 2 ## 18 21.5 1 ## 19 22.8 2 ## 20 24.4 1 ## 21 26.0 1 ## 22 27.3 1 ## 23 30.4 2 ## 24 32.4 1 ## 25 33.9 1 5.1.2 for loop Concept map for a for loop. Source: https://teachtogether.tech/en/index.html#s:memory-concept-maps Loops in R also work basically the same way as in Python, with just a few adjustments. First, recall that index positions in R start at 1. Second, while() and for() are functions rather than reserved operators, meaning they must take arguments in parentheses. Third, just like else, the in operator is reserved and takes no arguments in parentheses. Fourth, the conditional execution must appear between curly brackets. Finally, indentation is meaningless, but each new operation must appear on a new line. while(): when we have no idea how many times loop needs to be executed. for(): when we know how many times loop needs to be executed. This is likely to be the loop you are going to use most frequently. fruits &lt;- c(&quot;apples&quot;, &quot;oranges&quot;, &quot;pears&quot;, &quot;bananas&quot;) # a while loop i &lt;- 1 while (i &lt;= length(fruits)) { print(fruits[i]) i &lt;- i + 1 } ## [1] &quot;apples&quot; ## [1] &quot;oranges&quot; ## [1] &quot;pears&quot; ## [1] &quot;bananas&quot; # a for loop for (i in 1:length(fruits)) { print(fruits[i]) } ## [1] &quot;apples&quot; ## [1] &quot;oranges&quot; ## [1] &quot;pears&quot; ## [1] &quot;bananas&quot; 5.1.3 apply family While and for loops in R can be very slow. For this reason, R has a number of built-in iteration methods to speed up execution times. In many cases, packages will have “behind-the-scenes” ways to avoid for loops, but what if you need to write your own function? A common method of getting around for loops is the apply family of functions. These take a data structure and a function, and applies a function over all the elements in the object. fruit &lt;- c(&quot;apple&quot;, &quot;orange&quot;, &quot;pear&quot;, &quot;banana&quot;) # make function that takes in only one element make.plural &lt;- function(x){ plural &lt;- paste(x, &#39;s&#39;, sep = &#39;&#39;) # sep is for collapse, so collpase &#39;&#39; return(plural) } make.plural(&#39;apple&#39;) ## [1] &quot;apples&quot; apply() : loop over the margins (1 = row, 2 = column) of an array lapply() : loop over a list then returns a list sapply() : loop over a list then returns a named vector tapply(): loop over subsets of a vector mapply(): multivariate version of lapply(). Use this if you have a function that takes in 2 or more arguments. # apply that function to every element lapply(fruit, make.plural) # returns a list ## [[1]] ## [1] &quot;apples&quot; ## ## [[2]] ## [1] &quot;oranges&quot; ## ## [[3]] ## [1] &quot;pears&quot; ## ## [[4]] ## [1] &quot;bananas&quot; sapply(fruit, make.plural) # returns a named vector ## apple orange pear banana ## &quot;apples&quot; &quot;oranges&quot; &quot;pears&quot; &quot;bananas&quot; library(purrr) # load package map(fruit, make.plural) # type consistent ## [[1]] ## [1] &quot;apples&quot; ## ## [[2]] ## [1] &quot;oranges&quot; ## ## [[3]] ## [1] &quot;pears&quot; ## ## [[4]] ## [1] &quot;bananas&quot; # Why sapply is bad sapply(1:100, paste) # return character ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; ## [13] &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; ## [25] &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; &quot;36&quot; ## [37] &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; ## [49] &quot;49&quot; &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; ## [61] &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; &quot;67&quot; &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; ## [73] &quot;73&quot; &quot;74&quot; &quot;75&quot; &quot;76&quot; &quot;77&quot; &quot;78&quot; &quot;79&quot; &quot;80&quot; &quot;81&quot; &quot;82&quot; &quot;83&quot; &quot;84&quot; ## [85] &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;92&quot; &quot;93&quot; &quot;94&quot; &quot;95&quot; &quot;96&quot; ## [97] &quot;97&quot; &quot;98&quot; &quot;99&quot; &quot;100&quot; sapply(integer(), paste) # return list! ## list() library(purrr) map(1:100, paste) # return list ## [[1]] ## [1] &quot;1&quot; ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] &quot;3&quot; ## ## [[4]] ## [1] &quot;4&quot; ## ## [[5]] ## [1] &quot;5&quot; ## ## [[6]] ## [1] &quot;6&quot; ## ## [[7]] ## [1] &quot;7&quot; ## ## [[8]] ## [1] &quot;8&quot; ## ## [[9]] ## [1] &quot;9&quot; ## ## [[10]] ## [1] &quot;10&quot; ## ## [[11]] ## [1] &quot;11&quot; ## ## [[12]] ## [1] &quot;12&quot; ## ## [[13]] ## [1] &quot;13&quot; ## ## [[14]] ## [1] &quot;14&quot; ## ## [[15]] ## [1] &quot;15&quot; ## ## [[16]] ## [1] &quot;16&quot; ## ## [[17]] ## [1] &quot;17&quot; ## ## [[18]] ## [1] &quot;18&quot; ## ## [[19]] ## [1] &quot;19&quot; ## ## [[20]] ## [1] &quot;20&quot; ## ## [[21]] ## [1] &quot;21&quot; ## ## [[22]] ## [1] &quot;22&quot; ## ## [[23]] ## [1] &quot;23&quot; ## ## [[24]] ## [1] &quot;24&quot; ## ## [[25]] ## [1] &quot;25&quot; ## ## [[26]] ## [1] &quot;26&quot; ## ## [[27]] ## [1] &quot;27&quot; ## ## [[28]] ## [1] &quot;28&quot; ## ## [[29]] ## [1] &quot;29&quot; ## ## [[30]] ## [1] &quot;30&quot; ## ## [[31]] ## [1] &quot;31&quot; ## ## [[32]] ## [1] &quot;32&quot; ## ## [[33]] ## [1] &quot;33&quot; ## ## [[34]] ## [1] &quot;34&quot; ## ## [[35]] ## [1] &quot;35&quot; ## ## [[36]] ## [1] &quot;36&quot; ## ## [[37]] ## [1] &quot;37&quot; ## ## [[38]] ## [1] &quot;38&quot; ## ## [[39]] ## [1] &quot;39&quot; ## ## [[40]] ## [1] &quot;40&quot; ## ## [[41]] ## [1] &quot;41&quot; ## ## [[42]] ## [1] &quot;42&quot; ## ## [[43]] ## [1] &quot;43&quot; ## ## [[44]] ## [1] &quot;44&quot; ## ## [[45]] ## [1] &quot;45&quot; ## ## [[46]] ## [1] &quot;46&quot; ## ## [[47]] ## [1] &quot;47&quot; ## ## [[48]] ## [1] &quot;48&quot; ## ## [[49]] ## [1] &quot;49&quot; ## ## [[50]] ## [1] &quot;50&quot; ## ## [[51]] ## [1] &quot;51&quot; ## ## [[52]] ## [1] &quot;52&quot; ## ## [[53]] ## [1] &quot;53&quot; ## ## [[54]] ## [1] &quot;54&quot; ## ## [[55]] ## [1] &quot;55&quot; ## ## [[56]] ## [1] &quot;56&quot; ## ## [[57]] ## [1] &quot;57&quot; ## ## [[58]] ## [1] &quot;58&quot; ## ## [[59]] ## [1] &quot;59&quot; ## ## [[60]] ## [1] &quot;60&quot; ## ## [[61]] ## [1] &quot;61&quot; ## ## [[62]] ## [1] &quot;62&quot; ## ## [[63]] ## [1] &quot;63&quot; ## ## [[64]] ## [1] &quot;64&quot; ## ## [[65]] ## [1] &quot;65&quot; ## ## [[66]] ## [1] &quot;66&quot; ## ## [[67]] ## [1] &quot;67&quot; ## ## [[68]] ## [1] &quot;68&quot; ## ## [[69]] ## [1] &quot;69&quot; ## ## [[70]] ## [1] &quot;70&quot; ## ## [[71]] ## [1] &quot;71&quot; ## ## [[72]] ## [1] &quot;72&quot; ## ## [[73]] ## [1] &quot;73&quot; ## ## [[74]] ## [1] &quot;74&quot; ## ## [[75]] ## [1] &quot;75&quot; ## ## [[76]] ## [1] &quot;76&quot; ## ## [[77]] ## [1] &quot;77&quot; ## ## [[78]] ## [1] &quot;78&quot; ## ## [[79]] ## [1] &quot;79&quot; ## ## [[80]] ## [1] &quot;80&quot; ## ## [[81]] ## [1] &quot;81&quot; ## ## [[82]] ## [1] &quot;82&quot; ## ## [[83]] ## [1] &quot;83&quot; ## ## [[84]] ## [1] &quot;84&quot; ## ## [[85]] ## [1] &quot;85&quot; ## ## [[86]] ## [1] &quot;86&quot; ## ## [[87]] ## [1] &quot;87&quot; ## ## [[88]] ## [1] &quot;88&quot; ## ## [[89]] ## [1] &quot;89&quot; ## ## [[90]] ## [1] &quot;90&quot; ## ## [[91]] ## [1] &quot;91&quot; ## ## [[92]] ## [1] &quot;92&quot; ## ## [[93]] ## [1] &quot;93&quot; ## ## [[94]] ## [1] &quot;94&quot; ## ## [[95]] ## [1] &quot;95&quot; ## ## [[96]] ## [1] &quot;96&quot; ## ## [[97]] ## [1] &quot;97&quot; ## ## [[98]] ## [1] &quot;98&quot; ## ## [[99]] ## [1] &quot;99&quot; ## ## [[100]] ## [1] &quot;100&quot; map(integer(), paste) # return list ## list() 5.2 purrr 5.2.1 Why map? 5.2.1.1 Objectives How to use purrr to automate workflow in a cleaner, faster, and more extendable way 5.2.1.2 Copy-and-paste programming Copy-and-paste programming, sometimes referred to as just pasting, is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), or certain programming idioms, and it is supported by some source code editors in the form of snippets. - Wikipedia The following exercise was inspired by Wickham’s example. Let’s imagine df is a survey dataset. a, b, c, d = Survey questions -99: non-responses Your goal: replace -99 with NA # Data set.seed(1234) # for reproducibility df &lt;- tibble( &quot;a&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;b&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;c&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;d&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE) ) # Copy and paste df$a[df$a == -99] &lt;- NA df$b[df$b == -99] &lt;- NA df$c[df$c == -99] &lt;- NA df$d[df$d == -99] &lt;- NA df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Challenge. Explain why this solution is not very efficient (Hint: If df$a[df$a == -99] &lt;- NA has an error, how are you going to fix it? A solution is not scalable if it’s not automatable. 5.2.1.3 Using a function Let’s recall what’s function in R: input + computation + output If you write a function, you gain efficiency because you don’t need to copy and paste the computation part. ` function(input){ computation return(output) } ` # Function fix_missing &lt;- function(x) { x[x == -99] &lt;- NA x } # Apply function to each column (vector) df$a &lt;- fix_missing(df$a) df$b &lt;- fix_missing(df$b) df$c &lt;- fix_missing(df$c) df$d &lt;- fix_missing(df$d) df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Challenge Why using function is more efficient than 100% copying and pasting? Can you think about a way we can automate the process? Many options for automation in R: for loop, apply family, etc. Here’s a tidy solution comes from purrr package. The power and joy of one-liner. df &lt;- purrr::map_df(df, fix_missing) df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 map() is a higher-order function that applies a given function to each element of a list/vector. This is how map() works. It’s easier to understand with a picture. - Input: Takes a vector/list. - Computation: Calls the function once for each element of the vector - Output: Returns in a list or whatever data format you prefer (e.g., `_df helper: dataframe`) Challenge If you run the code below, what’s going to be the data type of the output? map(df, fix_missing) ## $a ## [1] 3 3 1 1 NA ## ## $b ## [1] 3 2 NA NA 1 ## ## $c ## [1] 3 3 1 2 1 ## ## $d ## [1] 1 1 2 1 3 Why map() is a good alternative to for loop. The Joy of Functional Programming (for Data Science) - Hadley Wickham # Built-in data data(&quot;airquality&quot;) tic() # Placeholder out1 &lt;- vector(&quot;double&quot;, ncol(airquality)) # Sequence variable for (i in seq_along(airquality)) { # Assign an iteration result to each element of the placeholder list out1[[i]] &lt;- mean(airquality[[i]], na.rm = TRUE) } toc() ## 0.006 sec elapsed map is faster because it applies function to the items on the list/vector in parallel. Also, using map_dbl reduces an extra step you need to take. Hint: map_dbl(x, mean, na.rm = TRUE) = vapply(x, mean, na.rm = TRUE, FUN.VALUE = double(1)) tic() out1 &lt;- airquality %&gt;% map_dbl(mean, na.rm = TRUE) toc() ## 0.002 sec elapsed In short, map() is more readable, faster, and easily extendable with other data science tasks (e.g., wrangling, modeling, and visualization) using %&gt;%. Final point: Why not base R apply family? Short answer: purrr::map() is simpler to write. For instance, Additional tips Performance testing (profiling) is an important part of programming. tictic() measures the time that needs to take to run a target function for once. If you want a more robust measure of timing as well as information on memory (speed and space both matter for performance testing), consider using the bench package that is designed for high precising timing of R expressions. map_mark &lt;- bench::mark( out1 &lt;- airquality %&gt;% map_dbl(mean, na.rm = TRUE) ) map_mark ## # A tibble: 1 x 6 ## expression min median `itr/sec` ## &lt;bch:expr&gt; &lt;bch:&gt; &lt;bch:&gt; &lt;dbl&gt; ## 1 out1 &lt;- airquality %&gt;% map_dbl(mean, na.rm = TRUE) 66.9µs 76.1µs 7956. ## # … with 2 more variables: mem_alloc &lt;bch:byt&gt;, `gc/sec` &lt;dbl&gt; 5.2.1.4 Applications Many models One popular application of map() is to run regression models (or whatever model you want to run) on list-columns. No more copying and pasting for running many regression models on subgroups! # Have you ever tried this? lm_A &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_A&quot;)) lm_B &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_B&quot;)) lm_C &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_C&quot;)) lm_D &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_D&quot;)) lm_E &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_E&quot;)) For more information on this technique, read the Many Models subchapter of the R for Data Science. # Function lm_model &lt;- function(df) { lm(Temp ~ Ozone, data = df) } # Map models &lt;- airquality %&gt;% group_by(Month) %&gt;% nest() %&gt;% # Create list-columns mutate(ols = map(data, lm_model)) # Map models$ols[1] ## [[1]] ## ## Call: ## lm(formula = Temp ~ Ozone, data = df) ## ## Coefficients: ## (Intercept) Ozone ## 62.8842 0.1629 # Add tidying tidy_lm_model &lt;- purrr::compose( # compose multiple functions broom::tidy, # convert lm objects into tidy tibbles lm_model ) tidied_models &lt;- airquality %&gt;% group_by(Month) %&gt;% nest() %&gt;% # Create list-columns mutate(ols = map(data, tidy_lm_model)) tidied_models$ols[1] ## [[1]] ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 62.9 1.61 39.2 2.88e-23 ## 2 Ozone 0.163 0.0500 3.26 3.31e- 3 Simulations A good friend of map() function is rerun() function. This comibination is really useful for simulations. Consider the following example. Base R approach set.seed(1234) small_n &lt;- 100 ; k &lt;- 1000 ; mu &lt;- 500 ; sigma &lt;- 20 y_list &lt;- rep(list(NA), k) for (i in seq(k)) { y_list[[i]] &lt;- rnorm(small_n, mu, sigma) } y_means &lt;- unlist(lapply(y_list, mean)) qplot(y_means) + geom_vline(xintercept = 500, linetype = &quot;dotted&quot;, color = &quot;red&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. rerun() + map() small_n &lt;- 100 ; k &lt;- 1000 y_tidy &lt;- rerun(k, rnorm(small_n, mu, sigma)) y_means_tidy &lt;- map_dbl(y_tidy, mean) # Visualize (qplot(y_means) + geom_vline(xintercept = 500, linetype = &quot;dotted&quot;, color = &quot;red&quot;)) + (qplot(y_means_tidy) + geom_vline(xintercept = 500, linetype = &quot;dotted&quot;, color = &quot;red&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 5.3 Automote 2 or 2+ tasks 5.3.1 Objectives Learning how to use map2() and pmap() to avoid writing nested loops. 5.3.2 Problem Problem: How can you create something like below? [1] “University = Berkeley | Department = waterbenders” [1] “University = Berkeley | Department = earthbenders” [1] “University = Berkeley | Department = firebenders” [1] “University = Berkeley | Department = airbenders” [1] “University = Stanford | Department = waterbenders” [1] “University = Stanford | Department = earthbenders” [1] “University = Stanford | Department = firebenders” [1] “University = Stanford | Department = airbenders” The most manual way: You can copy and paste eight times. paste(&quot;University = Berkeley | Department = CS&quot;) ## [1] &quot;University = Berkeley | Department = CS&quot; 5.3.3 For loop A slightly more efficient way: using a for loop. Think about which part of the statement is constant and which part varies ( = parameters). Do we need a placeholder? No. We don’t need a placeholder because we don’t store the result of iterations. Challenge: How many parameters do you need to solve the problem below? # Outer loop for (univ in c(&quot;Berkeley&quot;, &quot;Stanford&quot;)) { # Inner loop for (dept in c(&quot;waterbenders&quot;, &quot;earthbenders&quot;, &quot;firebenders&quot;, &quot;airbenders&quot;)) { print(paste(&quot;University = &quot;, univ, &quot;|&quot;, &quot;Department = &quot;, dept)) } } ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Berkeley | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Berkeley | Department = airbenders&quot; ## [1] &quot;University = Stanford | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Stanford | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; This is not bad, but … n arguments -&gt; n-nested for loops. As a scale of your problem grows, your code gets really complicated. To become significantly more reliable, code must become more transparent. In particular, nested conditions and loops must be viewed with great suspicion. Complicated control flows confuse programmers. Messy code often hides bugs. — Bjarne Stroustrup 5.3.4 map2 &amp; pmap Step 1: Define inputs and a function. Challenge Why are we using rep() to create input vectors? For instance, for univ_list why not just use c(\"Berkeley\", \"Stanford\")? # Inputs (remember the length of these inputs should be identical) univ_list &lt;- rep(c(&quot;Berkeley&quot;, &quot;Stanford&quot;), 4) dept_list &lt;- rep(c(&quot;waterbenders&quot;, &quot;earthbenders&quot;, &quot;firebenders&quot;, &quot;airbenders&quot;), 2) # Function print_lists &lt;- function(univ, dept) { print(paste( &quot;University = &quot;, univ, &quot;|&quot;, &quot;Department = &quot;, dept )) } # Test print_lists(univ_list[1], dept_list[1]) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; Step2: Using map2() or pmap() # 2 arguments map2_output &lt;- map2(univ_list, dept_list, print_lists) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; # 3+ arguments pmap_output &lt;- pmap(list(univ_list, dept_list), print_lists) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; Challenge Have you noticed that we used a slightly different input for pmap() compared to map() or map2()? What is the difference? 5.4 Automate plotting 5.4.1 Objective Learning how to use map() and glue() to automate creating multiple plots 5.4.2 Problem Making the following data visualization process more efficient. data(&quot;airquality&quot;) airquality %&gt;% ggplot(aes(x = Ozone, y = Solar.R)) + geom_point() + labs( title = &quot;Relationship between Ozone and Solar.R&quot;, y = &quot;Solar.R&quot; ) ## Warning: Removed 42 rows containing missing values (geom_point). airquality %&gt;% ggplot(aes(x = Ozone, y = Wind)) + geom_point() + labs( title = &quot;Relationship between Ozone and Wind&quot;, y = &quot;Wind&quot; ) ## Warning: Removed 37 rows containing missing values (geom_point). airquality %&gt;% ggplot(aes(x = Ozone, y = Temp)) + geom_point() + labs( title = &quot;Relationship between Ozone and Temp&quot;, y = &quot;Temp&quot; ) ## Warning: Removed 37 rows containing missing values (geom_point). 5.4.3 Solution Learn how glue() works. glue() combines strings and objects and it works simpler and faster than paste() or sprintif(). names &lt;- c(&quot;Jae&quot;, &quot;Aniket&quot;, &quot;Avery&quot;) fields &lt;- c(&quot;Political Science&quot;, &quot;Law&quot;, &quot;Public Health&quot;) glue(&quot;{names} studies {fields}.&quot;) ## Jae studies Political Science. ## Aniket studies Law. ## Avery studies Public Health. So, our next step is to combine glue() and map(). Let’s first think about writing a function that includes glue(). Challenge How can you create the character vector of column names? Challenge How can you make ggplot2() take strings as x and y variable names? (Hint: Type ?aes_string()) airquality %&gt;% ggplot(aes_string(x = names(airquality)[1], y = names(airquality)[2])) + geom_point() + labs( title = glue(&quot;Relationship between Ozone and {names(airquality)[2]}&quot;), y = glue(&quot;{names(airquality)[2]}&quot;) ) ## Warning: Removed 42 rows containing missing values (geom_point). The next step is to write an automatic plotting function. Note that in the function argument i (abstract) replaced 2 (specific): abstraction create_point_plot &lt;- function(i) { airquality %&gt;% ggplot(aes_string(x = names(airquality)[1], y = names(airquality)[i])) + geom_point() + labs( title = glue(&quot;Relationship between Ozone and {names(airquality)[i]}&quot;), y = glue(&quot;{names(airquality)[i]}&quot;) ) } The final step is to put the function in map(). map(2:ncol(airquality), create_point_plot) ## [[1]] ## Warning: Removed 42 rows containing missing values (geom_point). ## ## [[2]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[3]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[4]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[5]] ## Warning: Removed 37 rows containing missing values (geom_point). 5.5 Automate joining 5.5.1 Objective Learning how to use reduce() to automate row-binding multiple dataframes 5.5.2 Problem How can you make row-binding multiple dataframes more efficient? df1 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) df2 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) df3 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) 5.5.3 Copy and paste first_bind &lt;- bind_rows(df1, df2) second_bind &lt;- bind_rows(first_bind, df3) Challenge Why the above solution is not efficient? 5.5.4 reduce How reduce() works. - Input: Takes a vector of length n - Computation: Calls a function with a pair of values at a time - Output: Returns a vector of length 1 reduced &lt;- reduce(list(df1, df2, df3), bind_rows) 5.6 Make automation slower or faster 5.6.1 Objectives Learning how to use slowly() and future_ to make automation process either slower or faster 5.6.2 How to make automation slower Scraping 50 pages from a website and you don’t want to overload the server. How can you do that? 5.6.3 For loop 5.6.4 Map walk() works same as map() but doesn’t store its output. If you’re web scraping, one problem with this approach is it’s too fast by human standards. If you want to make the function run slowly … slowly() takes a function and modifies it to wait a given amount of time between each call. - purrr package vignette - If a function is a verb, then a helper function is an adverb (modifying the behavior of the verb). 5.6.5 How to make automation Faster In a different situation, you want to make your function run faster. This is a common situation when you collect and analyze data at large-scale. You can solve this problem using parallel processing. For more on the parallel processing in R, read this review. Parallel processing setup Step1: Determine the number of max workers (availableCores()) Step2: Determine the parallel processing mode (plan()) 5.7 Make error handling easier 5.7.1 Learning objective Learning how to use safely() and possibly() to make error handling easier ### Problem Challenge Explain why we can’t run map(url_list, read_html) url_list &lt;- c( &quot;https://en.wikipedia.org/wiki/University_of_California,_Berkeley&quot;, &quot;https://en.wikipedia.org/wiki/Stanford_University&quot;, &quot;https://en.wikipedia.org/wiki/Carnegie_Mellon_University&quot;, &quot;https://DLAB&quot; ) map(url_list, read_html) This is a very simple problem so it’s easy to tell where the problem is. How can you make your error more informative? 5.7.2 Solution 5.7.2.1 Try-catch There are three kinds of messages you will run into, if your code has an error based on the following functions. stop(): errors; Functions must stop. warning(): warnings; Functions may still work. Nonetheless, something is possibly messed up. message(): messages; Some actions happened. The basic logic of try-catch, R’s basic error handling function, works like the following. tryCatch( { map(url_list, read_html) }, warning = function(w) { &quot;Warning&quot; }, error = function(e) { &quot;Error&quot; }, finally = { &quot;Message&quot; } ) ## [1] &quot;Error&quot; Here’s purrr version of the try-catch mechanism (evaluates code and assigns exception handlers). 5.7.2.2 safely Outputs result: result or NULL error: NULL or error map(url_list, safely(read_html)) ## [[1]] ## [[1]]$result ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[1]]$error ## NULL ## ## ## [[2]] ## [[2]]$result ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[2]]$error ## NULL ## ## ## [[3]] ## [[3]]$result ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[3]]$error ## NULL ## ## ## [[4]] ## [[4]]$result ## NULL ## ## [[4]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Could not resolve host: DLAB&gt; The easier way to solve this problem is just avoiding the error. map(url_list, safely(read_html)) %&gt;% map(&quot;result&quot;) %&gt;% # = map(function(x) x[[&quot;result&quot;]]) = map(~.x[[&quot;name&quot;]]) purrr::compact() # Remove empty elements ## [[1]] ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[2]] ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[3]] ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... 5.7.2.3 possibly What if the best way to solve the problem is not ignoring the error … # If error occurred, &quot;The URL is broken.&quot; will be stored in that element(s). out &lt;- map( url_list, possibly(read_html, otherwise = &quot;The URL is broken.&quot; ) ) # Let&#39;s find the broken URL. url_list[out[seq(out)] == &quot;The URL is broken.&quot;] ## [1] &quot;https://DLAB&quot; 5.8 Developing your own data products A data product is the production output from a statistical analysis. - Brian Caffo 5.8.1 Developing R packages Reuse your code Automate your workflow Help others (be part of an open source development community) 5.8.1.1 Workflow Write code in \\R Document code in \\man (automated by roxygen2 package) devtools::document() Check dependencies in NAMESPACE devtools::update() updates the documentation devtools::check() to see whether your package is ready to be submitted to CRAN Build a package (for more information, read this section in Hadley’s R package development book) devtools::build() (Optional) Test (devtools::test()), teach in \\vignettes, and add data in \\data Distribute the package either via CRAN or GitHub It’s time to learn five states of R code: source, bundled, binary, installed, and in-memory. If you’re just using an R package, you’re only concerned of the last two states: install.packages(\"pkg\") and library(pkg) If you’re developing an R package, you first write source code (*.R), bundle it (compressed file like *.tar.gz; done by devtools::build()), then make it binary (devtools::build(binary = TRUE); This is how a package is stored in CRAN/GitHub, etc.). 5.8.1.2 Required Components The 4 required components are necessary to build and distribute a minimally viable R package. The other steps are optional. Package \\R: R functions \\man: function documentations DESCRIPTION: provides meta data about the package (e.g., author) LICENSE GNU, MIT, etc. NAMESPACE: package dependencies (to make your package self-contained) README (optional) Setup (DESCRIPTION) # This function creates DESCRIPTION file usethis::create_package(here(&quot;mypkg&quot;)) # Initialize git repo usethis::use_git() # License the package # You can use the MIT license by typing devtools::use_mit_license(&quot;author name&quot;). The function produces MIT license related files (LICENSE, LICENSE.md). use_mit_license(&quot;Jae Yeon Kim&quot;) # Add README (optional) # Makes the package more use-friendly use_readme_md() # Add news (optional) # Helps track changes use_news_md() Write code (R) usethis::use_r(&quot;rbind_mutate&quot;) #&#39; Add two numbers #&#39; #&#39; @param x A number #&#39; @param y A number #&#39; @return The sum of x and y #&#39; @export add &lt;- function(x, y){ x + y } If you used a function from other packages you need to reference it in the following way: #' @importFrom &lt;package&gt; &lt;function&gt; Document (man) # Document # The function creates documentation related files (NAMESPACE, function_name.rd) devtools::document() # Check; updates the documentation; builds and checks the package devtools::check() Organize (NAMESPACE) usethis::use_package(&quot;dplyr&quot;) 5.8.1.3 Optional Components Test (test) usethis::use_testthat() usethis::use_test(&quot;rbind_mutate&quot;) Add data (data) x &lt;- &quot;Jae&quot; y &lt;- &quot;Sun&quot; z &lt;- &quot;Jane&quot; usethis::use_data(x, y, z, overwrite = TRUE) Teach (vignetts) usethis::use_vignette(&quot;rbind_mutate&quot;) title: &quot;Vignette title&quot; author: &quot;Vignette author&quot; date: &quot;2021-01-16&quot; output: rmarkdown::html_vignette vignette: blah blah You can build a package website using pkgdown # install.packages(&quot;pkgdown&quot;) usethis::use_pkgdown() pkgdown::build_site() A package site includes information on METADATA, Function references, Articles, News, etc. 5.8.1.4 Building an R package CMD (in the terminal) You can run R commands in the terminal using R CMD. devtools # Build devtools::build() # Install devtools::install() 5.8.1.5 Distributing an R package # Version update usethis::use_version() # Spell check usethis::use_spell_check() CRAN (The Comprehensive R Archive Network) R package submission should comply with the CRAN Repository Policy GitHub Push everything to the Git repository (you can do it using command-line interface or RStudio). Don’t forget that your repository should be public. I highly recommend connecting GitHub with SSH. For more information, visit this link. Additional tips If you want to use pipe operator (%) in your functions, save the following script as utils-pipe.R. #&#39; Pipe operator #&#39; #&#39; See \\code{magrittr::\\link[magrittr:pipe]{\\%&gt;\\%}} for details. #&#39; #&#39; @name %&gt;% #&#39; @rdname pipe #&#39; @keywords internal #&#39; @export #&#39; @importFrom magrittr %&gt;% #&#39; @usage lhs \\%&gt;\\% rhs NULL ## NULL Sometimes, you get the following error: “Undefined global functions or variables” If you experience this problem, save the following script as globals.r. utils::globalVariables(c(&quot;&lt;undefined variable name1&gt;&quot;, &quot;&lt;undefined variable name2&gt;&quot;, &quot;&lt;undefinedvariable name3&quot;)) ## [1] &quot;&lt;undefined variable name1&gt;&quot; &quot;&lt;undefined variable name2&gt;&quot; ## [3] &quot;&lt;undefinedvariable name3&quot; 5.8.2 Developing Shiny apps Shiny is a “framework for creating web applications using R code.” You can create a dashboard or an interactive map without knowing anything about HTML, CSS, or JavaScript. Developing a shiny app helps people with little technical expertise to learn from your data in an intuitive and interactive way. Shiny in production: Principles, practices, and tools - Joe Cheng COVID-19 tracker by Edward Parker 5.8.2.1 Workflow The workflow follows what Hadley Wickham recommended in his book on mastering shiny. Install libraries install.packages(&quot;shiny&quot;) Create app directory and file Add an app.R file. The key objective here is defining your UI (how the app looks; front-end = INPUT) (defined in object ui) and server (how the app works; back-end = OUTPUT) (defined in object server). If you’re creating a complex app, you can achieve the same goal with two files: ui.R and server.R. 5.8.2.2 app.r Front-end # Load packages # Do not use install.packages(), pacman::p_load(), or library() if you intend to deploy the app using shinyapps.io require(&quot;wordcloud2&quot;) require(&quot;shiny&quot;) require(&quot;shinydashboard&quot;) require(&quot;colourpicker&quot;) # Load data df &lt;- read.csv(url(&quot;https://github.com/jaeyk/covid19antiasian/raw/master/processed_data/hash_counts.csv&quot;))[,-1] # Defines the user interface; how the app looks ui &lt;- fluidPage( # Application title titlePanel(&quot;Word Cloud on the Hashtags of the Tweets related to COVID-19 &amp; Asian|Chinese|Wuhan&quot;), h4(tags$a(href = &quot;https://jaeyk.github.io/&quot;, &quot;Developer: Jae Yeon Kim&quot;)), sidebarLayout( # Sidebar with sliders sidebarPanel( sliderInput(&quot;size&quot;, &quot;Font size:&quot;, min = 1, max = 10, value = 2) ), mainPanel( wordcloud2Output(&quot;cloud&quot;), ) ) ) Back-end server &lt;- function(input, output, session) { output$cloud &lt;- renderWordcloud2({ wordcloud2(df, size = input$size, color = &quot;random-dark&quot;) }) } Build a shiny app shinyApp(ui = ui, server = server) 5.8.2.3 Deployment Deploy to the shinyapps.io cloud # Install packages install.packages(&quot;rsconnect&quot;) library(rsconnect) # Setup rsconnect::setAccountInfo(name = &quot;&lt;Account name&gt;&quot;, token = &quot;&lt;Token&gt;&quot;, secret = &quot;&lt;Secret&gt;&quot;) rsconnect::deployApp(appNames = &quot;&lt;App name&gt;&quot;) 5.8.2.4 References Mastering Shiny by Hadley Wickham. For newbies. Shiny Documents by Yihui Xie Engineering Production-Grade Shiny Apps by Colin Fay, Sébastien Rochette, Vincent Guyader, Cervan Girard. For experienced developers. Building Shiny Apps by Dean Attali. 5.8.3 Other useful data products Automating data reports using rmarkdown (called parameterized reports) Automating R presentation using slidify Creating interactive web apps using leaflet "],["semi-structured-data.html", "Chapter 6 Semi-structured data 6.1 Setup 6.2 Objectives 6.3 What is semi-structured data? 6.4 Workflow 6.5 HTML/CSS: web scraping 6.6 XML/JSON: government database/social media scraping", " Chapter 6 Semi-structured data 6.1 Setup # Install packages if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(tidyverse, # tidyverse pkgs including purrr furrr, # parallel processing tictoc, # performance test tcltk, # GUI for choosing a dir path tidyjson, # tidying JSON files XML, # parsing XML rvest, # parsing HTML jsonlite, # downloading JSON file from web glue, # pasting string and objects xopen, # opepn URLs in browser urltools, # regex and url parsing here) # computational reproducibility ## Install the current development version from GitHub devtools::install_github(&quot;jaeyk/tidytweetjson&quot;, dependencies = TRUE) ; library(tidytweetjson) ## Skipping install of &#39;tidytweetjson&#39; from a github remote, the SHA1 (b598dcc1) has not changed since last install. ## Use `force = TRUE` to force installation 6.2 Objectives Automating the process of turning semi-structured data (input) into structured data (output) 6.3 What is semi-structured data? Semi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure. - Wikipedia Examples: HTML (Hypertext Markup Language) files (e.g., websites) and JSON (JavaScript Object Notation) files (e.g., tweets) Why should we care semi-structured data? Because this is what the data frontier looks like: # of unstructured data &gt; # of semi-structured data &gt; # of structured data There are easy and fast ways to turn semi-structured data into structured data (ideally in a tidy format) using R, Python, and command-line tools. See my own examples (tidyethnicnews and tidytweetjson). 6.4 Workflow Import/connect to a semi-structured file using rvest, jsonlite, xml2, pdftools, tidyjson, etc. Define target elements in the single file and extract them readr package providers parse_ functions that are useful for vector parsing. stringr package for string manipulations (e.g., using regular expressions in a tidy way). Quite useful for parsing PDF files (see this example). rvest package for parsing HTML (R equivalent to beautiful soup in Python) tidyjson package for parsing JSON data Create a list of files (in this case URLs) to parse Write a parsing function Automate parsing process 6.5 HTML/CSS: web scraping Let’s go back to the example we covered in the earlier chapter of the book. url_list &lt;- c( &quot;https://en.wikipedia.org/wiki/University_of_California,_Berkeley&quot;, &quot;https://en.wikipedia.org/wiki/Stanford_University&quot;, &quot;https://en.wikipedia.org/wiki/Carnegie_Mellon_University&quot;, &quot;https://DLAB&quot; ) Step 1: Inspection Examine the Berkeley website so that we could identify a node which indicates the school’s motto. If you’re using Chrome, draw the elements of your interest, then right click &gt; inspect &gt; copy full xpath. url &lt;- &quot;https://en.wikipedia.org/wiki/University_of_California,_Berkeley&quot; download.file(url, destfile = &quot;scraped_page.html&quot;, quiet = TRUE) target &lt;- read_html(&quot;scraped_page.html&quot;) # If you want character vector output target %&gt;% html_nodes(xpath = &quot;/html/body/div[3]/div[3]/div[5]/div[1]/table[1]&quot;) %&gt;% html_text() # If you want table output target %&gt;% html_nodes(xpath = &quot;/html/body/div[3]/div[3]/div[5]/div[1]/table[1]&quot;) %&gt;% html_table() Step 2: Write a function I highly recommend writing your fucntion workking slowly by wrapping the function with slowly(). get_table_from_wiki &lt;- function(url){ download.file(url, destfile = &quot;scraped_page.html&quot;, quiet = TRUE) target &lt;- read_html(&quot;scraped_page.html&quot;) table &lt;- target %&gt;% html_nodes(xpath = &quot;/html/body/div[3]/div[3]/div[5]/div[1]/table[1]&quot;) %&gt;% html_table() return(table) } Step 3: Test get_table_from_wiki(url_list[[2]]) Step 4: Automation map(url_list, get_table_from_wiki) Step 5: Error handling map(url_lists, safely(get_table_from_wiki)) %&gt;% map(&quot;result&quot;) %&gt;% # = map(function(x) x[[&quot;result&quot;]]) = map(~.x[[&quot;name&quot;]]) purrr::compact() # Remove empty elements # If error occurred, &quot;The URL is broken.&quot; will be stored in that element(s). out &lt;- map( url_list, possibly(get_table_from_wiki, otherwise = &quot;The URL is broken.&quot; ) ) 6.6 XML/JSON: government database/social media scraping 6.6.1 Governemnt database (XML) The following tax return data example comes from the US Internal Revenue Service (IRS) Amazon database. This PDf file shows what the original document looks like. Workflow Get the XML link and parse it Go to the root of the XML document Identify a specific node you care about Get values related to that node XML DOM (Document Object Model). Source: https://www.w3schools.com Step1: Get an XML document link xml_link &lt;- c(&quot;http://s3.amazonaws.com/irs-form-990/201910919349301206_public.xml&quot;) Step 2: Get page and parse the XML document xml_root &lt;- xml_link %&gt;% # Get page and parse xml xmlTreeParse() %&gt;% # Get root xmlRoot() # Data output: list typeof(xml_root) ## [1] &quot;list&quot; # Two elements. Our target is the second one. summary(xml_root) ## Length Class Mode ## ReturnHeader 11 XMLNode list ## ReturnData 6 XMLNode list Step 3: Get nodes We grab the mission statement of this org from its tax report (990). // is an XPath syntax that helps to “select nodes in the document from the current node that match the selection no matter where they are.” xml_root %&gt;% purrr::pluck(2) %&gt;% # Second element (Return Data) getNodeSet(&quot;//MissionDesc&quot;) # Mission statement ## [[1]] ## &lt;MissionDesc&gt;DISTRIBUTION OF LITERATURE, MUSIC, AND OTHER RELATED RESOURCES WHICH COMPLIMENT LITERATURE; SUPPORT OF MINISTRIES.&lt;/MissionDesc&gt; Step 4: Get values xml_root %&gt;% purrr::pluck(2) %&gt;% # Second element (Return Data) getNodeSet(&quot;//MissionDesc&quot;) %&gt;% # Mission statement xmlValue() ## [1] &quot;DISTRIBUTION OF LITERATURE, MUSIC, AND OTHER RELATED RESOURCES WHICH COMPLIMENT LITERATURE; SUPPORT OF MINISTRIES.&quot; 6.6.2 Social media API (JSON) 6.6.2.1 Objectives Learning what kind of social media data are accessible through application programming interfaces (APIs) Review question In the previous session, we learned the difference between semi-structured data and structured data. Can anyone tell us the difference between them? 6.6.2.2 The big picture for digital data collection Input: semi-structured data Output: structured data Process: Getting target data from a remote server The target data is usually huge (&gt;10GB) by the traditional social science standard. Parsing the target data your laptop/database Laptop (sample-parse): Downsamle the large target data and parse it on your laptop. This is just one option to deal with big data in R. It’s a simple strategy as it doesn’t require storing target data in your own database. Database (push-parse): Push the large target data to a database, then explore, select, and filter it. If you were interested in using this option, then check out my SQL for R Users workshop. Sample-Parse. From RStudio. Push-Parse. From RStudio. But what exactly is this target data? When you scrape websites, you mostly deal with HTML (defines a structure of a website), CSS (its style), and JavaScript (its dynamic interactions). When you access social media data through API, you deal with either XML or JSON (major formats for storing and transporting data; they are light and flexible). XML and JSON have tree-like (nested; a root and branches) structures and keys and values (or elements and attributes). If HTML, CSS, and JavaScript are storefronts, then XML and JSON are warehouses. By Andreas Praefcke (Own work), via Wikimedia Commons 6.6.2.3 Opportunities and challenges for parsing social media data This explanation draws on Pablo Barbara’s LSE social media workshop slides. Basic information What is an API?: An interface (you can think of it as something akin to a restaurant menu. API parameters are menu items.) REST (Representational state transfer) API: static information (e.g., user profiles, list of followers and friends) R packages: tweetscores, twitteR, rtweet Streaming API: dynamic information (e..g, new tweets) This streaming data is filtered by (1) keywords, (2) location, and (3) sample (1% of the total tweets) R packages: streamR Status Twitter API is still widely accessible (v2 recently released; new fields available such as conversation threads). Twitter data is unique from data shared by most other social platforms because it reflects information that users choose to share publicly. Our API platform provides broad access to public Twitter data that users have chosen to share with the world. - Twitter Help Center What does this policy mean? If Twitter users don’t share the locations of their tweets (e.g., GPS), you can’t collect them. Facebook API access has become much constrained with the exception of Social Science One since the 2016 U.S. election. YouTube API access is somewhat limited (but you need to check as I’m not updated on this). Upside Legal and well-documented. Web scraping (Wild Wild West) &lt;&gt; API (Big Gated Garden) You have legal but limited access to (growing) big data that can be divided into text, image, and video and transformed into cross-sectional (geocodes), longitudinal (timestamps), and event historical data (hashtags). For more information, see Zachary C. Steinert-Threlkeld’s 2020 APSA Short Course Generating Event Data From Social Media. Social media data are also well-organized, managed, and curated data. It’s easy to navigate because XML and JSON have keys and values. If you find keys, you will find observations you look for. Downside Rate-limited. If you want to access to more and various data than those available, you need to pay for premium access. 6.6.2.4 Next steps If you want to know how to sign up a new Twitter developer account and access Twitter API, then see Steinert-Threlkeld’s APSA workshop slides. If you want to know about how to use tweetscore package, then see Pablo Barbara’s R markdown file for scraping data from Twitter’s REST API 6.6.3 Hydrating 6.6.3.1 Objectives Learning how hydrating works Learning how to use Twarc to communicate with Twitter’s API Review question What are the main two types of Twitter’s API? 6.6.3.2 Hydrating: An Alternative Way to Collect Historical Twitter Data You can collect Twitter data using Twitter’s API or you can hydrate Tweet IDs collected by other researchers. This is a good resource to collect historical Twitter data. Covid-19 Twitter chatter dataset for scientic use by Panacealab Women’s March Dataset by Littman and Park Harvard Dataverse has a number of dehydrated Tweet IDs that could be of interest to social scientists. Dehydrated Tweet IDs 6.6.3.3 Twarc: one solution to (almost) all Twitter’s API problems Why Twarc? A command-line tool and Python library that works for almost every Twitter’s API related problem. It’s really well-documented, tested, and maintained. Twarc documentation covers basic commands. Tward-cloud documentation explains how to collect data from Twitter’s API using Twarc running in Amazon Web Services (AWS). Twarc was developed as part of the Documenting the Now project which was funded by the Mellon Foundation. One ring that rules them all. There’s no reason to be afraid of using a command-line tool and Python library, even though you primarily use R. It’s easy to embed Python code and shell scripts in R Markdown. Even though you don’t know how to write Python code or shell scripts, it’s really useful to know how to integrate them in your R workflow. I assume that you have already installed Python 3. pip3 install twarc 6.6.3.3.1 Applications The following examples are created by the University of Virginia library. 6.6.3.3.1.1 Search Download pre-existing tweets (7-day window) matching certain conditions In command-line, &gt; = Create a file I recommend running the following commands in the terminal because it’s more stable than doing so in R Markdown. You can type commands in the Terminal in R Studio. # Key word twarc search blacklivesmatter &gt; blm_tweets.jsonl # Hashtag twarc search &#39;#blacklivesmatter&#39; &gt; blm_tweets_hash.jsonl # Hashtag + Language twarc search &#39;#blacklivesmatter&#39; --lang en &gt; blm_tweets_hash.jsonl It is really important to save these tweets into a jsonl format; jsonl extension refers to JSON Lines files. This structure is useful for splitting JSON data into smaller chunks, if it is too large. 6.6.3.3.1.2 Filter Download tweets meeting certain conditions as they happen. # Key word twarc filter blacklivesmatter &gt; blm_tweets.jsonl 6.6.3.3.1.3 Sample Use Twitter’s random sample of recent tweets. twarc sample &gt; tweets.jsonl 6.6.3.3.1.4 Hydrate Tweet IDs -&gt; Tweets twarc hydrate tweet_ids.txt &gt; tweets.jsonl 6.6.3.3.1.5 Dehydrate Hydrate &lt;&gt; Dehydrate Tweets -&gt; Tweet IDs twarc dehydrate tweets.jsonl &gt; tweet_ids.txt Challenge Collect tweets contain some key words of your choice using twarc search and save them as tweets.jsonl. Using less command in the terminal, inspect twarc.log. Using less command in the terminal, inspect tweets.json. 6.6.4 Parsing JSON 6.6.4.1 Objectives Learning chunk and pull strategy Learning how tidyjson works Learning how to apply tidyjson to tweets 6.6.4.2 Chunk and Pull 6.6.4.2.1 Problem What if the size of the Twitter data you downloaded is too big (e.g., &gt;10GB) to do complex wrangling in R? 6.6.4.2.2 Solution Chunk and Pull. From Studio. Step1: Split the large JSON file in small chunks. #Divide the JSON file by 100 lines (tweets) # Linux and Windows (in Bash) $ split -100 search.jsonl # macOS $ gsplit -100 search.jsonl After that, you will see several files appeared in the directory. Each of these files should have 100 tweets or fewer. All of these file names should start with “x,” as in “xaa.” Step 2: Apply the parsing function to each chunk and pull all of these chunks together. # You need to choose a Tweet JSON file filepath &lt;- file.choose() # Assign the parsed result to the `df` object # 11.28 sec elapsed to parse 17,928 tweets tic() df &lt;- jsonl_to_df(filepath) toc() # Setup n_cores &lt;- availableCores() - 1 n_cores # This number depends on your computer spec. plan(multiprocess, # multicore, if supported, otherwise multisession workers = n_cores) # the maximum number of workers # You need to designate a directory path where you saved the list of JSON files. # 9.385 sec elapsed to parse 17,928 tweets dirpath &lt;- tcltk::tk_choose.dir() tic() df_all &lt;- tidytweetjson::jsonl_to_df_all(dirpath) toc() 6.6.4.2.3 tidyjson The tidyjson package helps to use tidyverse framework to JSON data. toy example # JSON collection; nested structure + keys and values worldbank[1] ## [1] &quot;{\\&quot;_id\\&quot;:{\\&quot;$oid\\&quot;:\\&quot;52b213b38594d8a2be17c780\\&quot;},\\&quot;boardapprovaldate\\&quot;:\\&quot;2013-11-12T00:00:00Z\\&quot;,\\&quot;closingdate\\&quot;:\\&quot;2018-07-07T00:00:00Z\\&quot;,\\&quot;countryshortname\\&quot;:\\&quot;Ethiopia\\&quot;,\\&quot;majorsector_percent\\&quot;:[{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:46},{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:26},{\\&quot;Name\\&quot;:\\&quot;Public Administration, Law, and Justice\\&quot;,\\&quot;Percent\\&quot;:16},{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:12}],\\&quot;project_name\\&quot;:\\&quot;Ethiopia General Education Quality Improvement Project II\\&quot;,\\&quot;regionname\\&quot;:\\&quot;Africa\\&quot;,\\&quot;totalamt\\&quot;:130000000}&quot; # Check out keys (objects) worldbank %&gt;% as.tbl_json() %&gt;% gather_object() %&gt;% filter(document.id == 1) ## # A tbl_json: 8 x 3 tibble with a &quot;JSON&quot; attribute ## ..JSON document.id name ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 &quot;{\\&quot;$oid\\&quot;:\\&quot;52b213...&quot; 1 _id ## 2 &quot;\\&quot;2013-11-12T00:...&quot; 1 boardapprovaldate ## 3 &quot;\\&quot;2018-07-07T00:...&quot; 1 closingdate ## 4 &quot;\\&quot;Ethiopia\\&quot;&quot; 1 countryshortname ## 5 &quot;[{\\&quot;Name\\&quot;:\\&quot;Educa...&quot; 1 majorsector_percent ## 6 &quot;\\&quot;Ethiopia Gener...&quot; 1 project_name ## 7 &quot;\\&quot;Africa\\&quot;&quot; 1 regionname ## 8 &quot;130000000&quot; 1 totalamt # Get the values associated with the keys worldbank %&gt;% as.tbl_json() %&gt;% # Turn JSON into tbl_json object enter_object(&quot;project_name&quot;) %&gt;% # Enter the objects append_values_string() %&gt;% # Append the values as_tibble() # To reduce the size of the file ## # A tibble: 500 x 2 ## document.id string ## &lt;int&gt; &lt;chr&gt; ## 1 1 Ethiopia General Education Quality Improvement Project II ## 2 2 TN: DTF Social Protection Reforms Support ## 3 3 Tuvalu Aviation Investment Project - Additional Financing ## 4 4 Gov&#39;t and Civil Society Organization Partnership ## 5 5 Second Private Sector Competitiveness and Economic Diversificati… ## 6 6 Additional Financing for Cash Transfers for Orphans and Vulnerab… ## 7 7 National Highways Interconnectivity Improvement Project ## 8 8 China Renewable Energy Scale-Up Program Phase II ## 9 9 Rajasthan Road Sector Modernization Project ## 10 10 MA Accountability and Transparency DPL ## # … with 490 more rows The following example draws on my tidytweetjson R package. The package applies tidyjson to Tweets. 6.6.4.2.3.1 Individual file jsonl_to_df &lt;- function(file_path){ # Save file name file_name &lt;- strsplit(x = file_path, split = &quot;[/]&quot;) file_name &lt;- file_name[[1]][length(file_name[[1]])] # Import a Tweet JSON file listed &lt;- read_json(file_path, format = c(&quot;jsonl&quot;)) # IDs of the tweets with country codes ccodes &lt;- listed %&gt;% enter_object(&quot;place&quot;) %&gt;% enter_object(&quot;country_code&quot;) %&gt;% append_values_string() %&gt;% as_tibble() %&gt;% rename(&quot;country_code&quot; = &quot;string&quot;) # IDs of the tweets with location locations &lt;- listed %&gt;% enter_object(&quot;user&quot;) %&gt;% enter_object(&quot;location&quot;) %&gt;% append_values_string() %&gt;% as_tibble() %&gt;% rename(location = &quot;string&quot;) # Extract other key elements from the JSON file df &lt;- listed %&gt;% spread_values( id = jnumber(&quot;id&quot;), created_at = jstring(&quot;created_at&quot;), full_text = jstring(&quot;full_text&quot;), retweet_count = jnumber(&quot;retweet_count&quot;), favorite_count = jnumber(&quot;favorite_count&quot;), user.followers_count = jnumber(&quot;user.followers_count&quot;), user.friends_count = jnumber(&quot;user.friends_count&quot;) ) %&gt;% as_tibble message(paste(&quot;Parsing&quot;, file_name, &quot;done.&quot;)) # Full join outcome &lt;- full_join(ccodes, df) %&gt;% full_join(locations) # Or you can write this way: outcome &lt;- reduce(list(df, ccodes, locations), full_join) # Select outcome %&gt;% select(-c(&quot;document.id&quot;))} 6.6.4.2.3.2 Many files Set up parallel processing. n_cores &lt;- availableCores() - 1 n_cores # This number depends on your computer spec. ## system ## 7 plan(multiprocess, # multicore, if supported, otherwise multisession workers = n_cores) # the maximum number of workers ## Warning in supportsMulticoreAndRStudio(...): [ONE-TIME WARNING] Forked ## processing (&#39;multicore&#39;) is not supported when running R from RStudio ## because it is considered unstable. For more details, how to control forked ## processing or not, and how to silence this warning in future R sessions, see ? ## parallelly::supportsMulticore Parsing in parallel. Review There are at least three ways you can use function + purrr::map(). squared &lt;- function(x){ x*2 } # Named function map(1:3, squared) # Anonymous function map(1:3, function(x){ x *2 }) # Using formula; ~ = formula, .x = input map(1:3,~.x*2) # Create a list of file paths filename &lt;- list.files(dir_path, pattern = &#39;^x&#39;, full.names = TRUE) df &lt;- filename %&gt;% # Apply jsonl_to_df function to items on the list future_map(~jsonl_to_df(.)) %&gt;% # Full join the list of dataframes reduce(full_join, by = c(&quot;id&quot;, &quot;location&quot;, &quot;country_code&quot;, &quot;created_at&quot;, &quot;full_text&quot;, &quot;retweet_count&quot;, &quot;favorite_count&quot;, &quot;user.followers_count&quot;, &quot;user.friends_count&quot;)) # Output df "],["machine-learning.html", "Chapter 7 High-dimensional data 7.1 Overview 7.2 Dataset 7.3 Workflow 7.4 tidymodels 7.5 Pre-processing 7.6 Supervised learning 7.7 Unsupervised learning 7.8 Bias and fairness in machine learning 7.9 References", " Chapter 7 High-dimensional data 7.1 Overview The rise of high-dimensional data. The new data frontiers in social sciences—text (Gentzkow et al. 2019; Grimmer and Stewart 2013) and and image (Joo and Steinert-Threlkeld 2018)—are all high-dimensional data. 1000 common English words for 30-word tweets: \\(1000^{30}\\) similar to N of atoms in the universe (Gentzkow et al. 2019) Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. “High-dimensional methods and inference on structural and treatment effects.” Journal of Economic Perspectives 28, no. 2 (2014): 29-50. The rise of new approach: statistics + computer science = machine learning Statistical inference \\(y\\) &lt;- some probability models (e.g., linear regression, logistic regression) &lt;- \\(x\\) \\(y\\) = \\(X\\beta\\) + \\(\\epsilon\\) The goal is to estimate \\(\\beta\\) Machine learning \\(y\\) &lt;- unknown &lt;- \\(x\\) \\(y\\) &lt;-&gt; decision trees, neutral nets &lt;-&gt; \\(x\\) For the main idea behind prediction modeling, see Breiman, Leo (Berkeley stat faculty who passed away in 2005). “Statistical modeling: The two cultures (with comments and a rejoinder by the author).” Statistical science 16, no. 3 (2001): 199-231. “The problem is to find an algorithm \\(f(x)\\) such that for future \\(x\\) in a test set, \\(f(x)\\) will be a good predictor of \\(y\\).” “There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.” How ML differs from econometrics? A review by Athey, Susan, and Guido W. Imbens. “Machine learning methods that economists should know about.” Annual Review of Economics 11 (2019): 685-725. Stat: Specifying a target (i.e., an estimand) Fitting a model to data using an objective function (e.g., the sum of squared errors) Reporting point estimates (effect size) and standard errors (uncertainty) Validation by yes-no using goodness-of-fit tests and residual examination ML: Developing algorithms (estimating f(x)) Prediction power not structural/causal parameters Basically, high-dimensional data statistics (N &lt; P) The major problem is to avoid “the curse of dimensionality” (too many features - &gt; overfitting) Validation: out-of-sample comparisons (cross-validation) not in-sample goodness-of-fit measures So, it’s curve-fitting but the primary focus is unseen (test data) not seen data (training data) A quick review on ML lingos for those trained in econometrics Sample to estimate parameters = Training sample Estimating the model = Being trained Regressors, covariates, or predictors = Features Regression parameters = weights Prediction problems = Supervised (some \\(y\\) are known) + Unsupervised (\\(y\\) unknown) How to teach machines. Based on vas3k blog. Many images in this chapter come from vas3k blog. The main types of machine learning. Based on vas3k blog The map of the machine learning universe. Based on vas3k blog Classical machine learning. Based on vas3k blog 7.2 Dataset Heart disease data from UCI One of the popular datasets used in machine learning competitions # Load packages ## CRAN packages pacman::p_load(here, tidyverse, tidymodels, doParallel, # parallel processing patchwork, # arranging ggplots remotes, SuperLearner, vip, tidymodels, glmnet, xgboost, rpart, ranger, conflicted) remotes::install_github(&quot;ck37/ck37r&quot;) ## Skipping install of &#39;ck37r&#39; from a github remote, the SHA1 (24d1757a) has not changed since last install. ## Use `force = TRUE` to force installation conflicted::conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;) ## [conflicted] Will prefer dplyr::filter over any other package ## Jae&#39;s custom functions source(here(&quot;functions&quot;, &quot;ml_utils.r&quot;)) # Import the dataset data_original &lt;- read_csv(here(&quot;data&quot;, &quot;heart.csv&quot;)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## age = col_double(), ## sex = col_double(), ## cp = col_double(), ## trestbps = col_double(), ## chol = col_double(), ## fbs = col_double(), ## restecg = col_double(), ## thalach = col_double(), ## exang = col_double(), ## oldpeak = col_double(), ## slope = col_double(), ## ca = col_double(), ## thal = col_double(), ## target = col_double() ## ) glimpse(data_original) ## Rows: 303 ## Columns: 14 ## $ age &lt;dbl&gt; 63, 37, 41, 56, 57, 57, 56, 44, 52, 57, 54, 48, 49, 64, 58, … ## $ sex &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, … ## $ cp &lt;dbl&gt; 3, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 3, 3, 2, 2, 3, 0, 3, … ## $ trestbps &lt;dbl&gt; 145, 130, 130, 120, 120, 140, 140, 120, 172, 150, 140, 130, … ## $ chol &lt;dbl&gt; 233, 250, 204, 236, 354, 192, 294, 263, 199, 168, 239, 275, … ## $ fbs &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ restecg &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, … ## $ thalach &lt;dbl&gt; 150, 187, 172, 178, 163, 148, 153, 173, 162, 174, 160, 139, … ## $ exang &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ oldpeak &lt;dbl&gt; 2.3, 3.5, 1.4, 0.8, 0.6, 0.4, 1.3, 0.0, 0.5, 1.6, 1.2, 0.2, … ## $ slope &lt;dbl&gt; 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, … ## $ ca &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, … ## $ thal &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ target &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … # Createa a copy data &lt;- data_original theme_set(theme_minimal()) For more information on the Iowa housing data, read Cook (2011). This is one of the famous datastets used in many prediction modeling competitions. 7.3 Workflow Preprocessing Model building Model fitting Model evaluation Model tuning Prediction 7.4 tidymodels Like tidyverse, tidymodels is a collection of packages. rsample: for data splitting recipes: for pre-processing parsnip: for model building tune: hyperparameter tuning yardstick: for model evaluations workflows: for bundling a pieplne that bundles together pre-processing, modeling, and post-processing requests Why taking a tidyverse approach to machine learning? Benefits Readable code Reusable data structures Extendable code Tidymodels. From RStudio. tidymodels are an integrated, modular, extensible set of packages that implement a framework that facilitates creating predicative stochastic models. - Joseph Rickert@RStudio Currently, 238 models are available The following materials are based on the machine learning with tidymodels workshop I developed for D-Lab. The original workshop was designed by Chris Kennedy and [Evan Muzzall](https://dlab.berkeley.edu/people/evan-muzzall. 7.5 Pre-processing recipes: for pre-processing textrecipes for text pre-processing Step 1: recipe() defines target and predictor variables (ingredients). Step 2: step_*() defines preprocessing steps to be taken (recipe). The list of the preprocessing steps draws on the vignette of the parsnip package. dummy: Also called one-hot encoding zero variance: Removing columns (or features) with a single unique value impute: Imputing missing values decorrelate: Mitigating correlated predictors (e.g., principal component analysis) normalize: Centering and/or scaling predictors (e.g., log scaling). Scaling matters because many algorithms (e.g., lasso) are scale-variant (except tree-based algorithms). Remind you that normalization (sensitive to outliers) = \\(\\frac{X - X_{min}}{X_{max} - X_{min}}\\) and standardization (not sensitive to outliers) = \\(\\frac{X - \\mu}{\\sigma}\\) transform: Making predictors symmetric Step 3: prep() prepares a dataset to base each step on. Step 4: bake() applies the pre-processing steps to your datasets. In this course, we focus on two preprocessing tasks. One-hot encoding (creating dummy/indicator variables) # Turn selected numeric variables into factor variables data &lt;- data %&gt;% dplyr::mutate(across(c(&quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, &quot;thal&quot;), as.factor)) glimpse(data) ## Rows: 303 ## Columns: 14 ## $ age &lt;dbl&gt; 63, 37, 41, 56, 57, 57, 56, 44, 52, 57, 54, 48, 49, 64, 58, … ## $ sex &lt;fct&gt; 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, … ## $ cp &lt;fct&gt; 3, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 3, 3, 2, 2, 3, 0, 3, … ## $ trestbps &lt;dbl&gt; 145, 130, 130, 120, 120, 140, 140, 120, 172, 150, 140, 130, … ## $ chol &lt;dbl&gt; 233, 250, 204, 236, 354, 192, 294, 263, 199, 168, 239, 275, … ## $ fbs &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ restecg &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, … ## $ thalach &lt;dbl&gt; 150, 187, 172, 178, 163, 148, 153, 173, 162, 174, 160, 139, … ## $ exang &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ oldpeak &lt;dbl&gt; 2.3, 3.5, 1.4, 0.8, 0.6, 0.4, 1.3, 0.0, 0.5, 1.6, 1.2, 0.2, … ## $ slope &lt;fct&gt; 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, … ## $ ca &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, … ## $ thal &lt;fct&gt; 1, 2, 2, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ target &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … Imputation # Check missing values map_df(data, ~ is.na(.) %&gt;% sum()) ## # A tibble: 1 x 14 ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 0 0 0 0 0 0 0 ## # … with 3 more variables: ca &lt;int&gt;, thal &lt;int&gt;, target &lt;int&gt; # Add missing values data$oldpeak[sample(seq(data), size = 10)] &lt;- NA # Check missing values # Check the number of missing values data %&gt;% map_df(~is.na(.) %&gt;% sum()) ## # A tibble: 1 x 14 ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 0 0 0 0 0 10 0 ## # … with 3 more variables: ca &lt;int&gt;, thal &lt;int&gt;, target &lt;int&gt; # Check the rate of missing values data %&gt;% map_df(~is.na(.) %&gt;% mean()) ## # A tibble: 1 x 14 ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 0 0 0 0 0.0330 0 ## # … with 3 more variables: ca &lt;dbl&gt;, thal &lt;dbl&gt;, target &lt;dbl&gt; 7.5.1 Regression setup 7.5.1.1 Outcome variable # Continuous variable data$age %&gt;% class() ## [1] &quot;numeric&quot; 7.5.1.2 Data splitting using random sampling # for reproducibility set.seed(1234) # split split_reg &lt;- initial_split(data, prop = 0.7) # training set raw_train_x_reg &lt;- training(split_reg) # test set raw_test_x_reg &lt;- testing(split_reg) 7.5.1.3 recipe # Regression recipe rec_reg &lt;- raw_train_x_reg %&gt;% # Define the outcome variable recipe(age ~ .) %&gt;% # Median impute oldpeak column step_medianimpute(oldpeak) %&gt;% # Expand &quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, and &quot;thal&quot; features out into dummy variables (indicators). step_dummy(c(&quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, &quot;thal&quot;)) # Prepare a dataset to base each step on prep_reg &lt;- rec_reg %&gt;% prep(retain = TRUE) # x features train_x_reg &lt;- juice(prep_reg, all_predictors()) test_x_reg &lt;- bake(object = prep_reg, new_data = raw_test_x_reg, all_predictors()) # y variables train_y_reg &lt;- juice(prep_reg, all_outcomes())$age %&gt;% as.numeric() test_y_reg &lt;- bake(prep_reg, raw_test_x_reg, all_outcomes())$age %&gt;% as.numeric() # Checks names(train_x_reg) # Make sure there&#39;s no age variable! ## [1] &quot;trestbps&quot; &quot;chol&quot; &quot;fbs&quot; &quot;restecg&quot; &quot;thalach&quot; &quot;exang&quot; ## [7] &quot;oldpeak&quot; &quot;target&quot; &quot;sex_X1&quot; &quot;ca_X1&quot; &quot;ca_X2&quot; &quot;ca_X3&quot; ## [13] &quot;ca_X4&quot; &quot;cp_X1&quot; &quot;cp_X2&quot; &quot;cp_X3&quot; &quot;slope_X1&quot; &quot;slope_X2&quot; ## [19] &quot;thal_X1&quot; &quot;thal_X2&quot; &quot;thal_X3&quot; class(train_y_reg) # Make sure this is a continuous variable! ## [1] &quot;numeric&quot; Note that other imputation methods are also available. grep(&quot;impute&quot;, ls(&quot;package:recipes&quot;), value = TRUE) ## [1] &quot;step_bagimpute&quot; &quot;step_impute_linear&quot; &quot;step_knnimpute&quot; ## [4] &quot;step_lowerimpute&quot; &quot;step_meanimpute&quot; &quot;step_medianimpute&quot; ## [7] &quot;step_modeimpute&quot; &quot;step_rollimpute&quot; You can also create your own step_ functions. For more information, see tidymodels.org. 7.5.2 Classification setup 7.5.2.1 Outcome variable data$target %&gt;% class() ## [1] &quot;numeric&quot; data$target &lt;- as.factor(data$target) data$target %&gt;% class() ## [1] &quot;factor&quot; 7.5.2.2 Data splitting using stratified random sampling # split split_class &lt;- initial_split(data %&gt;% mutate(target = as.factor(target)), prop = 0.7, strata = target) # training set raw_train_x_class &lt;- training(split_class) # testing set raw_test_x_class &lt;- testing(split_class) 7.5.2.3 recipe # Classification recipe rec_class &lt;- raw_train_x_class %&gt;% # Define the outcome variable recipe(target ~ .) %&gt;% # Median impute oldpeak column step_medianimpute(oldpeak) %&gt;% # Expand &quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, and &quot;thal&quot; features out into dummy variables (indicators). step_normalize(age) %&gt;% step_dummy(c(&quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, &quot;thal&quot;)) # Prepare a dataset to base each step on prep_class &lt;- rec_class %&gt;% prep(retain = TRUE) # x features train_x_class &lt;- juice(prep_class, all_predictors()) test_x_class &lt;- bake(prep_class, raw_test_x_class, all_predictors()) # y variables train_y_class &lt;- juice(prep_class, all_outcomes())$target %&gt;% as.factor() test_y_class &lt;- bake(prep_class, raw_test_x_class, all_outcomes())$target %&gt;% as.factor() # Checks names(train_x_class) # Make sure there&#39;s no target variable! ## [1] &quot;age&quot; &quot;trestbps&quot; &quot;chol&quot; &quot;fbs&quot; &quot;restecg&quot; &quot;thalach&quot; ## [7] &quot;exang&quot; &quot;oldpeak&quot; &quot;sex_X1&quot; &quot;ca_X1&quot; &quot;ca_X2&quot; &quot;ca_X3&quot; ## [13] &quot;ca_X4&quot; &quot;cp_X1&quot; &quot;cp_X2&quot; &quot;cp_X3&quot; &quot;slope_X1&quot; &quot;slope_X2&quot; ## [19] &quot;thal_X1&quot; &quot;thal_X2&quot; &quot;thal_X3&quot; class(train_y_class) # Make sure this is a factor variable! ## [1] &quot;factor&quot; 7.6 Supervised learning x -&gt; f - &gt; y (defined) 7.6.1 OLS and Lasso 7.6.1.1 parsnip Build models (parsnip) Specify a model Specify an engine Specify a mode # OLS spec ols_spec &lt;- linear_reg() %&gt;% # Specify a model set_engine(&quot;lm&quot;) %&gt;% # Specify an engine: lm, glmnet, stan, keras, spark set_mode(&quot;regression&quot;) # Declare a mode: regression or classification # Lasso spec lasso_spec &lt;- linear_reg(penalty = 0.1, # tuning hyperparameter mixture = 1) %&gt;% # 1 = lasso, 0 = ridge set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;regression&quot;) # If you don&#39;t understand parsnip arguments lasso_spec %&gt;% translate() # See the documentation ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 0.1 ## mixture = 1 ## ## Computational engine: glmnet ## ## Model fit template: ## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), ## alpha = 1, family = &quot;gaussian&quot;) Fit models ols_fit &lt;- ols_spec %&gt;% fit_xy(x = train_x_reg, y = train_y_reg) # fit(train_y_reg ~ ., train_x_reg) # When you data are not preprocessed lasso_fit &lt;- lasso_spec %&gt;% fit_xy(x = train_x_reg, y = train_y_reg) 7.6.1.2 yardstick Visualize model fits map2(list(ols_fit, lasso_fit), c(&quot;OLS&quot;, &quot;Lasso&quot;), visualize_fit) ## [[1]] ## ## [[2]] # Define performance metrics metrics &lt;- yardstick::metric_set(rmse, mae, rsq) # Evaluate many models evals &lt;- purrr::map(list(ols_fit, lasso_fit), evaluate_reg) %&gt;% reduce(bind_rows) %&gt;% mutate(type = rep(c(&quot;OLS&quot;, &quot;Lasso&quot;), each = 3)) # Visualize the test results evals %&gt;% ggplot(aes(x = fct_reorder(type, .estimate), y = .estimate)) + geom_point() + labs(x = &quot;Model&quot;, y = &quot;Estimate&quot;) + facet_wrap(~glue(&quot;{toupper(.metric)}&quot;), scales = &quot;free_y&quot;) - For more information, read Tidy Modeling with R by Max Kuhn and Julia Silge. 7.6.1.3 tune Hyperparameters are parameters which control the learning process. 7.6.1.3.1 tune ingredients # tune() = placeholder tune_spec &lt;- linear_reg(penalty = tune(), # tuning hyperparameter mixture = 1) %&gt;% # 1 = lasso, 0 = ridge set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;regression&quot;) tune_spec ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet # penalty() searches 50 possible combinations lambda_grid &lt;- grid_regular(penalty(), levels = 50) # 10-fold cross-validation set.seed(1234) # for reproducibility rec_folds &lt;- vfold_cv(train_x_reg %&gt;% bind_cols(tibble(age = train_y_reg))) 7.6.1.3.2 Add these elements to a workflow # Workflow rec_wf &lt;- workflow() %&gt;% add_model(tune_spec) %&gt;% add_formula(age~.) # Tuning results rec_res &lt;- rec_wf %&gt;% tune_grid( resamples = rec_folds, grid = lambda_grid ) 7.6.1.3.3 Visualize # Visualize rec_res %&gt;% collect_metrics() %&gt;% ggplot(aes(penalty, mean, col = .metric)) + geom_errorbar(aes( ymin = mean - std_err, ymax = mean + std_err ), alpha = 0.3 ) + geom_line(size = 2) + scale_x_log10() + labs(x = &quot;log(lambda)&quot;) + facet_wrap(~glue(&quot;{toupper(.metric)}&quot;), scales = &quot;free&quot;, nrow = 2) + theme(legend.position = &quot;none&quot;) 7.6.1.3.4 Select top_rmse &lt;- show_best(rec_res, metric = &quot;rmse&quot;) best_rmse &lt;- select_best(rec_res, metric = &quot;rmse&quot;) best_rmse ## # A tibble: 1 x 2 ## penalty .config ## &lt;dbl&gt; &lt;chr&gt; ## 1 0.153 Preprocessor1_Model46 glue(&#39;The RMSE of the intiail model is {evals %&gt;% filter(type == &quot;Lasso&quot;, .metric == &quot;rmse&quot;) %&gt;% select(.estimate) %&gt;% round(2)}&#39;) ## The RMSE of the intiail model is ## 7.88 glue(&#39;The RMSE of the tuned model is {rec_res %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% arrange(mean) %&gt;% dplyr::slice(1) %&gt;% select(mean) %&gt;% round(2)}&#39;) ## The RMSE of the tuned model is 7.71 Finalize your workflow and visualize variable importance finalize_lasso &lt;- rec_wf %&gt;% finalize_workflow(best_rmse) finalize_lasso %&gt;% fit(train_x_reg %&gt;% bind_cols(tibble(age = train_y_reg))) %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.6.1.3.5 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_lasso %&gt;% fit(test_x_reg %&gt;% bind_cols(tibble(age = test_y_reg))) evaluate_reg(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 7.09 ## 2 mae standard 5.84 ## 3 rsq standard 0.414 7.6.2 Decision tree 7.6.2.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow tree_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec tree_spec &lt;- decision_tree( # Mode mode = &quot;classification&quot;, # Tuning hyperparameters cost_complexity = NULL, tree_depth = NULL) %&gt;% set_engine(&quot;rpart&quot;) # rpart, c5.0, spark tree_wf &lt;- tree_wf %&gt;% add_model(tree_spec) Fit a model tree_fit &lt;- tree_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) 7.6.2.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) From wikipedia To learn more about other metrics, check out the yardstick package references. # Define performance metrics metrics &lt;- yardstick::metric_set(accuracy, precision, recall) # Visualize tree_fit_viz_metr &lt;- visualize_class_eval(tree_fit) tree_fit_viz_metr tree_fit_viz_mat &lt;- visualize_class_conf(tree_fit) tree_fit_viz_mat 7.6.2.3 tune 7.6.2.3.1 tune ingredients Decisions trees tend to overfit. Broadly speaking, there are two things we need to consider to reduce this problem: how to split and when to stop a tree. complexity parameter: a high CP means a simple decision tree with few splits. tree_depth tune_spec &lt;- decision_tree( cost_complexity = tune(), # how to split tree_depth = tune(), # when to stop mode = &quot;classification&quot; ) %&gt;% set_engine(&quot;rpart&quot;) tree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), levels = 5) # 2 hyperparameters -&gt; 5*5 = 25 combinations tree_grid %&gt;% count(tree_depth) ## # A tibble: 5 x 2 ## tree_depth n ## * &lt;int&gt; &lt;int&gt; ## 1 1 5 ## 2 4 5 ## 3 8 5 ## 4 11 5 ## 5 15 5 # 10-fold cross-validation set.seed(1234) # for reproducibility tree_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 7.6.2.3.2 Add these elements to a workflow # Update workflow tree_wf &lt;- tree_wf %&gt;% update_model(tune_spec) # Determine the number of cores no_cores &lt;- detectCores() - 1 # Initiate cl &lt;- makeCluster(no_cores) registerDoParallel(cl) # Tuning results tree_res &lt;- tree_wf %&gt;% tune_grid( resamples = tree_folds, grid = tree_grid, metrics = metrics ) 7.6.2.3.3 Visualize The following plot draws on the vignette of the tidymodels package. tree_res %&gt;% collect_metrics() %&gt;% mutate(tree_depth = factor(tree_depth)) %&gt;% ggplot(aes(cost_complexity, mean, col = .metric)) + geom_point(size = 3) + # Subplots facet_wrap(~ tree_depth, scales = &quot;free&quot;, nrow = 2) + # Log scale x scale_x_log10(labels = scales::label_number()) + # Discrete color scale scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) + labs(x = &quot;Cost complexity&quot;, col = &quot;Tree depth&quot;, y = NULL) + coord_flip() 7.6.2.3.4 Select # Optimal hyperparameter best_tree &lt;- select_best(tree_res, &quot;recall&quot;) # Add the hyperparameter to the workflow finalize_tree &lt;- tree_wf %&gt;% finalize_workflow(best_tree) tree_fit_tuned &lt;- finalize_tree %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) # Metrics (tree_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(tree_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (tree_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(tree_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance tree_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.6.2.3.5 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_tree %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.744 ## 2 precision binary 0.705 ## 3 recall binary 0.756 In the next subsection, we will learn variants of ensemble models that improve decision tree model by putting models together. 7.6.3 Bagging (Random forest) Key idea applied across all ensemble models (bagging, boosting, and stacking): single learner -&gt; N learners (N &gt; 1) Many learners could perform better than a single learner as this approach reduces the variance of a single estimate and provides more stability. Here we focus on the difference between bagging and boosting. In short, boosting may reduce bias while increasing variance. Bagging may reduce variance but has nothing to do with bias. For more information, please check out What is the difference between Bagging and Boosting? by aporras. bagging Data: Training data will be random sampled with replacement (bootstrapping samples + drawing random subsets of features for training individual trees) Learning: Building models in parallel (independently) Prediction: Simple average of the estimated responses (majority vote system) From Sebastian Raschka’s blog boosting Data: Weighted training data will be random sampled Learning: Building models sequentially (mispredicted cases would receive more weights) Prediction: Weighted average of the estimated responses From Sebastian Raschka’s blog 7.6.3.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow rand_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec rand_spec &lt;- rand_forest( # Mode mode = &quot;classification&quot;, # Tuning hyperparameters mtry = NULL, # The number of predictors to available for splitting at each node min_n = NULL, # The minimum number of data points needed to keep splitting nodes trees = 500) %&gt;% # The number of trees set_engine(&quot;ranger&quot;, # We want the importance of predictors to be assessed. seed = 1234, importance = &quot;permutation&quot;) rand_wf &lt;- rand_wf %&gt;% add_model(rand_spec) Fit a model rand_fit &lt;- rand_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) 7.6.3.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) # Define performance metrics metrics &lt;- yardstick::metric_set(accuracy, precision, recall) rand_fit_viz_metr &lt;- visualize_class_eval(rand_fit) rand_fit_viz_metr Visualize the confusion matrix. rand_fit_viz_mat &lt;- visualize_class_conf(rand_fit) rand_fit_viz_mat 7.6.3.3 tune 7.6.3.3.1 tune ingredients We focus on the following two hyperparameters: mtry: The number of predictors to available for splitting at each node. min_n: The minimum number of data points needed to keep splitting nodes. tune_spec &lt;- rand_forest( mode = &quot;classification&quot;, # Tuning hyperparameters mtry = tune(), min_n = tune()) %&gt;% set_engine(&quot;ranger&quot;, seed = 1234, importance = &quot;permutation&quot;) rand_grid &lt;- grid_regular(mtry(range = c(1, 10)), min_n(range = c(2, 10)), levels = 5) rand_grid %&gt;% count(min_n) ## # A tibble: 5 x 2 ## min_n n ## * &lt;int&gt; &lt;int&gt; ## 1 2 5 ## 2 4 5 ## 3 6 5 ## 4 8 5 ## 5 10 5 # 10-fold cross-validation set.seed(1234) # for reproducibility rand_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 7.6.3.3.2 Add these elements to a workflow # Update workflow rand_wf &lt;- rand_wf %&gt;% update_model(tune_spec) # Tuning results rand_res &lt;- rand_wf %&gt;% tune_grid( resamples = rand_folds, grid = rand_grid, metrics = metrics ) 7.6.3.3.3 Visualize rand_res %&gt;% collect_metrics() %&gt;% mutate(min_n = factor(min_n)) %&gt;% ggplot(aes(mtry, mean, color = min_n)) + # Line + Point plot geom_line(size = 1.5, alpha = 0.6) + geom_point(size = 2) + # Subplots facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + # Log scale x scale_x_log10(labels = scales::label_number()) + # Discrete color scale scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) + labs(x = &quot;The number of predictors to be sampled&quot;, col = &quot;The minimum number of data points needed for splitting&quot;, y = NULL) + theme(legend.position=&quot;bottom&quot;) # Optimal hyperparameter best_tree &lt;- select_best(rand_res, &quot;accuracy&quot;) best_tree ## # A tibble: 1 x 3 ## mtry min_n .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 2 Preprocessor1_Model01 # Add the hyperparameter to the workflow finalize_tree &lt;- rand_wf %&gt;% finalize_workflow(best_tree) rand_fit_tuned &lt;- finalize_tree %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) # Metrics (rand_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(rand_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (rand_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(rand_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance rand_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.6.3.3.4 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_tree %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.933 ## 2 precision binary 0.973 ## 3 recall binary 0.878 7.6.4 Boosting (XGboost) 7.6.4.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow xg_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec xg_spec &lt;- boost_tree( # Mode mode = &quot;classification&quot;, # Tuning hyperparameters # The number of trees to fit, aka boosting iterations trees = c(100, 300, 500, 700, 900), # The depth of the decision tree (how many levels of splits). tree_depth = c(1, 6), # Learning rate: lower means the ensemble will adapt more slowly. learn_rate = c(0.0001, 0.01, 0.2), # Stop splitting a tree if we only have this many obs in a tree node. min_n = 10L ) %&gt;% set_engine(&quot;xgboost&quot;) xg_wf &lt;- xg_wf %&gt;% add_model(xg_spec) Fit a model xg_fit &lt;- xg_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) ## Warning in begin_iteration:end_iteration: numerical expression has 5 elements: ## only the first used ## [14:48:32] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 7.6.4.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) metrics &lt;- metric_set(yardstick::accuracy, yardstick::precision, yardstick::recall) evaluate_class(xg_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.733 ## 2 precision binary 0.730 ## 3 recall binary 0.659 xg_fit_viz_metr &lt;- visualize_class_eval(xg_fit) xg_fit_viz_metr Visualize the confusion matrix. xg_fit_viz_mat &lt;- visualize_class_conf(xg_fit) xg_fit_viz_mat 7.6.4.3 tune 7.6.4.3.1 tune ingredients We focus on the following hyperparameters: trees, tree_depth, learn_rate, min_n, mtry, loss_reduction, and sample_size tune_spec &lt;- xg_spec &lt;- boost_tree( # Mode mode = &quot;classification&quot;, # Tuning hyperparameters # The number of trees to fit, aka boosting iterations trees = tune(), # The depth of the decision tree (how many levels of splits). tree_depth = tune(), # Learning rate: lower means the ensemble will adapt more slowly. learn_rate = tune(), # Stop splitting a tree if we only have this many obs in a tree node. min_n = tune(), loss_reduction = tune(), # The number of randomly selected hyperparameters mtry = tune(), # The size of the data set used for modeling within an iteration sample_size = tune() ) %&gt;% set_engine(&quot;xgboost&quot;) # Space-filling hyperparameter grids xg_grid &lt;- grid_latin_hypercube( trees(), tree_depth(), learn_rate(), min_n(), loss_reduction(), sample_size = sample_prop(), finalize(mtry(), train_x_class), size = 30 ) # 10-fold cross-validation set.seed(1234) # for reproducibility xg_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 7.6.4.3.2 Add these elements to a workflow # Update workflow xg_wf &lt;- xg_wf %&gt;% update_model(tune_spec) # Tuning results xg_res &lt;- xg_wf %&gt;% tune_grid( resamples = xg_folds, grid = xg_grid, control = control_grid(save_pred = TRUE) ) 7.6.4.3.3 Visualize xg_res %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;roc_auc&quot;) %&gt;% pivot_longer(mtry:sample_size, values_to = &quot;value&quot;, names_to = &quot;parameter&quot;) %&gt;% ggplot(aes(x = value, y = mean, color = parameter)) + geom_point(alpha = 0.8, show.legend = FALSE) + facet_wrap(~parameter, scales = &quot;free_x&quot;) + labs(y = &quot;AUC&quot;, x = NULL) # Optimal hyperparameter best_xg &lt;- select_best(xg_res, &quot;roc_auc&quot;) best_xg ## # A tibble: 1 x 8 ## mtry trees min_n tree_depth learn_rate loss_reduction sample_size .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3 985 6 12 0.0000944 0.000000105 0.598 Preprocess… # Add the hyperparameter to the workflow finalize_xg &lt;- xg_wf %&gt;% finalize_workflow(best_xg) xg_fit_tuned &lt;- finalize_xg %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) ## [14:49:51] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. # Metrics (xg_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(xg_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (xg_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(xg_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance xg_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.6.4.3.4 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_xg %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) ## [14:49:52] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.767 ## 2 precision binary 0.857 ## 3 recall binary 0.585 7.6.5 Stacking (SuperLearner) This stacking part of the book heavily relies on Chris Kennedy’s notebook. 7.6.5.1 Overview 7.6.5.1.1 Stacking Wolpert, D.H., 1992. Stacked generalization. Neural networks, 5(2), pp.241-259. Breiman, L., 1996. [Stacked regressions]((https://statistics.berkeley.edu/sites/default/files/tech-reports/367.pdf). Machine learning, 24(1), pp.49-64. 7.6.5.1.2 SuperLearner The “SuperLearner” R package is a method that simplifies ensemble learning by allowing you to simultaneously evaluate the cross-validated performance of multiple algorithms and/or a single algorithm with differently tuned hyperparameters. This is a generally advisable approach to machine learning instead of fitting single algorithms. Let’s see how the four classification algorithms you learned in this workshop (1-lasso, 2-decision tree, 3-random forest, and 4-gradient boosted trees) compare to each other and also to 5-binary logistic regression (glm) and to the 6-mean of Y as a benchmark algorithm, in terms of their cross-validated error! A “wrapper” is a short function that adapts an algorithm for the SuperLearner package. Check out the different algorithm wrappers offered by SuperLearner: 7.6.5.2 Choose algorithms # Review available models SuperLearner::listWrappers() ## All prediction algorithm wrappers in SuperLearner: ## [1] &quot;SL.bartMachine&quot; &quot;SL.bayesglm&quot; &quot;SL.biglasso&quot; ## [4] &quot;SL.caret&quot; &quot;SL.caret.rpart&quot; &quot;SL.cforest&quot; ## [7] &quot;SL.earth&quot; &quot;SL.extraTrees&quot; &quot;SL.gam&quot; ## [10] &quot;SL.gbm&quot; &quot;SL.glm&quot; &quot;SL.glm.interaction&quot; ## [13] &quot;SL.glmnet&quot; &quot;SL.ipredbagg&quot; &quot;SL.kernelKnn&quot; ## [16] &quot;SL.knn&quot; &quot;SL.ksvm&quot; &quot;SL.lda&quot; ## [19] &quot;SL.leekasso&quot; &quot;SL.lm&quot; &quot;SL.loess&quot; ## [22] &quot;SL.logreg&quot; &quot;SL.mean&quot; &quot;SL.nnet&quot; ## [25] &quot;SL.nnls&quot; &quot;SL.polymars&quot; &quot;SL.qda&quot; ## [28] &quot;SL.randomForest&quot; &quot;SL.ranger&quot; &quot;SL.ridge&quot; ## [31] &quot;SL.rpart&quot; &quot;SL.rpartPrune&quot; &quot;SL.speedglm&quot; ## [34] &quot;SL.speedlm&quot; &quot;SL.step&quot; &quot;SL.step.forward&quot; ## [37] &quot;SL.step.interaction&quot; &quot;SL.stepAIC&quot; &quot;SL.svm&quot; ## [40] &quot;SL.template&quot; &quot;SL.xgboost&quot; ## ## All screening algorithm wrappers in SuperLearner: ## [1] &quot;All&quot; ## [1] &quot;screen.corP&quot; &quot;screen.corRank&quot; &quot;screen.glmnet&quot; ## [4] &quot;screen.randomForest&quot; &quot;screen.SIS&quot; &quot;screen.template&quot; ## [7] &quot;screen.ttest&quot; &quot;write.screen.template&quot; # Compile the algorithm wrappers to be used. sl_lib &lt;- c(&quot;SL.mean&quot;, # Marginal mean of the outcome () &quot;SL.glmnet&quot;, # GLM with lasso/elasticnet regularization &quot;SL.rpart&quot;, # Decision tree &quot;SL.ranger&quot;, # Random forest &quot;SL.xgboost&quot;) # Xgbboost 7.6.5.3 Fit model Fit the ensemble! # This is a seed that is compatible with multicore parallel processing. # See ?set.seed for more information. set.seed(1, &quot;L&#39;Ecuyer-CMRG&quot;) # This will take a few minutes to execute - take a look at the .html file to see the output! cv_sl &lt;- SuperLearner::CV.SuperLearner( Y = as.numeric(as.character(train_y_class)), X = train_x_class, family = binomial(), # For a real analysis we would use V = 10. cvControl = list(V = 5L, stratifyCV = TRUE), SL.library = sl_lib, verbose = FALSE) ## [14:49:53] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:49:54] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:49:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:49:55] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:49:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:49:56] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:49:58] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:49:58] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:49:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:49:59] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:00] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:00] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:02] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:02] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:03] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:03] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:05] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:05] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:06] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:06] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:07] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:08] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:09] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:10] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:10] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:12] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:12] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:13] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:13] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:15] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:16] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:17] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:17] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:19] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:19] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:19] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:20] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:20] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:21] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:22] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:23] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:23] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:24] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:25] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:26] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:27] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:28] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:30] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:31] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:31] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ## [14:50:33] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. 7.6.5.4 Risk Risk is the average loss, and loss is how far off the prediction was for an individual observation. The lower the risk, the fewer errors the model makes in its prediction. SuperLearner’s default loss metric is squared error \\((y_{actual} - y_{predicted})^2\\), so the risk is the mean-squared error (just like in ordinary least squares regression). View the summary, plot results, and compute the Area Under the ROC Curve (AUC)! 7.6.5.4.1 Summary Discrete SL chooses the best single learner (in this case, SL.glmnet or lasso). SuperLearner takes a weighted average of the models using the coefficients (importance of each individual learner in the overall ensemble). Coefficient 0 means that learner is not used at all. SL.mean_All (the weighted mean of \\(Y\\)) is a benchmark algorithm (ignoring features). summary(cv_sl) ## ## Call: ## SuperLearner::CV.SuperLearner(Y = as.numeric(as.character(train_y_class)), ## X = train_x_class, family = binomial(), SL.library = sl_lib, verbose = FALSE, ## cvControl = list(V = 5L, stratifyCV = TRUE)) ## ## Risk is based on: Mean Squared Error ## ## All risk estimates are based on V = 5 ## ## Algorithm Ave se Min Max ## Super Learner 0.12966 0.0149979 0.065605 0.17767 ## Discrete SL 0.12871 0.0150981 0.063205 0.17767 ## SL.mean_All 0.24802 0.0030531 0.247747 0.24893 ## SL.glmnet_All 0.12871 0.0150981 0.063205 0.17767 ## SL.rpart_All 0.18111 0.0197908 0.137814 0.22434 ## SL.ranger_All 0.14390 0.0134090 0.098379 0.17742 ## SL.xgboost_All 0.15584 0.0168627 0.121047 0.17071 7.6.5.4.2 Plot # Plot the cross-validated risk estimate with 95% CIs. plot(cv_sl) 7.6.5.5 Compute AUC for all estimators ROC ROC: an ROC (receiver operating characteristic curve) plots the relationship between True Positive Rate (Y-axis) and FALSE Positive Rate (X-axis). Area Under the ROC Curve AUC AUC: Area Under the ROC Curve 1 = perfect 0.5 = no better than chance ck37r::auc_table(cv_sl) ## auc se ci_lower ci_upper p-value ## SL.mean_All 0.5000000 0.06879264 0.3651689 0.6348311 3.510583e-09 ## SL.rpart_All 0.7911151 0.04274540 0.7073356 0.8748945 6.063783e-03 ## SL.xgboost_All 0.8477489 0.02803903 0.7927934 0.9027043 3.559655e-02 ## SL.ranger_All 0.8784640 0.02355329 0.8323004 0.9246276 1.993921e-01 ## SuperLearner 0.8962367 0.02136205 0.8543678 0.9381055 4.608180e-01 ## SL.glmnet_All 0.8983381 0.02119261 0.8568013 0.9398749 5.000000e-01 ## DiscreteSL 0.8983381 0.02119261 0.8568013 0.9398749 5.000000e-01 7.6.5.5.1 Plot the ROC curve for the best estimator (DiscretSL) plot_roc(cv_sl) 7.6.5.5.2 Review weight distribution for the SuperLearner print(cvsl_weights(cv_sl), row.names = FALSE) ## # Learner Mean SD Min Max ## 1 glmnet 0.92701 0.06884 0.86048 1.00000 ## 2 ranger 0.06278 0.06878 0.00000 0.13952 ## 3 xgboost 0.00681 0.01523 0.00000 0.03407 ## 4 rpart 0.00192 0.00429 0.00000 0.00959 ## 5 mean 0.00147 0.00330 0.00000 0.00737 General stacking approach is available in the tidymodels framework through stacks package (developmental stage). However, SuperLearner is currently not available in the tidymodels framework. If you’d like to, you can easily build and add a parsnip model. If you are interested in knowing more about it, please take a look at this vignette of the tidymodels. 7.6.6 Applications 7.6.6.1 Bandit algorithm (optimizing an experiment) 7.6.6.2 Causal forest (estimating heterogeneous treatment effect) 7.7 Unsupervised learning x -&gt; f - &gt; y (not defined) 7.7.1 Dimension reduction Projecting 2D-data to a line (PCA). From vas3k.com 7.7.1.1 Correlation analysis Notice some problems? NAs Scaling issues data_original %&gt;% corrr::correlate() ## ## Correlation method: &#39;pearson&#39; ## Missing treated using: &#39;pairwise.complete.obs&#39; ## # A tibble: 14 x 15 ## term age sex cp trestbps chol fbs restecg thalach ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age NA -0.0984 -0.0687 0.279 0.214 0.121 -0.116 -0.399 ## 2 sex -0.0984 NA -0.0494 -0.0568 -0.198 0.0450 -0.0582 -0.0440 ## 3 cp -0.0687 -0.0494 NA 0.0476 -0.0769 0.0944 0.0444 0.296 ## 4 tres… 0.279 -0.0568 0.0476 NA 0.123 0.178 -0.114 -0.0467 ## 5 chol 0.214 -0.198 -0.0769 0.123 NA 0.0133 -0.151 -0.00994 ## 6 fbs 0.121 0.0450 0.0944 0.178 0.0133 NA -0.0842 -0.00857 ## 7 rest… -0.116 -0.0582 0.0444 -0.114 -0.151 -0.0842 NA 0.0441 ## 8 thal… -0.399 -0.0440 0.296 -0.0467 -0.00994 -0.00857 0.0441 NA ## 9 exang 0.0968 0.142 -0.394 0.0676 0.0670 0.0257 -0.0707 -0.379 ## 10 oldp… 0.210 0.0961 -0.149 0.193 0.0540 0.00575 -0.0588 -0.344 ## 11 slope -0.169 -0.0307 0.120 -0.121 -0.00404 -0.0599 0.0930 0.387 ## 12 ca 0.276 0.118 -0.181 0.101 0.0705 0.138 -0.0720 -0.213 ## 13 thal 0.0680 0.210 -0.162 0.0622 0.0988 -0.0320 -0.0120 -0.0964 ## 14 targ… -0.225 -0.281 0.434 -0.145 -0.0852 -0.0280 0.137 0.422 ## # … with 6 more variables: exang &lt;dbl&gt;, oldpeak &lt;dbl&gt;, slope &lt;dbl&gt;, ca &lt;dbl&gt;, ## # thal &lt;dbl&gt;, target &lt;dbl&gt; 7.7.1.2 Preprocessing recipe is essential for preprocesssing multiple features at once. pca_recipe &lt;- recipe(~., data = data_original) %&gt;% # Imputing NAs using mean step_meanimpute(all_predictors()) %&gt;% # Normalize some numeric variables step_normalize(c(&quot;age&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;thalach&quot;, &quot;oldpeak&quot;)) 7.7.1.3 PCA analysis pca_res &lt;- pca_recipe %&gt;% step_pca(all_predictors(), id = &quot;pca&quot;) %&gt;% # id argument identifies each PCA step prep() pca_res %&gt;% tidy(id = &quot;pca&quot;) ## # A tibble: 196 x 4 ## terms value component id ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 age -0.00101 PC1 pca ## 2 sex 0.216 PC1 pca ## 3 cp 0.321 PC1 pca ## 4 trestbps 0.00118 PC1 pca ## 5 chol -0.000292 PC1 pca ## 6 fbs 0.0468 PC1 pca ## 7 restecg 0.166 PC1 pca ## 8 thalach 0.0137 PC1 pca ## 9 exang 0.0962 PC1 pca ## 10 oldpeak -0.00863 PC1 pca ## # … with 186 more rows 7.7.1.3.1 Screeplot pca_recipe %&gt;% step_pca(all_predictors(), id = &quot;pca&quot;) %&gt;% # id argument identifies each PCA step prep() %&gt;% tidy(id = &quot;pca&quot;, type = &quot;variance&quot;) %&gt;% filter(terms == &quot;percent variance&quot;) %&gt;% ggplot(aes(x = component, y = value)) + geom_col() + labs(x = &quot;PCAs of heart disease&quot;, y = &quot;% of variance&quot;, title = &quot;Scree plot&quot;) 7.7.1.3.2 View factor loadings Loadings are the covariances between the features and the principal components (=eigenvectors). pca_recipe %&gt;% step_pca(all_predictors(), id = &quot;pca&quot;) %&gt;% # id argument identifies each PCA step prep() %&gt;% tidy(id = &quot;pca&quot;) %&gt;% filter(component %in% c(&quot;PC1&quot;, &quot;PC2&quot;)) %&gt;% ggplot(aes(x = fct_reorder(terms, value), y = value, fill = component)) + geom_col(position = &quot;dodge&quot;) + coord_flip() + labs(x = &quot;Terms&quot;, y = &quot;Contribtutions&quot;, fill = &quot;PCAs&quot;) You can use these low-dimensional data to solve prediction problems. Compressing feature space via dimension reduction techniques is called feature extraction. PCA is one way of doing this. 7.7.2 Topic modeling 7.7.2.1 Setup pacman::p_load(tidytext, # tidy text analysis glue, # paste string and objects stm, # structural topic modeling gutenbergr) # toy datasets 7.7.2.2 Dataset The data munging process draws on Julia Silge’s blog post. sherlock_raw &lt;- gutenberg_download(1661) ## Determining mirror for Project Gutenberg from http://www.gutenberg.org/robot/harvest ## Using mirror http://aleph.gutenberg.org glimpse(sherlock_raw) ## Rows: 12,648 ## Columns: 2 ## $ gutenberg_id &lt;int&gt; 1661, 1661, 1661, 1661, 1661, 1661, 1661, 1661, 1661, 16… ## $ text &lt;chr&gt; &quot;THE ADVENTURES OF SHERLOCK HOLMES&quot;, &quot;&quot;, &quot;by&quot;, &quot;&quot;, &quot;SIR … sherlock &lt;- sherlock_raw %&gt;% # Mutate story using a conditional statement mutate(story = ifelse(str_starts(text, &quot;ADVENTURE&quot;), text, NA)) %&gt;% # Fill in missing values with next value tidyr::fill(story, .direction = &quot;down&quot;) %&gt;% # Filter filter(story != &quot;THE ADVENTURES OF SHERLOCK HOLMES&quot;) %&gt;% # Factor mutate(story = factor(story, levels = unique(story))) sherlock &lt;- sherlock[,2:3] 7.7.2.3 Key ideas Topics as distributions of words Documents as distributions of topics What distributions? Probability Multinominal (e.g., Latent Dirichlet Distribution) Words lie on a lower dimensional space (dimension reduction) Co-occurrence of words (clustering) Bag of words (feature engineering) Upside: easy and fast (also quite working well) Downside: ignored grammatical structures and rich interactions among words (Alternative: word embeddings. Please check out text2vec) 7.7.2.4 Exploratory data analysis sherlock_n &lt;- sherlock %&gt;% unnest_tokens(output = word, input = text) %&gt;% count(story, word, sort = TRUE) sherlock_total_n &lt;- sherlock_n %&gt;% group_by(story) %&gt;% summarise(total = sum(n)) sherlock_words &lt;- sherlock_n %&gt;% left_join(sherlock_total_n) ## Joining, by = &quot;story&quot; sherlock_words %&gt;% mutate(freq = n/total) %&gt;% group_by(story) %&gt;% top_n(10) %&gt;% ggplot(aes(x = fct_reorder(word, freq), y = freq, fill = story)) + geom_col() + coord_flip() + facet_wrap(~story, ncol = 2, scales = &quot;free_y&quot;) + scale_fill_viridis_d() + labs(x = &quot;&quot;) ## Selecting by freq 7.7.2.5 STM 7.7.2.5.1 Turn text into document-term matrix stm package has its own preprocessing function. dtm &lt;- textProcessor(documents = sherlock$text, metadata = sherlock, removestopwords = TRUE, verbose = FALSE) 7.7.2.5.2 Tuning K K is the number of topics. Let’s try K = 5, 10, 15. test_res &lt;- searchK(dtm$documents, dtm$vocab, K = c(5, 10, 15), prevalence =~ story, data = dtm$meta) ## Beginning Spectral Initialization ## Calculating the gram matrix... ## Finding anchor words... ## ..... ## Recovering initialization... ## ........................................................ ## Initialization complete. ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 1 (approx. per word bound = -7.581) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 2 (approx. per word bound = -7.482, relative change = 1.312e-02) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 3 (approx. per word bound = -7.408, relative change = 9.916e-03) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 4 (approx. per word bound = -7.383, relative change = 3.336e-03) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 5 (approx. per word bound = -7.372, relative change = 1.424e-03) ## Topic 1: holm, now, come, look, yes ## Topic 2: upon, littl, man, hand, door ## Topic 3: know, think, came, back, day ## Topic 4: said, will, can, face, matter ## Topic 5: one, see, shall, time, must ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 6 (approx. per word bound = -7.367, relative change = 6.889e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 7 (approx. per word bound = -7.365, relative change = 3.221e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 8 (approx. per word bound = -7.364, relative change = 1.281e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 9 (approx. per word bound = -7.364, relative change = 1.012e-05) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Model Converged ## Beginning Spectral Initialization ## Calculating the gram matrix... ## Finding anchor words... ## .......... ## Recovering initialization... ## ........................................................ ## Initialization complete. ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 1 (approx. per word bound = -7.666) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 2 (approx. per word bound = -7.481, relative change = 2.408e-02) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 3 (approx. per word bound = -7.387, relative change = 1.265e-02) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 4 (approx. per word bound = -7.361, relative change = 3.497e-03) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 5 (approx. per word bound = -7.351, relative change = 1.396e-03) ## Topic 1: upon, littl, paper, even, came ## Topic 2: holm, back, two, busi, sat ## Topic 3: one, case, word, remark, point ## Topic 4: come, said, room, miss, say ## Topic 5: said, man, eye, yes, took ## Topic 6: may, just, away, fact, mind ## Topic 7: see, one, time, face, look ## Topic 8: know, now, can, hand, must ## Topic 9: will, sherlock, two, might, famili ## Topic 10: tabl, heard, die, might, record ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 6 (approx. per word bound = -7.346, relative change = 7.034e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 7 (approx. per word bound = -7.342, relative change = 5.221e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 8 (approx. per word bound = -7.338, relative change = 5.161e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 9 (approx. per word bound = -7.336, relative change = 2.460e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Model Converged ## Beginning Spectral Initialization ## Calculating the gram matrix... ## Finding anchor words... ## ............... ## Recovering initialization... ## ........................................................ ## Initialization complete. ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 1 (approx. per word bound = -7.738) ## .................................................................................................... ## Completed E-Step (3 seconds). ## Completed M-Step. ## Completing Iteration 2 (approx. per word bound = -7.461, relative change = 3.577e-02) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 3 (approx. per word bound = -7.367, relative change = 1.264e-02) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 4 (approx. per word bound = -7.343, relative change = 3.252e-03) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 5 (approx. per word bound = -7.333, relative change = 1.367e-03) ## Topic 1: matter, like, made, much, street ## Topic 2: look, door, face, room, saw ## Topic 3: sir, someth, wife, mean, instant ## Topic 4: said, holm, ask, well, miss ## Topic 5: morn, littl, remark, quit, interest ## Topic 6: back, chair, close, get, step ## Topic 7: time, read, put, seen, part ## Topic 8: two, now, case, cri, yet ## Topic 9: upon, one, sherlock, famili, knew ## Topic 10: may, howev, tell, long, clear ## Topic 11: will, think, shall, good, came ## Topic 12: see, littl, hand, yes, way ## Topic 13: holm, answer, turn, return, mrs ## Topic 14: man, reason, certain, strang, crime ## Topic 15: might, twist, hand, never, come ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 6 (approx. per word bound = -7.328, relative change = 7.011e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 7 (approx. per word bound = -7.324, relative change = 4.535e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 8 (approx. per word bound = -7.322, relative change = 3.650e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 9 (approx. per word bound = -7.320, relative change = 2.220e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 10 (approx. per word bound = -7.318, relative change = 2.408e-04) ## Topic 1: matter, much, like, even, away ## Topic 2: look, room, door, face, saw ## Topic 3: sir, went, someth, wife, dark ## Topic 4: said, holm, well, ask, heard ## Topic 5: quit, morn, remark, left, give ## Topic 6: back, get, chair, step, close ## Topic 7: time, put, seen, paper, three ## Topic 8: two, case, cri, seem, yet ## Topic 9: upon, one, sherlock, knew, famili ## Topic 10: may, howev, tell, long, clear ## Topic 11: will, think, come, shall, can ## Topic 12: see, littl, hand, yes, way ## Topic 13: turn, holm, answer, return, observ ## Topic 14: man, reason, certain, strang, lord ## Topic 15: might, thing, follow, told, help ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 11 (approx. per word bound = -7.317, relative change = 1.808e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 12 (approx. per word bound = -7.316, relative change = 1.221e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 13 (approx. per word bound = -7.315, relative change = 8.460e-05) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 14 (approx. per word bound = -7.315, relative change = 4.530e-05) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 15 (approx. per word bound = -7.315, relative change = 2.133e-05) ## Topic 1: matter, much, like, even, made ## Topic 2: look, room, door, face, eye ## Topic 3: sir, went, someth, wife, dark ## Topic 4: said, holm, well, ask, know ## Topic 5: quit, remark, morn, left, found ## Topic 6: back, get, chair, step, close ## Topic 7: time, year, paper, put, seen ## Topic 8: two, case, seem, cri, yet ## Topic 9: upon, one, sherlock, knew, famili ## Topic 10: may, howev, tell, long, clear ## Topic 11: will, come, think, now, can ## Topic 12: littl, see, hand, yes, way ## Topic 13: turn, answer, return, holm, observ ## Topic 14: man, reason, certain, strang, lord ## Topic 15: might, make, thing, word, follow ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Model Converged 7.7.2.5.3 Evaludating models There are several metrics to assess the performance of topic models: the held-out likelihood, residuals, semantic coherence, and exclusivity. In this course, we examine the relationship between semantic coherence and exclusivity to understand the trade-off involved in selecting K. test_res$results %&gt;% unnest(K, exclus, semcoh) %&gt;% dplyr::select(K, exclus, semcoh) %&gt;% mutate(K = as.factor(K)) %&gt;% ggplot(aes(x = exclus, y = semcoh)) + geom_text(label = glue(&quot;K = {test_res$results$K}&quot;), size = 5, color = &quot;red&quot;) ## Warning: unnest() has a new interface. See ?unnest for details. ## Try `df %&gt;% unnest(c(K, exclus, semcoh))`, with `mutate()` if needed 7.7.2.5.4 Finalize final_stm &lt;- stm(dtm$documents, dtm$vocab, K = 10, prevalence = ~story, max.em.its = 75, data = dtm$meta, init.type = &quot;Spectral&quot;, seed = 1234567, verbose = FALSE) 7.7.2.5.5 Explore the results Using the stm package. # plot plot(final_stm) Using ggplot2 # tidy tidy_stm &lt;- tidy(final_stm) # top terms tidy_stm %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(term, beta), beta, fill = as.factor(topic))) + geom_col(alpha = 0.8, show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free_y&quot;) + coord_flip() + scale_y_continuous(labels = scales::percent) + scale_fill_viridis_d() 7.8 Bias and fairness in machine learning This section introduces the issues surrounding the fairness and bias in machine learning applications with a focus on the ProPublica’s Analysis of the COMPAS algorithm. I revised the ProPublica’s original R and Python code to increase its code readability. A gif of defendants being put into an algorithm by SELMAN DESIGN Outline Bias in the data Risk of Recidivism Data Risk of Violent Recidivism Data Bias in the algorithm References For more information on the ProPublica’s Machine Bias project, we encourage to check out the following references. Argument by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner Counterargument by Sam Corbett-Davies, Emma Pierson, Avi Feller and Sharad Goel Methodology 7.8.1 Bias in the Data (Risk of Recidivism Analysis) 7.8.1.1 Setup if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( tidyverse, # tidyverse packages conflicted, # an alternative conflict resolution strategy ggthemes, # other themes for ggplot2 patchwork, # arranging ggplots scales, # rescaling survival, # survival analysis broom, # for modeling here, # reproducibility glue # pasting strings and objects ) # To avoid conflicts conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;) ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::filter over any other package conflict_prefer(&quot;select&quot;, &quot;dplyr&quot;) ## [conflicted] Will prefer dplyr::select over any other package 7.8.1.2 Load data We select fields for severity of charge, number of priors, demographics, age, sex, COMPAS scores, and whether each person was accused of a crime within two years. two_years &lt;- read_csv(here(&quot;data&quot;, &quot;compas-scores-two-years.csv&quot;)) ## Warning: Duplicated column names deduplicated: &#39;decile_score&#39; =&gt; ## &#39;decile_score_1&#39; [40], &#39;priors_count&#39; =&gt; &#39;priors_count_1&#39; [49] glue(&quot;N of observations (rows): {nrow(two_years)} N of variables (columns): {ncol(two_years)}&quot;) ## N of observations (rows): 7214 ## N of variables (columns): 53 7.8.1.3 Wrangling Not all of the observations are useable for the first round of analysis. There are a number of reasons to remove rows because of missing data: If the charge date of a defendants COMPAS scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense. We coded the recidivist flag – is_recid – to be -1 if we could not find a COMPAS case at all. In a similar vein, ordinary traffic offenses – those with a c_charge_degree of ‘O’ – will not result in Jail time are removed (only two of them). We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility. Create a function wrangle_data &lt;- function(data){ df &lt;- data %&gt;% # Select variables select(age, c_charge_degree, race, age_cat, score_text, sex, priors_count, days_b_screening_arrest, decile_score, is_recid, two_year_recid, c_jail_in, c_jail_out) %&gt;% # Filter rows filter(days_b_screening_arrest &lt;= 30, days_b_screening_arrest &gt;= -30, is_recid != -1, c_charge_degree != &quot;O&quot;, score_text != &#39;N/A&#39;) %&gt;% # Mutate variables mutate(length_of_stay = as.numeric(as.Date(c_jail_out) - as.Date(c_jail_in)), c_charge_degree = factor(c_charge_degree), age_cat = factor(age_cat), race = factor(race, levels = c(&quot;Caucasian&quot;,&quot;African-American&quot;,&quot;Hispanic&quot;,&quot;Other&quot;,&quot;Asian&quot;,&quot;Native American&quot;)), sex = factor(sex, levels = c(&quot;Male&quot;,&quot;Female&quot;)), score_text = factor(score_text, levels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;)), score = score_text, # I added this new variable to test whether measuring the DV as a binary or continuous var makes a difference score_num = as.numeric(score_text)) %&gt;% # Rename variables rename(crime = c_charge_degree, gender = sex) return(df)} Apply the function to the data df &lt;- wrangle_data(two_years) names(df) ## [1] &quot;age&quot; &quot;crime&quot; ## [3] &quot;race&quot; &quot;age_cat&quot; ## [5] &quot;score_text&quot; &quot;gender&quot; ## [7] &quot;priors_count&quot; &quot;days_b_screening_arrest&quot; ## [9] &quot;decile_score&quot; &quot;is_recid&quot; ## [11] &quot;two_year_recid&quot; &quot;c_jail_in&quot; ## [13] &quot;c_jail_out&quot; &quot;length_of_stay&quot; ## [15] &quot;score&quot; &quot;score_num&quot; # Check whether the function works as expected head(df, 5) ## # A tibble: 5 x 16 ## age crime race age_cat score_text gender priors_count days_b_screenin… ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 69 F Other Greate… Low Male 0 -1 ## 2 34 F Afri… 25 - 45 Low Male 0 -1 ## 3 24 F Afri… Less t… Low Male 4 -1 ## 4 44 M Other 25 - 45 Low Male 0 0 ## 5 41 F Cauc… 25 - 45 Medium Male 14 -1 ## # … with 8 more variables: decile_score &lt;dbl&gt;, is_recid &lt;dbl&gt;, ## # two_year_recid &lt;dbl&gt;, c_jail_in &lt;dttm&gt;, c_jail_out &lt;dttm&gt;, ## # length_of_stay &lt;dbl&gt;, score &lt;fct&gt;, score_num &lt;dbl&gt; 7.8.1.4 Descriptive analysis Higher COMPAS scores are slightly correlated with a longer length of stay. cor(df$length_of_stay, df$decile_score) ## [1] 0.2073297 df %&gt;% group_by(score) %&gt;% count() %&gt;% ggplot(aes(x = score, y = n)) + geom_col() + labs(x = &quot;Score&quot;, y = &quot;Count&quot;, title = &quot;Score distribution&quot;) Judges are often presented with two sets of scores from the COMPAS system – one that classifies people into High, Medium and Low risk, and a corresponding decile score. There is a clear downward trend in the decile scores as those scores increase for white defendants. df %&gt;% ggplot(aes(ordered(decile_score))) + geom_bar() + facet_wrap(~race, nrow = 2) + labs(x = &quot;Decile Score&quot;, y = &quot;Count&quot;, Title = &quot;Defendant&#39;s Decile Score&quot;) 7.8.1.5 Modeling After filtering out bad rows, our first question is whether there is a significant difference in COMPAS scores between races. To do so we need to change some variables into factors, and run a logistic regression, comparing low scores to high scores. Model building model_data &lt;- function(data){ # Logistic regression model lr_model &lt;- glm(score ~ gender + age_cat + race + priors_count + crime + two_year_recid, family = &quot;binomial&quot;, data = data) # OLS, DV = score_num ols_model1 &lt;- lm(score_num ~ gender + age_cat + race + priors_count + crime + two_year_recid, data = data) # OLS, DV = decile_score ols_model2 &lt;- lm(decile_score ~ gender + age_cat + race + priors_count + crime + two_year_recid, data = data) # Extract model outcomes with confidence intervals lr_est &lt;- lr_model %&gt;% tidy(conf.int = TRUE) ols_est1 &lt;- ols_model1 %&gt;% tidy(conf.int = TRUE) ols_est2 &lt;- ols_model2 %&gt;% tidy(conf.int = TRUE) # AIC scores lr_AIC &lt;- AIC(lr_model) ols_AIC1 &lt;- AIC(ols_model1) ols_AIC2 &lt;- AIC(ols_model2) list(lr_est, ols_est1, ols_est2, lr_AIC, ols_AIC1, ols_AIC2) } Model comparisons glue(&quot;AIC score of logistic regression: {model_data(df)[4]} AIC score of OLS regression (with categorical DV): {model_data(df)[5]} AIC score of OLS regression (with continuous DV): {model_data(df)[6]}&quot;) ## AIC score of logistic regression: 6192.40169473357 ## AIC score of OLS regression (with categorical DV): 11772.1148541111 ## AIC score of OLS regression (with continuous DV): 26779.9512226999 Logistic regression model lr_model &lt;- model_data(df)[1] %&gt;% data.frame() lr_model %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% mutate(term = gsub(&quot;race|age_cat|gender|M&quot;,&quot;&quot;, term)) %&gt;% ggplot(aes(x = fct_reorder(term, estimate), y = estimate, ymax = conf.high, ymin = conf.low)) + geom_pointrange() + coord_flip() + labs(y = &quot;Estimate&quot;, x = &quot;&quot;, title = &quot;Logistic regression&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) Logistic regression coefficients are log odds ratios. Remember an odd is \\(\\frac{p}{1-p}\\). p could be defined as a success and 1-p could be as a failure. Here, coefficient 1 indicates equal probability for the binary outcomes. Coefficient greater than 1 indicates strong chance for p and weak chance for 1-p. Coefficient smaller than 1 indicates the opposite. Nonetheless, the exact interpretation is not very interpretive as an odd of 2.0 corresponds to the probability of 1/3 (!). (To refresh your memory, note that probability is bounded between [0, 1]. Odds ranges between 0 and infinity. Log odds ranges from negative to positive infinity. We’re going through this hassle because we used log function to map predictor variables to probability to fit the model to the binary outcomes.) In this case, we reinterpret coefficients by turning log odds ratios into relative risks. Relative risk = odds ratio / 1 - p0 + (p0 * odds ratio) p-0 is the baseline risk. For more information on relative risks and its value in statistical communication, see Grant (2014), Wang (2013), and Zhang and Yu (1998). odds_to_risk &lt;- function(model){ # Calculating p0 (baseline or control group) intercept &lt;- model$estimate[model$term == &quot;(Intercept)&quot;] control &lt;- exp(intercept) / (1 + exp(intercept)) # Calculating relative risk model &lt;- model %&gt;% filter(term != &quot;(Intercept)&quot;) model$relative_risk &lt;- (exp(model$estimate) / (1 - control + (control * exp(model$estimate)))) return(model) } odds_to_risk(lr_model) %&gt;% relocate(relative_risk) %&gt;% arrange(desc(relative_risk)) ## relative_risk term estimate std.error statistic ## 1 2.6152880 raceNative American 1.3942077 0.76611816 1.8198338 ## 2 2.4961195 age_catLess than 25 1.3083903 0.07592869 17.2318308 ## 3 1.6882587 two_year_recid 0.6858625 0.06401955 10.7133288 ## 4 1.4528374 raceAfrican-American 0.4772070 0.06934914 6.8812245 ## 5 1.2402135 priors_count 0.2689453 0.01110379 24.2210342 ## 6 1.1947947 genderFemale 0.2212667 0.07951020 2.7828714 ## 7 0.8077863 raceAsian -0.2544147 0.47821105 -0.5320135 ## 8 0.7692955 crimeM -0.3112408 0.06654750 -4.6769729 ## 9 0.6948050 raceHispanic -0.4283949 0.12812549 -3.3435572 ## 10 0.4865228 raceOther -0.8263469 0.16208006 -5.0983873 ## 11 0.2971899 age_catGreater than 45 -1.3556332 0.09908053 -13.6821355 ## p.value conf.low conf.high ## 1 6.878432e-02 -0.05694017 3.0383160 ## 2 1.532239e-66 1.16008750 1.4577645 ## 3 8.813460e-27 0.56039880 0.8113799 ## 4 5.934025e-12 0.34137020 0.6132514 ## 5 1.335783e-129 0.24750487 0.2910343 ## 6 5.388016e-03 0.06532360 0.3770591 ## 7 5.947167e-01 -1.25877950 0.6389894 ## 8 2.911407e-06 -0.44178937 -0.1808904 ## 9 8.271164e-04 -0.68190124 -0.1794075 ## 10 3.425594e-07 -1.15026143 -0.5142075 ## 11 1.298233e-42 -1.55226716 -1.1637224 Relative risk score 1.45 (African American) indicates that black defendants are 45% more likely than white defendants to receive a higher score. The plot visualizes this and other results from the table. odds_to_risk(lr_model) %&gt;% mutate(term = gsub(&quot;race|age_cat|gender&quot;,&quot;&quot;, term)) %&gt;% ggplot(aes(x = fct_reorder(term, relative_risk), y = relative_risk)) + geom_point(size = 3) + coord_flip() + labs(y = &quot;Likelihood&quot;, x = &quot;&quot;, title = &quot;Logistic regression&quot;) + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;) 7.8.2 Bias in the Data (Risk of Violent Recidivism Analysis) 7.8.2.1 Setup if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( tidyverse, # tidyverse packages conflicted, # an alternative conflict resolution strategy ggthemes, # other themes for ggplot2 patchwork, # arranging ggplots scales, # rescaling survival, # survival analysis broom, # for modeling here, # reproducibility glue # pasting strings and objects ) # To avoid conflicts conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;) ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::filter over any other package conflict_prefer(&quot;select&quot;, &quot;dplyr&quot;) ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::select over any other package # Set themes theme_set(ggthemes::theme_fivethirtyeight()) 7.8.2.2 Load data two_years_violent &lt;- read_csv(here(&quot;data&quot; ,&quot;compas-scores-two-years-violent.csv&quot;)) ## Warning: Duplicated column names deduplicated: &#39;decile_score&#39; =&gt; ## &#39;decile_score_1&#39; [40], &#39;priors_count&#39; =&gt; &#39;priors_count_1&#39; [49], &#39;two_year_recid&#39; ## =&gt; &#39;two_year_recid_1&#39; [54] ## ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## name = col_character(), ## first = col_character(), ## last = col_character(), ## compas_screening_date = col_date(format = &quot;&quot;), ## sex = col_character(), ## dob = col_date(format = &quot;&quot;), ## age_cat = col_character(), ## race = col_character(), ## c_jail_in = col_datetime(format = &quot;&quot;), ## c_jail_out = col_datetime(format = &quot;&quot;), ## c_case_number = col_character(), ## c_offense_date = col_date(format = &quot;&quot;), ## c_arrest_date = col_date(format = &quot;&quot;), ## c_charge_degree = col_character(), ## c_charge_desc = col_character(), ## r_case_number = col_character(), ## r_charge_degree = col_character(), ## r_offense_date = col_date(format = &quot;&quot;), ## r_charge_desc = col_character(), ## r_jail_in = col_date(format = &quot;&quot;) ## # ... with 14 more columns ## ) ## ℹ Use `spec()` for the full column specifications. glue(&quot;N of observations (rows): {nrow(two_years_violent)} N of variables (columns): {ncol(two_years_violent)}&quot;) ## N of observations (rows): 4743 ## N of variables (columns): 54 7.8.2.3 Wrangling Create a function wrangle_data &lt;- function(data){ df &lt;- data %&gt;% # Select variables select(age, c_charge_degree, race, age_cat, v_score_text, sex, priors_count, days_b_screening_arrest, v_decile_score, is_recid, two_year_recid) %&gt;% # Filter rows filter(days_b_screening_arrest &lt;= 30, days_b_screening_arrest &gt;= -30, is_recid != -1, c_charge_degree != &quot;O&quot;, v_score_text != &#39;N/A&#39;) %&gt;% # Mutate variables mutate(c_charge_degree = factor(c_charge_degree), age_cat = factor(age_cat), race = factor(race, levels = c(&quot;Caucasian&quot;,&quot;African-American&quot;,&quot;Hispanic&quot;,&quot;Other&quot;,&quot;Asian&quot;,&quot;Native American&quot;)), sex = factor(sex, levels = c(&quot;Male&quot;,&quot;Female&quot;)), v_score_text = factor(v_score_text, levels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;)), # I added this new variable to test whether measuring the DV as a binary or continuous var makes a difference score_num = as.numeric(v_score_text)) %&gt;% # Rename variables rename(crime = c_charge_degree, gender = sex, score = v_score_text) return(df)} Apply the function to the data df &lt;- wrangle_data(two_years_violent) names(df) ## [1] &quot;age&quot; &quot;crime&quot; ## [3] &quot;race&quot; &quot;age_cat&quot; ## [5] &quot;score&quot; &quot;gender&quot; ## [7] &quot;priors_count&quot; &quot;days_b_screening_arrest&quot; ## [9] &quot;v_decile_score&quot; &quot;is_recid&quot; ## [11] &quot;two_year_recid&quot; &quot;score_num&quot; head(df, 5) # Check whether the function works as expected ## # A tibble: 5 x 12 ## age crime race age_cat score gender priors_count days_b_screenin… ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 69 F Other Greate… Low Male 0 -1 ## 2 34 F Afri… 25 - 45 Low Male 0 -1 ## 3 44 M Other 25 - 45 Low Male 0 0 ## 4 43 F Other 25 - 45 Low Male 3 -1 ## 5 39 M Cauc… 25 - 45 Low Female 0 -1 ## # … with 4 more variables: v_decile_score &lt;dbl&gt;, is_recid &lt;dbl&gt;, ## # two_year_recid &lt;dbl&gt;, score_num &lt;dbl&gt; 7.8.2.4 Descriptive analysis Score distribution df %&gt;% group_by(score) %&gt;% count() %&gt;% ggplot(aes(x = score, y = n)) + geom_col() + labs(x = &quot;Score&quot;, y = &quot;Count&quot;, title = &quot;Score distribution&quot;) Score distribution by race df %&gt;% ggplot(aes(ordered(v_decile_score))) + geom_bar() + facet_wrap(~race, nrow = 2) + labs(x = &quot;Decile Score&quot;, y = &quot;Count&quot;, Title = &quot;Defendant&#39;s Decile Score&quot;) 7.8.2.5 Modeling After filtering out bad rows, our first question is whether there is a significant difference in COMPAS scores between races. To do so we need to change some variables into factors, and run a logistic regression, comparing low scores to high scores. model_data &lt;- function(data){ # Logistic regression model lr_model &lt;- glm(score ~ gender + age_cat + race + priors_count + crime + two_year_recid, family = &quot;binomial&quot;, data = data) # OLS ols_model1 &lt;- lm(score_num ~ gender + age_cat + race + priors_count + crime + two_year_recid, data = data) ols_model2 &lt;- lm(v_decile_score ~ gender + age_cat + race + priors_count + crime + two_year_recid, data = data) # Extract model outcomes with confidence intervals lr_est &lt;- lr_model %&gt;% tidy(conf.int = TRUE) ols_est1 &lt;- ols_model1 %&gt;% tidy(conf.int = TRUE) ols_est2 &lt;- ols_model2 %&gt;% tidy(conf.int = TRUE) # AIC scores lr_AIC &lt;- AIC(lr_model) ols_AIC1 &lt;- AIC(ols_model1) ols_AIC2 &lt;- AIC(ols_model2) list(lr_est, ols_est1, ols_est2, lr_AIC, ols_AIC1, ols_AIC2) } Model comparisons glue(&quot;AIC score of logistic regression: {model_data(df)[4]} AIC score of OLS regression (with categorical DV): {model_data(df)[5]} AIC score of OLS regression (with continuous DV): {model_data(df)[6]}&quot;) ## AIC score of logistic regression: 3022.77943765996 ## AIC score of OLS regression (with categorical DV): 5414.49127581608 ## AIC score of OLS regression (with continuous DV): 15458.3861723106 Logistic regression model lr_model &lt;- model_data(df)[1] %&gt;% data.frame() lr_model %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% mutate(term = gsub(&quot;race|age_cat|gender&quot;,&quot;&quot;, term)) %&gt;% ggplot(aes(x = fct_reorder(term, estimate), y = estimate, ymax = conf.high, ymin = conf.low)) + geom_pointrange() + coord_flip() + labs(y = &quot;Estimate&quot;, x = &quot;&quot;, title = &quot;Logistic regression&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) Logistic regression coefficients are log odds ratios. Remember an odd is \\(\\frac{p}{1-p}\\). p could be defined as a success and 1-p could be as a failure. Here, coefficient 1 indicates equal probability for the binary outcomes. Coefficient greater than 1 indicates strong chance for p and weak chance for 1-p. Coefficient smaller than 1 indicates the opposite. Nonetheless, the exact interpretation is not very interpretive as an odd of 2.0 corresponds to the probability of 1/3 (!). (To refresh your memory, note that probability is bounded between [0, 1]. Odds ranges between 0 and infinity. Log odds ranges from negative to positive infinity. We’re going through this hassle because we used log function to map predictor variables to probability to fit the model to the binary outcomes.) In this case, we reinterpret coefficients by turning log odds ratios into relative risks. Relative risk = odds ratio / 1 - p0 + (p0 * odds ratio) p-0 is the baseline risk. For more information on relative risks and its value in statistical communication, see Grant (2014), Wang (2013), and Zhang and Yu (1998). odds_to_risk &lt;- function(model){ # Calculating p0 (baseline or control group) intercept &lt;- model$estimate[model$term == &quot;(Intercept)&quot;] control &lt;- exp(intercept) / (1 + exp(intercept)) # Calculating relative risk model &lt;- model %&gt;% filter(term != &quot;(Intercept)&quot;) model$relative_risk &lt;- (exp(model$estimate) / (1 - control + (control * exp(model$estimate)))) return(model) } odds_to_risk(lr_model) %&gt;% relocate(relative_risk) %&gt;% arrange(desc(relative_risk)) ## relative_risk term estimate std.error statistic ## 1 7.4142320 age_catLess than 25 3.14590906 0.11540998 27.2585528 ## 2 2.2169566 two_year_recid 0.93447949 0.11527216 8.1067232 ## 3 1.7739274 raceAfrican-American 0.65893450 0.10814991 6.0927885 ## 4 1.4845555 raceNative American 0.44792984 1.03546096 0.4325898 ## 5 1.1315392 priors_count 0.13764241 0.01161172 11.8537476 ## 6 0.9434828 raceHispanic -0.06415947 0.19132794 -0.3353377 ## 7 0.8615079 crimeM -0.16366732 0.09806528 -1.6689631 ## 8 0.8290722 raceOther -0.20543235 0.22464062 -0.9144933 ## 9 0.5076551 genderFemale -0.72890371 0.12665509 -5.7550290 ## 10 0.3972545 raceAsian -0.98520588 0.70537045 -1.3967212 ## 11 0.1902151 age_catGreater than 45 -1.74207559 0.18414760 -9.4602135 ## p.value conf.low conf.high ## 1 1.315899e-163 2.9224937 3.37506621 ## 2 5.200316e-16 0.7084155 1.16039836 ## 3 1.109606e-09 0.4480948 0.87222287 ## 4 6.653128e-01 -1.9660912 2.24738803 ## 5 2.057779e-32 0.1151045 0.16064926 ## 6 7.373704e-01 -0.4439074 0.30657314 ## 7 9.512470e-02 -0.3563339 0.02822281 ## 8 3.604577e-01 -0.6533518 0.22789493 ## 9 8.662690e-09 -0.9800266 -0.48330469 ## 10 1.624974e-01 -2.4655693 0.33213464 ## 11 3.073150e-21 -2.1171742 -1.39384502 Relative risk score 1.45 (African American) indicates that black defendants are 45% more likely than white defendants to receive a higher score. The plot visualizes this and other results from the table. odds_to_risk(lr_model) %&gt;% mutate(term = gsub(&quot;race|age_cat|gender&quot;,&quot;&quot;, term)) %&gt;% ggplot(aes(x = fct_reorder(term, relative_risk), y = relative_risk)) + geom_point(size = 3) + coord_flip() + labs(y = &quot;Likelihood&quot;, x = &quot;&quot;, title = &quot;Logistic regression&quot;) + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;) 7.8.3 Bias in the algorithm In order to test whether COMPAS scores do an accurate job of deciding whether an offender is Low, Medium or High risk, we ran a Cox Proportional Hazards model. Northpointe, the company that created COMPAS and markets it to Law Enforcement, also ran a Cox model in their validation study. We used the counting model and removed people when they were incarcerated. Due to errors in the underlying jail data, we need to filter out 32 rows that have an end date more than the start date. Considering that there are 13,334 total rows in the data, such a small amount of errors will not affect the results. 7.8.3.1 Setup if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( tidyverse, # tidyverse packages conflicted, # an alternative conflict resolution strategy ggthemes, # other themes for ggplot2 patchwork, # arranging ggplots scales, # rescaling survival, # survival analysis broom, # for modeling here, # reproducibility glue, # pasting strings and objects reticulate # source python codes ) # To avoid conflicts conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;) ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::filter over any other package conflict_prefer(&quot;select&quot;, &quot;dplyr&quot;) ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::select over any other package # Set themes theme_set(ggthemes::theme_fivethirtyeight()) 7.8.3.2 Load data cox_data &lt;- read_csv(here(&quot;data&quot; ,&quot;cox-parsed.csv&quot;)) ## Warning: Duplicated column names deduplicated: &#39;decile_score&#39; =&gt; ## &#39;decile_score_1&#39; [40], &#39;priors_count&#39; =&gt; &#39;priors_count_1&#39; [49] ## ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## id = col_double(), ## compas_screening_date = col_date(format = &quot;&quot;), ## dob = col_date(format = &quot;&quot;), ## age = col_double(), ## juv_fel_count = col_double(), ## decile_score = col_double(), ## juv_misd_count = col_double(), ## juv_other_count = col_double(), ## priors_count = col_double(), ## days_b_screening_arrest = col_double(), ## c_jail_in = col_datetime(format = &quot;&quot;), ## c_jail_out = col_datetime(format = &quot;&quot;), ## c_offense_date = col_date(format = &quot;&quot;), ## c_arrest_date = col_date(format = &quot;&quot;), ## c_days_from_compas = col_double(), ## is_recid = col_double(), ## r_days_from_arrest = col_double(), ## r_offense_date = col_date(format = &quot;&quot;), ## r_jail_in = col_date(format = &quot;&quot;), ## r_jail_out = col_date(format = &quot;&quot;) ## # ... with 13 more columns ## ) ## ℹ Use `spec()` for the full column specifications. glue(&quot;N of observations (rows): {nrow(cox_data)} N of variables (columns): {ncol(cox_data)}&quot;) ## N of observations (rows): 13419 ## N of variables (columns): 52 7.8.3.3 Wrangling df &lt;- cox_data %&gt;% filter(score_text != &quot;N/A&quot;) %&gt;% filter(end &gt; start) %&gt;% mutate(c_charge_degree = factor(c_charge_degree), age_cat = factor(age_cat), race = factor(race, levels = c(&quot;Caucasian&quot;,&quot;African-American&quot;,&quot;Hispanic&quot;,&quot;Other&quot;,&quot;Asian&quot;,&quot;Native American&quot;)), sex = factor(sex, levels = c(&quot;Male&quot;,&quot;Female&quot;)), score_factor = factor(score_text, levels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;))) grp &lt;- df[!duplicated(df$id),] 7.8.3.4 Descriptive analysis Score distribution grp %&gt;% group_by(score_factor) %&gt;% count() %&gt;% ggplot(aes(x = score_factor, y = n)) + geom_col() + labs(x = &quot;Score&quot;, y = &quot;Count&quot;, title = &quot;Score distribution&quot;) Score distribution by race df %&gt;% ggplot(aes(ordered(score_factor))) + geom_bar() + facet_wrap(~race, nrow = 2) + labs(x = &quot;Decile Score&quot;, y = &quot;Count&quot;, Title = &quot;Defendant&#39;s Decile Score&quot;) 7.8.3.5 Modeling f2 &lt;- Surv(start, end, event, type=&quot;counting&quot;) ~ race + score_factor + race * score_factor model &lt;- coxph(f2, data = df) model %&gt;% broom::tidy(conf.int = TRUE) %&gt;% mutate(term = gsub(&quot;race|score_factor&quot;,&quot;&quot;, term)) %&gt;% filter(term != &quot;&lt;chr&gt;&quot;) %&gt;% ggplot(aes(x = fct_reorder(term, estimate), y = estimate, ymax = conf.high, ymin = conf.low)) + geom_pointrange() + coord_flip() + labs(y = &quot;Estimate&quot;, x = &quot;&quot;) The interaction term shows a similar disparity as the logistic regression above. High risk white defendants are 3.61 more likely than low risk white defendants, while High risk black defendants are 2.99 more likely than low. visualize_surv &lt;- function(input){ f &lt;- Surv(start, end, event, type=&quot;counting&quot;) ~ score_factor fit &lt;- survfit(f, data = input) fit %&gt;% tidy(conf.int = TRUE) %&gt;% mutate(strata = gsub(&quot;score_factor=&quot;,&quot;&quot;, strata)) %&gt;% mutate(strata = factor(strata, levels = c(&quot;High&quot;,&quot;Medium&quot;,&quot;Low&quot;))) %&gt;% ggplot(aes(x = time, y = estimate, ymax = conf.high, ymin = conf.low, group = strata, col = strata)) + geom_pointrange(alpha = 0.1) + guides(colour = guide_legend(override.aes = list(alpha = 1))) + ylim(c(0, 1)) + labs(x = &quot;Time&quot;, y = &quot;Estimated survival rate&quot;, col = &quot;Strata&quot;)} visualize_surv(df) + ggtitle(&quot;Overall&quot;) Black defendants do recidivate at higher rates according to race specific Kaplan Meier plots. (df %&gt;% filter(race == &quot;Caucasian&quot;) %&gt;% visualize_surv() + ggtitle(&quot;Caucasian&quot;)) / (df %&gt;% filter(race == &quot;African-American&quot;) %&gt;% visualize_surv() + ggtitle(&quot;African-American&quot;)) In terms of underlying recidivism rates, we can look at gender specific Kaplan Meier estimates. There is a striking difference between women and men. (df %&gt;% filter(sex == &quot;Female&quot;) %&gt;% visualize_surv() + ggtitle(&quot;Female&quot;)) / (df %&gt;% filter(sex == &quot;Male&quot;) %&gt;% visualize_surv() + ggtitle(&quot;Male&quot;)) As these plots show, the COMPAS score treats a High risk women the same as a Medium risk man. 7.8.3.6 Risk of Recidivism accuracy The above analysis shows that the COMPAS algorithm does overpredict African-American defendant’s future recidivism, but we haven’t yet explored the direction of the bias. We can discover fine differences in overprediction and underprediction by comparing COMPAS scores across racial lines. # create a new environment conda_create(&quot;r-reticulate&quot;) ## [1] &quot;/home/jae/.local/share/r-miniconda/envs/r-reticulate/bin/python&quot; # install libs conda_install(&quot;r-reticulate&quot;, c(&quot;pandas&quot;)) # indicate that we want to use a specific condaenv use_condaenv(&quot;r-reticulate&quot;) from truth_tables import PeekyReader, Person, table, is_race, count, vtable, hightable, vhightable from csv import DictReader people = [] with open(&quot;./data/cox-parsed.csv&quot;) as f: reader = PeekyReader(DictReader(f)) try: while True: p = Person(reader) if p.valid: people.append(p) except StopIteration: pass pop = list(filter(lambda i: ((i.recidivist == True and i.lifetime &lt;= 730) or i.lifetime &gt; 730), list(filter(lambda x: x.score_valid, people)))) recid = list(filter(lambda i: i.recidivist == True and i.lifetime &lt;= 730, pop)) rset = set(recid) surv = [i for i in pop if i not in rset] Define a function for a table. import pandas as pd def create_table(x, y): t = table(list(x), list(y)) df = pd.DataFrame(t.items(), columns = [&#39;Metrics&#39;, &#39;Scores&#39;]) return(df) All defenders create_table(list(recid), list(surv)).to_csv(&quot;data/table_recid.csv&quot;) read.csv(here(&quot;data&quot;, &quot;table_recid.csv&quot;))[,-1] %&gt;% ggplot(aes(x = Metrics, y = Scores)) + geom_col() + labs(title = &quot;Recidivism&quot;) That number is higher for African Americans at 44.85% and lower for whites at 23.45%. def create_comp_tables(recid_data, surv_data): # filtering variables is_afam = is_race(&quot;African-American&quot;) is_white = is_race(&quot;Caucasian&quot;) # dfs df1 = create_table(filter(is_afam, recid_data), filter(is_afam, surv_data)) df2 = create_table(filter(is_white, recid_data), filter(is_white, surv_data)) # concat dfs = pd.concat([df1, df2]) dfs[&#39;Group&#39;] = [&#39;African Americans&#39;,&#39;African Americans&#39;,&#39;Whites&#39;,&#39;Whites&#39;] return(dfs) create_comp_tables(recid, surv).to_csv(&quot;data/comp_tables_recid.csv&quot;) read.csv(here(&quot;data&quot;, &quot;comp_tables_recid.csv&quot;))[,-1] %&gt;% ggplot(aes(x = Metrics, y = Scores, fill = Group)) + geom_col(position = &quot;dodge&quot;) + coord_flip() + labs(title = &quot;Recidivism&quot;) 7.8.3.7 Risk of Violent Recidivism accuracy COMPAS also offers a score that aims to measure a persons risk of violent recidivism, which has a similar overall accuracy to the Recidivism score. vpeople = [] with open(&quot;./data/cox-violent-parsed.csv&quot;) as f: reader = PeekyReader(DictReader(f)) try: while True: p = Person(reader) if p.valid: vpeople.append(p) except StopIteration: pass vpop = list(filter(lambda i: ((i.violent_recidivist == True and i.lifetime &lt;= 730) or i.lifetime &gt; 730), list(filter(lambda x: x.vscore_valid, vpeople)))) vrecid = list(filter(lambda i: i.violent_recidivist == True and i.lifetime &lt;= 730, vpeople)) vrset = set(vrecid) vsurv = [i for i in vpop if i not in vrset] create_table(vrecid, vsurv).to_csv(&quot;data/table_vrecid.csv&quot;) read.csv(here(&quot;data&quot;, &quot;table_vrecid.csv&quot;))[,-1] %&gt;% ggplot(aes(x = Metrics, y = Scores)) + geom_col() + labs(title = &quot;Violent recidivism&quot;) Even more so for Black defendants. create_comp_tables(vrecid, vsurv).to_csv(&quot;data/comp_tables_vrecid.csv&quot;) read.csv(here(&quot;data&quot;, &quot;comp_tables_vrecid.csv&quot;))[,-1] %&gt;% ggplot(aes(x = Metrics, y = Scores, fill = Group)) + geom_col(position = &quot;dodge&quot;) + coord_flip() + labs(title = &quot;Violent recidivism&quot;) 7.9 References 7.9.1 Books An Introduction to Statistical Learning - with Applications in R (2013) by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Springer: New York. Amazon or free PDF. Hands-On Machine Learning with R (2020) by Bradley Boehmke &amp; Brandon Greenwell. CRC Press or Amazon Applied Predictive Modeling (2013) by Max Kuhn and Kjell Johnson. Springer: New York. Amazon Feature Engineering and Selection: A Practical Approach for Predictive Models (2019) by Kjell Johnson and Max Kuhn. Taylor &amp; Francis. Amazon or free HTML. Tidy Modeling with R (2020) by Max Kuhn and Julia Silge (work-in-progress) 7.9.2 Lecture slides An introduction to supervised and unsupervised learning (2015) by Susan Athey and Guido Imbens Introduction Machine Learning with the Tidyverse by Alison Hill 7.9.3 Blog posts “Using the recipes package for easy pre-processing” by Rebecca Barter "],["big-data.html", "Chapter 8 Big data 8.1 Overview 8.2 SQL 8.3 Spark", " Chapter 8 Big data 8.1 Overview Big data problem: data is too big to fit into memory (=local environment). R reads data into random-access memory (RAM) at once and this object lives in memory entirely. So, if object.size &gt; memory.size, the process will crash R. Therefore, the key to deal with big data in R is reducing the size of data you want to bring into it. Techniques to deal with big data Medium sized file (1-2 GB) Try to reduce the size of the file using slicing and dicing Tools: R:data.table::fread(file path, select = c(\"column 1\", \"column 2\")). This command imports data faster than read.csv() does. Command line: csvkit - a suite of command-line tools to and working with CSV Large file (&gt; 2-10 GB) Put the data into a database and ACCESS it Explore the data and pull the objects of interest Databases Types of databases Relational database = a collection of tables (fixed columns and rows): SQL is a staple tool to define, query (focus of the workshop today), control, and manipulate this type of database Non-relational database = a collection of documents (MongoDB), key-values (Redis and DyanoDB), wide-column stores (Cassandra and HBase), or graph (Neo4j and JanusGraph). Note that this type of database does not preclude SQL. NoSQL stands for “not only SQL.” Relational database example Relational Database. Source: MySQL Tutorial 8.2 SQL Structured Query Language. Called SEQUEL and developed by IBM Corporation in the 1970s. Remains the standard language for a relational database management system. It’s a DECLARATIVE language (what to do &gt; how to do) Database management systems figures optimal way to execute query (query optimization) SELECT COLUMN FROM TABLE 8.2.1 Learning objectives Embracing a new mindset: shifting from ownership (opening CSVs stored in your laptop) to access (accessing data stored in a database) Learning how to use R and SQL to access and query a database 8.2.2 SQL and R SQL and R SQL R SELECT select() for columns, mutate() for expressions, summarise() for aggregates FROM which data frame WHERE filter() GROUP BY group_by() HAVING filter() after group_by() ORDER BY arrange() LIMIT head() Challenge 1 1. Can you tell me the difference in the order in which the following R and SQL code were written to manipulate data? For instance, in R, what command comes first? In contrast, in SQL, what command comes first? R example data %&gt;% # Data select() %&gt;% # Column filter() %&gt;% # Row group_by() %&gt;% # Group by summarise(n = n()) %&gt;% # n() is one of the aggregate functions in r; it&#39;s count() used inside summarise() function filter() %&gt;% # Row order_by() # Arrange SQL example (in a SQL chunk, use -- instead of # to comment) SELECT column, aggregation (count())` -- Column FROM data # Data WHERE condition -- Filter rows GROUP BY column -- Group by HAVING condition -- Filter rows after group by ORDER BY column -- Arrange SQL Zine by by Julia Evans 8.2.3 Setup Let’s get to work. 8.2.4 Packages pacman::p_load() reduces steps for installing and loading several packages simultaneously. # pacman if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman # The rest of pkgs pacman::p_load( tidyverse, # tidyverse packages DBI, # using SQL queries RSQLite, # SQLite dbplyr, # use database with dplyr glue, # glue to automate workflow nycflights13 # toy data ) 8.2.5 NYC flights data The flight on-time performance data from the Bureau of Transportation Statistics of the U.S. government. The data goes back to 1987 and its size is more than 20 gigabytes. For practice, we only use a small subset of the original data (flight data departing NYC in 2013) provided by RStudio. From RStudio. 8.2.6 Workflow Create/connect to a database Note that server also can be your laptop (called localhost). Short answer: To do so, you need interfaces between R and a database. We use RSQLite in this tutorial because it’s easy to set up. Long answer: The DBI package in R provides a client-side interface that allows dplyr to work with databases. DBI is automatically installed when you installed dbplyr. However, you need to install a specific backend engine (a tool for communication between R and a database management system) for the database (e.g., RMariaDB, RPostgres, RSQLite). In this workshop, we use SQLite because it is the easiest to get started with. Personally, I love PostgreSQL because it’s an open-source and also powerful to do many amazing things (e.g., text mining, geospatial analysis). If you want to not only build a data warehouse, but an anlytical platform then consider using Spark (Hadoop). Copy a table to the database Option 1: You can create a table and insert rows manually. In order to do that, you also need to define data schema (the structure of the database). Table Collection of rows Collection of columns (fields or attributes) Each col has a type: String: VARCHAR(20) Integer: INTEGER Floating-point: FLOAT, DOUBLE Date/time: DATE, TIME, DATETIME Schema: the structure of the database The table name The names and types of its columns Various optional additional information Constraints Syntax: column datatype constraint Examples: NOT NULL, UNIQUE, INDEX -- Create table CREATE TABLE students ( id INT AUTO_INCREMENT, name VARCHAR(30), birth DATE, gpa FLOAT, grad INT, PRIMARY KEY(id)); -- Insert one additional row INSERT INTO students(name, birth, gpa, grad) VALUES (&#39;Adam&#39;, &#39;2000-08-04&#39;, 4.0, 2020); Option 2: Copy a file (object) to a table in a database using copy_to). We take this option as it’s fast and we would like to focus on querying in this workshop. Query the table Main focus Pull the results of interests (data) using collect() Disconnect the database 8.2.6.1 Create a database # Define a backend engine drv &lt;- RSQLite::SQLite() # Create an empty in-memory database con &lt;- DBI::dbConnect(drv, dbname = &quot;:memory:&quot;) # Connect to an existing database #con &lt;- DBI::dbConnect(RMariaDB::MariaDB(), # host = &quot;database.rstudio.com&quot;, # user = &quot;hadley&quot;, # password = rstudioapi::askForPassword(&quot;Database password&quot;) #) dbListTables(con) ## character(0) # character(0) = NULL Note that con is empty at this stage. 8.2.6.2 Copy an object as a table to the database (push) # Copy objects to the data # copy_to() comes from dplyr copy_to(dest = con, df = flights) copy_to(dest = con, df = airports) copy_to(dest = con, df = planes) copy_to(dest = con, df = weather) # If you need, you can also select which columns you would like to copy: # copy_to(dest = con, # df = flights, # name = &quot;flights&quot;, # indexes = list(c(&quot;year&quot;, &quot;tailnum&quot;, &quot;dest&quot;))) # Show two tables in the database dbListTables(con) ## [1] &quot;airports&quot; &quot;flights&quot; &quot;planes&quot; &quot;sqlite_stat1&quot; &quot;sqlite_stat4&quot; ## [6] &quot;weather&quot; # Show the columns/attributes/fields of a table dbListFields(con, &quot;flights&quot;) ## [1] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;dep_time&quot; ## [5] &quot;sched_dep_time&quot; &quot;dep_delay&quot; &quot;arr_time&quot; &quot;sched_arr_time&quot; ## [9] &quot;arr_delay&quot; &quot;carrier&quot; &quot;flight&quot; &quot;tailnum&quot; ## [13] &quot;origin&quot; &quot;dest&quot; &quot;air_time&quot; &quot;distance&quot; ## [17] &quot;hour&quot; &quot;minute&quot; &quot;time_hour&quot; dbListFields(con, &quot;weather&quot;) ## [1] &quot;origin&quot; &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;hour&quot; ## [6] &quot;temp&quot; &quot;dewp&quot; &quot;humid&quot; &quot;wind_dir&quot; &quot;wind_speed&quot; ## [11] &quot;wind_gust&quot; &quot;precip&quot; &quot;pressure&quot; &quot;visib&quot; &quot;time_hour&quot; 8.2.6.3 Quick demonstrations: SELECT desired columns FROM tables Select all columns (*) from flights table and show the first ten rows Note that you can combine SQL and R commands thanks to dbplyr. Option 1 DBI::dbGetQuery(con, &quot;SELECT * FROM flights;&quot;) %&gt;% # SQL head(10) # dplyr ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4 -18 B6 725 N804JB JFK BQN 183 1576 5 45 ## 5 -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 6 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## 7 19 B6 507 N516JB EWR FLL 158 1065 6 0 ## 8 -14 EV 5708 N829AS LGA IAD 53 229 6 0 ## 9 -8 B6 79 N593JB JFK MCO 140 944 6 0 ## 10 8 AA 301 N3ALAA LGA ORD 138 733 6 0 ## time_hour ## 1 1357034400 ## 2 1357034400 ## 3 1357034400 ## 4 1357034400 ## 5 1357038000 ## 6 1357034400 ## 7 1357038000 ## 8 1357038000 ## 9 1357038000 ## 10 1357038000 Option 2 (works faster) Option 3 (automating workflow) When local variables are updated, the SQL query is also automatically updated. This approach is called parameterized query (or prepared statement). ######################## PREPARATION ######################## # Local variables tbl &lt;- &quot;flights&quot; var &lt;- &quot;dep_delay&quot; num &lt;- 10 # Glue SQL query string # Note that to indicate a numeric value, you don&#39;t need `` sql_query &lt;- glue_sql(&quot; SELECT {`var`} FROM {`tbl`} LIMIT {num} &quot;, .con = con) ######################## EXECUTION ######################## # Run the query dbGetQuery(con, sql_query) ## dep_delay ## 1 2 ## 2 4 ## 3 2 ## 4 -1 ## 5 -6 ## 6 -4 ## 7 -5 ## 8 -3 ## 9 -3 ## 10 -2 Challenge 2 Can you rewrite the above code using LIMIT instead of head(10)? You may notice that using only SQL code makes querying faster. Select dep_delay and arr_delay from flights table, show the first ten rows, then turn the result into a tibble. Challenge 3 Could you remind me how to see the list of attributes of a table? Let’s say you want to see the attributes of flights table. How can you do it? Collect the selected columns and filtered rows df &lt;- dbGetQuery(con, &quot;SELECT dep_delay, arr_delay FROM flights;&quot;) %&gt;% head(10) %&gt;% collect() Counting rows Count all (*) dbGetQuery(con, &quot;SELECT COUNT(*) FROM flights;&quot;) ## COUNT(*) ## 1 336776 dbGetQuery(con, &quot;SELECT COUNT(dep_delay) FROM flights;&quot;) ## COUNT(dep_delay) ## 1 328521 Count distinct values dbGetQuery(con, &quot;SELECT COUNT(DISTINCT dep_delay) FROM flights;&quot;) ## COUNT(DISTINCT dep_delay) ## 1 527 8.2.6.4 Tidy-way: dplyr -&gt; SQL Thanks to the dbplyr package you can use the dplyr syntax to query SQL. Note that pipe (%) works. # tbl select tables flights &lt;- con %&gt;% tbl(&quot;flights&quot;) airports &lt;- con %&gt;% tbl(&quot;airports&quot;) planes &lt;- con %&gt;% tbl(&quot;planes&quot;) weather &lt;- con %&gt;% tbl(&quot;weather&quot;) select = SELECT flights %&gt;% select(contains(&quot;delay&quot;)) ## # Source: lazy query [?? x 2] ## # Database: sqlite 3.34.0 [:memory:] ## dep_delay arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 ## 2 4 20 ## 3 2 33 ## 4 -1 -18 ## 5 -6 -25 ## 6 -4 12 ## 7 -5 19 ## 8 -3 -14 ## 9 -3 -8 ## 10 -2 8 ## # … with more rows Challenge 4 Your turn: write the same code in SQL. Don’t forget to add connection argument to your SQL code chunk. mutate = SELECT AS flights %&gt;% select(distance, air_time) %&gt;% mutate(speed = distance / (air_time / 60)) ## # Source: lazy query [?? x 3] ## # Database: sqlite 3.34.0 [:memory:] ## distance air_time speed ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1400 227 370. ## 2 1416 227 374. ## 3 1089 160 408. ## 4 1576 183 517. ## 5 762 116 394. ## 6 719 150 288. ## 7 1065 158 404. ## 8 229 53 259. ## 9 944 140 405. ## 10 733 138 319. ## # … with more rows Challenge 5 Your turn: write the same code in SQL. ( Hint: mutate(new_var = var 1 * var2 (R) = SELECT var1 * var2 AS near_var (SQL) filter = WHERE flights %&gt;% filter(month == 1, day == 1) # filter(month ==1 &amp; day == 1) Both work in the same way. ## # Source: lazy query [?? x 19] ## # Database: sqlite 3.34.0 [:memory:] ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with more rows, and 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, ## # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dbl&gt; Challenge 6 Your turn: write the same code in SQL (hint: filter(condition1, condition2) = WHERE condition1 and condition2) Additional tips Note that R and SQL operators are not exactly alike. R uses != for Not equal to. SQL uses &lt;&gt; or !=. Furthermore, there are some cautions about using NULL (NA; unknown or missing): it should be IS NULL or IS NOT NULL not =NULL or !=NULL (this makes sense because NULL represents an absence of a value). Another pro-tip is LIKE operator, which is used in a WHERE statement to find values based on string patterns. SELECT DISTINCT(origin) -- Distinct values from origin column FROM flights WHERE origin LIKE &#39;J%&#39;; -- Find any origin values that start with &quot;J&quot; Table 8.1: 1 records origin JFK % is one of the wildcards that you can use for string matching. % matches any number of characters. So, J% matches Jae, JFK, Joseph, etc. _ is another useful wildcard and it matches exactly one character. So J_ matches only JA, JE, etc. If wildcards are not enough, then you should consider using regular expressions. arrange = ORDER BY flights %&gt;% arrange(carrier, desc(arr_delay)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT * ## FROM `flights` ## ORDER BY `carrier`, `arr_delay` DESC Challenge 7 Your turn: write the same code in SQL. Hint: arrange(var1, desc(var2) (R) = ORDER BY var1, var2 DESC (SQL) summarise = SELECT AS and group by = GROUP BY flights %&gt;% group_by(month, day) %&gt;% summarise(delay = mean(dep_delay)) ## Warning: Missing values are always removed in SQL. ## Use `mean(x, na.rm = TRUE)` to silence this warning ## This warning is displayed only once per session. ## # Source: lazy query [?? x 3] ## # Database: sqlite 3.34.0 [:memory:] ## # Groups: month ## month day delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 11.5 ## 2 1 2 13.9 ## 3 1 3 11.0 ## 4 1 4 8.95 ## 5 1 5 5.73 ## 6 1 6 7.15 ## 7 1 7 5.42 ## 8 1 8 2.55 ## 9 1 9 2.28 ## 10 1 10 2.84 ## # … with more rows Challenge 8 Your turn: write the same code in SQL (hint: in SQL the order should be SELECT group_var1, group_var2, AVG(old_var) AS new_var -&gt; FROM -&gt; GROUP BY) If you feel too much challenged, here’s a help. flights %&gt;% group_by(month, day) %&gt;% summarise(delay = mean(dep_delay)) %&gt;% show_query() # Show the SQL equivalent! ## &lt;SQL&gt; ## SELECT `month`, `day`, AVG(`dep_delay`) AS `delay` ## FROM `flights` ## GROUP BY `month`, `day` Joins Using joins is simpler in R than it is in SQL. However, more flexible joins exist in SQL and they are not available in R. Joins involving 3+ tables are not supported. Some advanced joins available in SQL are not supported. For more information, check out tidyquery to see the latest developments. SQL command FROM one table LEFT JOIN another table ON condition = condition (ON in SQL = BY in R) SELECT * FROM flights AS f LEFT JOIN weather AS w ON f.year = w.year AND f.month = w.month Table 8.2: Displaying records 1 - 10 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest air_time distance hour minute time_hour origin year month day hour temp dewp humid wind_dir wind_speed wind_gust precip pressure visib time_hour 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 1 39.02 26.06 59.37 270 10.35702 NA 0 1012.0 10 1357020000 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 2 39.02 26.96 61.63 250 8.05546 NA 0 1012.3 10 1357023600 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 3 39.02 28.04 64.43 240 11.50780 NA 0 1012.5 10 1357027200 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 4 39.92 28.04 62.21 250 12.65858 NA 0 1012.2 10 1357030800 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 5 39.02 28.04 64.43 260 12.65858 NA 0 1011.9 10 1357034400 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 6 37.94 28.04 67.21 240 11.50780 NA 0 1012.4 10 1357038000 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 7 39.02 28.04 64.43 240 14.96014 NA 0 1012.2 10 1357041600 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 8 39.92 28.04 62.21 250 10.35702 NA 0 1012.2 10 1357045200 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 9 39.92 28.04 62.21 260 14.96014 NA 0 1012.7 10 1357048800 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 EWR 2013 1 1 10 41.00 28.04 59.65 260 13.80936 NA 0 1012.4 10 1357052400 Can anyone explain why SQL query using dplyr then translated by show_query() looks so complex compared to the above? (Hint) flights %&gt;% left_join(weather, by = c(&quot;year&quot;, &quot;month&quot;)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT `LHS`.`year` AS `year`, `LHS`.`month` AS `month`, `LHS`.`day` AS `day.x`, `dep_time`, `sched_dep_time`, `dep_delay`, `arr_time`, `sched_arr_time`, `arr_delay`, `carrier`, `flight`, `tailnum`, `LHS`.`origin` AS `origin.x`, `dest`, `air_time`, `distance`, `LHS`.`hour` AS `hour.x`, `minute`, `LHS`.`time_hour` AS `time_hour.x`, `RHS`.`origin` AS `origin.y`, `RHS`.`day` AS `day.y`, `RHS`.`hour` AS `hour.y`, `temp`, `dewp`, `humid`, `wind_dir`, `wind_speed`, `wind_gust`, `precip`, `pressure`, `visib`, `RHS`.`time_hour` AS `time_hour.y` ## FROM `flights` AS `LHS` ## LEFT JOIN `weather` AS `RHS` ## ON (`LHS`.`year` = `RHS`.`year` AND `LHS`.`month` = `RHS`.`month`) 8.2.6.5 Collect (pull) collect() is used to pull the data. Depending on the data size, it may take a long time to run. The following code won’t work. Error in UseMethod(“collect”) : no applicable method for ‘collect’ applied to an object of class “c(‘LayerInstance,’ ‘Layer,’ ‘ggproto,’ ‘gg’)” origin_flights_plot &lt;- flights %&gt;% group_by(origin) %&gt;% tally() %&gt;% ggplot() + geom_col(aes(x = origin, y = n)) %&gt;% collect() This works. df &lt;- flights %&gt;% group_by(origin) %&gt;% tally() %&gt;% collect() origin_flights_plot &lt;- ggplot(df) + geom_col(aes(x = origin, y = n)) origin_flights_plot 8.2.6.6 Disconnect DBI::dbDisconnect(con) 8.2.7 Things we didn’t cover 8.2.7.1 Subquery Subquery = a query nested inside a query This is a hypothetical example inspired by dofactory blog post. SELECT names -- Outer query FROM consultants WHERE Id IN (SELECT ConsultingId FROM consulting_cases WHERE category = &#39;r&#39; AND category = &#39;sql&#39;); -- Subquery 8.2.7.2 Common table expression (WITH clauses) This is just a hypothetical example inspired by [James LeDoux’s blog post](https://jamesrledoux.com/code/sql-cte-common-table-expressions. -- cases about R and SQL from dlab-database WITH r_sql_consulting_cases AS ( -- The name of the CTE expression -- The CTE query SELECT id FROM dlab WHERE tags LIKE &#39;%sql%&#39; AND tags LIKE &#39;%r%&#39; ), -- count the number of open cases about this consulting category -- The outer query SELECT status, COUNT(status) AS open_status_count FROM dlab as d INNER JOIN r_sql_consulting_cases as r ON d.id = r.id WHERE status = &#39;open&#39;; 8.2.8 References csv2db - for loading large CSV files in to a database R Studio, Database using R Ian Cook, “Bridging the Gap between SQL and R” rstudio::conf 2020 slides Video recording Data Carpentry contributors, SQL database and R, Data Carpentry, September 10, 2019. Introduction to dbplyr Josh Erickson, SQL in R, STAT 701, University of Michigan SQL zine by Julia Evans q - a command line tool that allows direct execution of SQL-like queries on CSVs/TSVs (and any other tabular text files) 8.3 Spark 8.3.1 Setup Install sparklyr package Install spark using sparklyr package (If you haven’t) install Java 8 (see this guideline from the Java website) # if(!require(&quot;sparklyr&quot;)) install.packages(&quot;sparklyr&quot;) # sparklyr::spark_install(version = &quot;3.0.0&quot;) "]]
